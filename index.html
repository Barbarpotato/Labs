<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/Labs/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/Labs/_next/static/chunks/webpack-a39982bd6f80347b.js" defer=""></script><script src="/Labs/_next/static/chunks/framework-ecc4130bc7a58a64.js" defer=""></script><script src="/Labs/_next/static/chunks/main-1e09b50edce1e67f.js" defer=""></script><script src="/Labs/_next/static/chunks/pages/_app-753213cf38d40131.js" defer=""></script><script src="/Labs/_next/static/chunks/pages/index-6b5fb578ab874fe8.js" defer=""></script><script src="/Labs/_next/static/A-n-baZxhaPab-FReiSSJ/_buildManifest.js" defer=""></script><script src="/Labs/_next/static/A-n-baZxhaPab-FReiSSJ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div><h1>Blog Articles</h1><ul><li><a href="/Labs/labs/2b8dd94e-d12e-47ea-ba0b-dc2c18c37b68"><a>How to Set Up a Load Balancer in AWS: A Simple Guide to Keep Your Website Fast and Scalable</a></a></li><li><a href="/Labs/labs/986540df-4381-4405-9c40-7ff7b24e6098"><a>Understanding Rate Limiting: A Guide to Staying in Control of Your APIs</a></a></li><li><a href="/Labs/labs/0af995c6-3bd5-405a-ad71-6ebeaa675d38"><a>Building a Simple CQRS Pattern Architecture</a></a></li><li><a href="/Labs/labs/e7d3943c-c12a-42c1-9663-2d90449138dc"><a>The concept of splitting a frontend into smaller, manageable pieces.</a></a></li><li><a href="/Labs/labs/4a5ae2d7-0f8f-46b9-b49a-4ff130f22292"><a>Why You Need Kubernetes: A Comprehensive Guide</a></a></li><li><a href="/Labs/labs/6b4113f2-f30c-4e12-a34a-f5c02abbd1cb"><a>Mastering Apache Spark: An Engaging Dive into Its Architecture and Clusters</a></a></li><li><a href="/Labs/labs/9f224db5-a76d-4a6f-9f7b-32bfbdf5696f"><a>MapReduce: The Magic Behind Processing Big Data in Hadoop</a></a></li><li><a href="/Labs/labs/2418e501-daff-473e-b29c-81ba9d65d596"><a>Understanding Hadoop Distributed File System (HDFS)</a></a></li><li><a href="/Labs/labs/7a76b44a-8bc3-451e-95f0-3ccb1bc23e34"><a>Create Dashboard app with Electron.js</a></a></li><li><a href="/Labs/labs/bc2c576f-29ce-4513-ab68-b1c336eb9e90"><a>Introduction to Elastic Search</a></a></li><li><a href="/Labs/labs/4e309d73-dc31-47b8-816a-901fd092368d"><a>Integrate Mysql to Flask</a></a></li><li><a href="/Labs/labs/e27f5bbc-f1be-4013-89da-312a09e6f3c0"><a>Search Engine Optimization</a></a></li><li><a href="/Labs/labs/14d690d3-2201-43e4-b936-e06564ab67e2"><a>React PWA Fundamental</a></a></li><li><a href="/Labs/labs/9c490813-d5d6-4e27-8cc8-31f115046c0e"><a>GraphQL Fundamental</a></a></li><li><a href="/Labs/labs/978cd40e-18da-47a6-a4fe-917bec69dea1"><a>React Lazy Load</a></a></li><li><a href="/Labs/labs/c4973cc3-3c29-4fcb-9651-5d921b02eed2"><a>ETL using shell scripts</a></a></li><li><a href="/Labs/labs/0e1dd5fa-3510-411f-9dc9-85938efe6af9"><a>Working with Time &amp; Space Complexity </a></a></li><li><a href="/Labs/labs/59ebedc0-f362-4c2b-a7ee-a5fd8db2bc29"><a>Create a DAG for Apache Airflow with Python Operator</a></a></li><li><a href="/Labs/labs/cfb72d72-4754-48f1-b815-52edb986d17b"><a>Introduction to Apache Kafka</a></a></li><li><a href="/Labs/labs/8be4d7fd-b2ea-4b0f-ad3b-a6064553e06b"><a>Build Kafka Python Client</a></a></li><li><a href="/Labs/labs/82db10ae-f820-45d6-b985-9ac22fa7046f"><a>Create GRPC With MongoDB &amp; Node.js</a></a></li><li><a href="/Labs/labs/40c0ec5c-b8f4-4965-8bd6-be9f65e867fb"><a>Test-Driven Development with Python</a></a></li><li><a href="/Labs/labs/b731f0f7-895d-46df-9377-cb5bf9008e0a"><a>Build Redis in Next.js</a></a></li><li><a href="/Labs/labs/1fab12ad-ebb2-435c-9dc7-17e4015e9b95"><a>Performance Testing with k6</a></a></li></ul></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"articles":[{"blog_id":"2b8dd94e-d12e-47ea-ba0b-dc2c18c37b68","description":"\u003ch1\u003e\u003cstrong\u003eWhy You Need Load Balancing\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eImagine you‚Äôre throwing a huge party, and your house is the server. If everyone shows up at once, there‚Äôs no room to move, and things get crowded. But if you have a few people guiding guests into different rooms, everyone gets a chance to enjoy the party without overcrowding any single room. This is \u003cstrong\u003eLoad Balancing\u003c/strong\u003e!\u003c/p\u003e\u003cp\u003eLoad balancing is like your traffic manager: it takes the incoming website traffic and evenly distributes it across multiple servers. The result?\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eBetter Performance\u003c/strong\u003e: No server is overloaded.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMore Reliability\u003c/strong\u003e: If one server crashes, the others keep the party going.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEasy Scalability\u003c/strong\u003e: As your site grows, you can add more servers to handle more traffic without a hitch.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this guide, we‚Äôre going to set up a simple load balancing system on AWS using \u003cstrong\u003eEC2 instances\u003c/strong\u003e and \u003cstrong\u003eNginx\u003c/strong\u003e (a popular web server). We‚Äôll skip the fancy AWS Elastic Load Balancer (ELB) for now and do it ourselves. üòé\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eThe Load Balancing Setup in a Nutshell\u003c/strong\u003e\u003c/h1\u003e\u003ch1\u003e\u003cimg src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fload_balancing_structure.png?alt=media\u0026amp;token=92732226-d927-43d0-9d9e-3d82fed692f1\" alt=\"load balancing workflow example for demo\" width=\"720px\"\u003e\u003c/h1\u003e\u003cp\u003eIn a typical web application, you want to ensure that traffic is efficiently managed across your infrastructure. If all users are directed to a single server, that server might get overloaded, causing slow response times or crashes. This is where \u003cstrong\u003eload balancing\u003c/strong\u003e comes into play.\u003c/p\u003e\u003cp\u003eLet‚Äôs break down our setup:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- 1 Load Balancer\u003c/strong\u003e: This is like the traffic cop for your web traffic. When users visit your website, their requests first hit the load balancer. The load balancer‚Äôs job is to direct these requests to the available backend servers.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- 2 Server Instances\u003c/strong\u003e: These are your backend servers that actually process the users‚Äô requests and return content. The load balancer will send incoming requests to either Server 1 or Server 2, ensuring that no single server is overwhelmed. If one server goes down, the load balancer automatically redirects traffic to the other one, ensuring high availability.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eWhat You‚Äôll Need\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eBefore we start, make sure you have:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Three EC2 instances\u003c/strong\u003e on AWS: One for the load balancer and two for the backend servers.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2. Nginx installed\u003c/strong\u003e on all the instances. It‚Äôll handle the distribution of traffic to the backend servers.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eLet‚Äôs Get Started! Step by Step\u003c/strong\u003e\u003c/h1\u003e\u003ch3\u003e\u003cstrong\u003eStep 1: Launch EC2 Instances in AWS\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eYou‚Äôll need three EC2 instances:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eServer 1 (Backend)\u003c/strong\u003e: This server will handle part of the traffic.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eServer 2 (Backend)\u003c/strong\u003e: Another server to handle a different part of the traffic.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLoad Balancer\u003c/strong\u003e: This instance will distribute the traffic between the two backend servers.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAll servers should be in the same \u003cstrong\u003eVPC\u003c/strong\u003e (Virtual Private Cloud) for easy communication.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eStep 2: Install Nginx on Each EC2 Instance\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOnce your EC2 instances are running, it‚Äôs time to install Nginx. You can do that by connecting to each instance and running:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo apt update \nsudo apt install nginx\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3\u003e\u003cstrong\u003eStep 3: Configure Nginx on the Load Balancer\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eNow comes the fun part ‚Äî setting up Nginx on the \u003cstrong\u003eLoad Balancer\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Access your load balancer instance\u003c/strong\u003e and open the Nginx configuration file:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo nano /etc/nginx/nginx.conf\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003e2. Add the following configuration\u003c/strong\u003e inside the http block:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eupstream backend_servers {\n\u0026nbsp;\u0026nbsp;server 172.31.27.145;\u0026nbsp;# Private IP of Server 1\n\u0026nbsp;\u0026nbsp;server 172.31.27.91;\u0026nbsp;\u0026nbsp;# Private IP of Server 2\n}\n\nserver {\n\u0026nbsp;\u0026nbsp;listen 80;\n\u0026nbsp;\u0026nbsp;server_name \u0026lt;load-balancer-public-IP\u0026gt;;\n\n\u0026nbsp;\u0026nbsp;location / {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;proxy_pass http://backend_servers;\n\u0026nbsp;\u0026nbsp;}\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis is telling Nginx, ‚ÄúHey, I‚Äôve got two backend servers here (with private IPs), and I want to send all incoming traffic to them.‚Äù The load balancer will automatically send traffic to either Server 1 or Server 2.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCheck if the configuration is correct\u003c/strong\u003e:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo nginx -t\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eReload Nginx\u003c/strong\u003e to apply the new settings:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo systemctl reload nginx\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3\u003e\u003cstrong\u003eStep 4: Test Your Backend Servers\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eBefore you test the load balancing, make sure both of your backend servers are working properly.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- On Server 1\u003c/strong\u003e, create a simple HTML file that says ‚ÄúHello from Server 1‚Äù:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo nano /usr/share/nginx/html/index.html\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003e- Write: Hello from Server 2 n the file.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- On Server 2, do the same but say ‚ÄúHello from Server 2‚Äù.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- Test each server directly:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFrom your load balancer instance, you can use curl to check if each server is serving the correct page:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ecurl http://172.31.27.145  # Server 1\ncurl http://172.31.27.91   # Server 2\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eYou should see ‚ÄúHello from Server 1‚Äù for Server 1 and ‚ÄúHello from Server 2‚Äù for Server 2.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eStep 5: Test the Load Balancer\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eNow, let‚Äôs put the load balancing to the test. Open your browser and go to the \u003cstrong\u003epublic IP\u003c/strong\u003e of the \u003cstrong\u003eLoad Balancer\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eWhen you hit the load balancer‚Äôs IP:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIt should alternate between ‚ÄúHello from Server 1‚Äù and ‚ÄúHello from Server 2‚Äù as Nginx sends traffic to each backend server in turn.\u003c/li\u003e\u003c/ul\u003e\u003ch1\u003e\u003cstrong\u003eConclusion: You Did It!\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eCongratulations! üéâ You‚Äôve just set up a basic load balancing system using Nginx on AWS EC2 instances. By distributing traffic across multiple servers, you‚Äôve made your web app faster, more reliable, and more scalable.\u003c/p\u003e\u003cp\u003eThis is just the beginning ‚Äî you can easily expand this setup by adding more backend servers or even implementing more advanced load balancing techniques.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eWhat's Next?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003e1. Add More Servers\u003c/strong\u003e: Scale up by adding more backend servers to handle more traffic.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2. Secure Your Setup\u003c/strong\u003e: Set up SSL certificates to encrypt traffic.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Monitor Your Servers\u003c/strong\u003e: Use AWS CloudWatch or Nginx logs to monitor the performance of your servers and load balancer.\u003c/p\u003e\u003cp\u003eNow that you understand how load balancing works, you can implement it in your projects to keep your websites and applications fast and responsive, no matter how much traffic you get! üåêüöÄ\u003c/p\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fload_balancing_background.jpg?alt=media\u0026token=a5b062fc-45ad-4770-be8f-ff5b7097c6cc","image_alt":"Load Balancing Background Lab","short_description":"You‚Äôve built an amazing website or app, and the traffic starts pouring in. Users are clicking, scrolling, and browsing like never before. Everything is running smoothly‚Ä¶ until it‚Äôs not. Suddenly, your server gets overwhelmed, and your site slows down. Worse, it crashes. Yikes!  What do you do?  Enter Load Balancing ‚Äî your superhero in the world of web traffic. üí™  But why do we need it, and how do we set it up on AWS using Nginx? Let's dive in and break it down in simple terms.","timestamp":"2024-12-12 12:48:36","title":"How to Set Up a Load Balancer in AWS: A Simple Guide to Keep Your Website Fast and Scalable"},{"blog_id":"986540df-4381-4405-9c40-7ff7b24e6098","description":"\u003ch1\u003e\u003cstrong\u003eWhat Is Rate Limiting?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eAt its core, \u003cstrong\u003erate limiting\u003c/strong\u003e is a control mechanism used in software systems, especially APIs, to restrict how many requests a client can make within a specific timeframe.\u003c/p\u003e\u003cp\u003eThink of it as setting the speed limit on a highway. Without it, cars (or requests) might flood the lanes, causing congestion (or a system crash). Rate limiting ensures everyone gets to their destination (or data) without overwhelming the system.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eWhy Does Rate Limiting Matter?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eImagine running an online service where thousands (or even millions) of users access your API. What happens if one rogue user floods your system with excessive requests?\u003c/p\u003e\u003cp\u003eWithout rate limiting, here‚Äôs what you might face:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1.Server Overload:\u003c/strong\u003e Your system might slow down or crash entirely.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2.Unhappy Users:\u003c/strong\u003e Other users won‚Äôt get timely responses, leading to frustration.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3.Increased Costs:\u003c/strong\u003e Handling unnecessary requests eats up resources.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4.Security Risks:\u003c/strong\u003e It‚Äôs an open invitation for \u003cstrong\u003eDDoS (Distributed Denial of Service)\u003c/strong\u003e attacks.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eHow Does Rate Limiting Work?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003e\u003cimg src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Frate_limiter_flow.jpg?alt=media\u0026amp;token=0150c280-c4a3-42b2-8422-0a223711a465\" alt=\"Rate limieter workflow example\" width=\"720px\"\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003e1. The Client Sends a Request\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eLet‚Äôs start with the clients‚Äîyour users. They might be requesting to fetch data, submit a form, or interact with your app in some way. Every action sends a request to your API server.\u003c/p\u003e\u003cp\u003eNow, without a system in place, too many requests from too many clients could crush the API. This is where the rate limiter steps in.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003e2. The Rate Limiter Checks the Gate\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe rate limiter is your vigilant bouncer. Each incoming request is checked against a set of rules. For example:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRule:\u003c/strong\u003e No more than 10 requests per second per client.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRule:\u003c/strong\u003e A maximum of 1,000 requests per day for premium users.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf a request fits within the rules, it gets a thumbs-up. If not, the rate limiter steps in with a polite \"Sorry, you've reached your limit\" (a.k.a., the \u003ccode\u003e429 Too Many Requests\u003c/code\u003e error).\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003e3. Redis: The Silent Helper\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eNow, how does the rate limiter keep track of all this? Enter \u003cstrong\u003eRedis\u003c/strong\u003e, the speedy memory store.\u003c/p\u003e\u003cp\u003eRedis is like a super-efficient notebook that logs each client‚Äôs request count. Here‚Äôs how it works:\u003c/p\u003e\u003cp\u003eWhen a request comes in, Redis:\u003c/p\u003e\u003cp\u003e-Checks how many requests the client has already made.\u003c/p\u003e\u003cp\u003e-Updates the tally in real-time.\u003c/p\u003e\u003cp\u003eRedis‚Äôs speed and scalability make it perfect for handling this kind of workload.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003e4. Forwarding to the API Server\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIf the request passes the rate limiter‚Äôs scrutiny, it‚Äôs sent to the \u003cstrong\u003eAPI server\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eThe server processes the request, performs the required action (like retrieving data or updating a record), and sends the response back to the client.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eCommon Rate Limiting Strategies\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eHere are some popular methods to implement rate limiting:\u003c/p\u003e\u003ch3\u003e1. \u003cstrong\u003eFixed Window Algorithm\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThink of it as a time bucket. If you allow 100 requests per minute, the count resets every minute.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExample:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIf a user sends 99 requests in the last second of a window and 100 in the next second, they technically make 199 requests within two seconds. (Uh-oh!)\u003c/p\u003e\u003ch3\u003e2. \u003cstrong\u003eSliding Window Algorithm\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThis method smooths things out by tracking requests over a rolling time window. It‚Äôs like always looking back 60 seconds from the current moment to count requests.\u003c/p\u003e\u003ch3\u003e3. \u003cstrong\u003eToken Bucket\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eImagine each user has a bucket filled with tokens. Each request consumes a token. If the bucket is empty, no more requests are processed until it refills.\u003c/p\u003e\u003ch3\u003e4. \u003cstrong\u003eLeaky Bucket\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThis works like a dripping faucet. Even if the user sends requests in bursts, the system processes them at a consistent rate.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eWhere Is Rate Limiting Used?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eRate limiting isn‚Äôt just for APIs‚Äîit‚Äôs everywhere!\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Social Media Platforms:\u003c/strong\u003e To prevent spamming or abuse (e.g., limiting tweets per minute).\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2. E-Commerce Sites:\u003c/strong\u003e To stop bots from sniping deals during flash sales.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Gaming Servers:\u003c/strong\u003e To ensure fair play and prevent server overloads.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4. Banking APIs:\u003c/strong\u003e To protect sensitive systems from fraud or misuse.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eWhy Rate Limiting is Essential\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003eRate limiting isn‚Äôt just about saying ‚Äúno.‚Äù It‚Äôs about balance.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eHere‚Äôs what it brings to the table:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Fair Access:\u003c/strong\u003e Every client gets a fair chance to use the API without hogging resources.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2. Protection:\u003c/strong\u003e Prevents accidental overloads or deliberate attacks (like DDoS) from crashing the system.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Cost Efficiency:\u003c/strong\u003e By controlling traffic, you reduce server strain and save on infrastructure costs.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eThe Big Picture\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWith rate limiting, APIs can breathe easy, knowing that they‚Äôre protected from chaos while serving users efficiently. It‚Äôs not just a technical tool‚Äîit‚Äôs a safeguard for smooth operations.\u003c/p\u003e\u003cp\u003eSo, next time you‚Äôre designing an API or interacting with one, remember: there‚Äôs a silent hero ensuring everything runs seamlessly. Whether it‚Äôs Redis handling the count or the rate limiter enforcing rules, this system is your API‚Äôs best friend.\u003c/p\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Frate_limiter_bg.png?alt=media\u0026token=0c9fc3ba-7b9d-4fce-8a15-293f8a79d664","image_alt":"Rate limiter cover background","short_description":"Imagine you‚Äôre hosting a party, and everyone wants to grab snacks from the buffet table at the same time. It‚Äôs chaos! Some guests get everything they want, while others leave empty-handed. What if you had a rule where each guest could only take two items at a time? Suddenly, everyone gets a fair share, and your party doesn‚Äôt turn into a food fight. That‚Äôs rate limiting in a nutshell!  But what exactly is rate limiting, and why is it so important? Let‚Äôs dive in and explore this concept together.","timestamp":"2024-12-10 12:37:01","title":"Understanding Rate Limiting: A Guide to Staying in Control of Your APIs"},{"blog_id":"0af995c6-3bd5-405a-ad71-6ebeaa675d38","description":"\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eCommand Query Responsibility Segregation (CQRS)\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;pattern is an architectural pattern used to separate the\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ewrite\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;(commands) and\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eread\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;(queries) sides of an application. This separation ensures scalability, performance optimization, and flexibility, especially for systems with complex business logic.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThis article explains how to design a\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003esimple CQRS pattern\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;and implement it in a practical example.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u003cimg src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fcqrs.png?alt=media\u0026amp;token=6eab7b0b-37d8-49f2-9137-27dadd766c96\" alt=\"CQRS Basic Pattern\" width=\"720px\"\u003e\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eWhy Do We Need CQRS?\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eCQRS is designed to address challenges in systems where the\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eread\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;and\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ewrite\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;operations have distinct requirements. It is particularly helpful in large, complex applications with high performance, scalability, and maintainability needs. Here‚Äôs a breakdown of its importance:\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eHere‚Äôs a detailed explanation of\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ewhy we need CQRS (Command Query Responsibility Segregation)\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e:\u003c/span\u003e\u003c/p\u003e\u003ch3\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eSeparation of Concerns\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn traditional CRUD-based architectures, the same model is often used for both reading and writing data. This can lead to problems such as:\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e1.Bloated models trying to handle both reads and writes.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e2.Tight coupling between read and write logic, making it harder to change one without affecting the other.\u003c/span\u003e\u003c/p\u003e\u003ch3\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eOptimization of Reads and Writes\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn many applications, the requirements for\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ereading data\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;differ significantly from\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ewriting data\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e:\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e1.Writes\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;may require strict validation, transactional consistency, and complex domain logic.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e2.Reads\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;often focus on speed, scalability, and simplicity, potentially requiring optimized or denormalized views of data.\u003c/span\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePre-requisites for this lab:\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn this article we will build simple cqrs architecture with apache kafka, elastic search, and the backend service (Express.js).\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eÔÇ∑\u0026nbsp;VM 1: Runs\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eApache Kafka\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;for handling messaging and event distribution.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eÔÇ∑\u0026nbsp;VM 2: Runs\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eElasticsearch\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;for read-optimized data storage and retrieval.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eExpress.js Services\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e:\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eÔÇ∑\u0026nbsp;A\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eCommand service\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;with an endpoint for inserting data into the system.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eÔÇ∑\u0026nbsp;A\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eQuery service\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;to retrieve data from Elasticsearch.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eÔÇ∑\u0026nbsp;Mysql Database that connected to Express.js service\u003c/span\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePlanned Architecture\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u003cimg src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FCQRS.webp?alt=media\u0026amp;token=58c5dcef-485c-482d-8c6a-459473e51f04\" alt=\"Planned CQRS Architecture for this lab\" width=\"720px\"\u003e\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe architecture depicted in your diagram showcases a practical implementation of the CQRS (Command Query Responsibility Segregation) pattern using separate services and data stores for handling write and read operations. At its core, this design focuses on decoupling the responsibilities of updating and retrieving data, ensuring better scalability, performance, and maintainability.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eCommand Service\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e, built with Express.js, serves as the entry point for handling all write operations. Whenever a client sends a request to add or update data, the Command Service writes the data to a MySQL database. This database acts as the system's primary source of truth, ensuring the durability and consistency of all data. Once the data is successfully persisted in MySQL, the Command Service publishes an event to the\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eMessage Broker\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e, implemented with Apache Kafka. The role of Kafka here is to act as an intermediary that reliably propagates changes across the system, enabling asynchronous communication between services.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eOn the other side of the architecture, a consumer service listens to the events broadcasted by Kafka. Whenever a new event is received, the consumer retrieves the relevant data from MySQL, transforms it if needed, and indexes it into an\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eElasticSearch instance\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e. ElasticSearch, being optimized for querying and search operations, ensures that data is structured for fast retrieval. This makes it the perfect choice for systems that need to handle complex queries or search-heavy workloads without compromising performance.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eRead Service\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e, also built with Express.js, provides an API for retrieving data from ElasticSearch. By querying ElasticSearch directly, the Read Service delivers low-latency responses to clients, even under high query loads. This design ensures that the performance of the read operations does not interfere with or degrade the performance of write operations in the Command Service. The use of ElasticSearch also enables advanced search capabilities, such as full-text search, aggregations, and filtering, which are often slow or complex to implement in traditional relational databases.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThis architecture embodies the essence of CQRS by segregating the responsibilities of writing and querying data into distinct paths. The Command Service and MySQL handle writes and ensure data consistency, while the Read Service and ElasticSearch are optimized for delivering fast and efficient queries. The inclusion of Kafka as a Message Broker enables asynchronous processing, allowing the system to remain responsive to client requests even when downstream systems take time to process data.\u003c/span\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eSetting Up Apache Kafka on Ubuntu Server:\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn this guide, I‚Äôll walk you through setting up\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eApache Kafka\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;on an\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eUbuntu Server\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;running on a virtual machine. While you can certainly use Docker and Docker Compose for a Kafka setup, I decided to go the manual route to test the installation process. So, if you‚Äôre ready to roll up your sleeves, let‚Äôs dive in!\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 1: Install Java\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eKafka runs on the Java Virtual Machine (JVM), so the first step is to install Java. We‚Äôll use OpenJDK 17 for this:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo apt update\nsudo apt install openjdk-17-jdk -y\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eOnce installed, you can verify the version with:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ejava -version\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 2: Download Kafka\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eNext, we need to download the Kafka binaries. Use the following command to grab the latest Kafka release:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ewget https://downloads.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eAfter downloading, extract the archive:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003etar -xvf kafka_2.13-3.6.0.tgz\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eNow, let‚Äôs move the extracted Kafka directory to\u0026nbsp;/opt for easier access:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo mv kafka_2.13-3.6.0 /opt/kafka\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eFinally, navigate to the Kafka directory:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ecd /opt/kafka\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 3: Configure Kafka Server Properties\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eBefore we start Kafka, we need to tweak its configuration a bit. Open the\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eserver.properties\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;file with a text editor:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003enano config/server.properties\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eHere are a couple of key settings to look for:\u003c/span\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003elog.dirs: This is where Kafka will store its log files. You can set it to a directory of your choice.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003ezookeper.connect: Ensure this points to your ZooKeeper instance. If you‚Äôre running ZooKeeper locally, the default setting should work.\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 4: Start ZooKeeper\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eKafka relies on\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eZooKeeper\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;to manage its metadata, so we‚Äôll need to start ZooKeeper before starting Kafka. Use the following command to get ZooKeeper up and running:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/zookeeper-server-start.sh config/zookeeper.properties\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eTo run ZooKeeper as a background process (so you can keep using your terminal), use this instead:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/zookeeper-server-start.sh config/zookeeper.properties \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 5: Start the Kafka Broker\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eNow that ZooKeeper is running, it‚Äôs time to fire up Kafka. Use this command to start the Kafka broker:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-server-start.sh config/server.properties\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eOr, to run Kafka in the background:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-server-start.sh config/server.properties \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 6: Test Your Kafka Setup\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eCongratulations! Your Kafka instance is now up and running. Let‚Äôs do a quick test to ensure everything works as expected.\u003c/span\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eCreate a Topic\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eKafka organizes messages into topics. Let‚Äôs create a topic named\u0026nbsp;test-topic:\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eList Topics\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eTo confirm that the topic was created, list all topics:\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-topics.sh --list --bootstrap-server localhost:9092\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStart a Producer\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eA producer sends messages to a Kafka topic. Start the producer for\u0026nbsp;test-topic:\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eType a few messages in the terminal, and they‚Äôll be sent to the topic.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStart a Consumer\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eA consumer reads messages from a topic. Start a consumer to read messages from\u0026nbsp;test-topic:\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-console-consumer.sh --topic test-topic --from-beginning --bootstrap-server localhost:9092\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eYou should see the messages you typed in the producer terminal appear here!\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePart 2: Installing Elasticsearch on a Virtual Machine\u003c/strong\u003e\u003c/h2\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 1: SSH into Your Server\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eConnect to your EC2 instance (or VM):\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003essh -i /path/to/your-key.pem ec2-user@your-ec2-public-ip\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 2: Install Java\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eElasticsearch also needs Java:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo apt update\nsudo apt install -y openjdk-11-jdk\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 3: Install Elasticsearch\u003c/strong\u003e\u003c/h4\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ewget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\nsudo apt install -y apt-transport-https\necho \"deb https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-8.x.list\n\nsudo apt update\n\nsudo apt install -y elasticsearch\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 4: Enable and Start Elasticsearch\u003c/strong\u003e\u003c/h4\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo systemctl enable elasticsearch\nsudo systemctl start elasticsearch\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 5: Configure Elasticsearch for External Access\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eEdit the configuration:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo nano /etc/elasticsearch/elasticsearch.yml\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eSet these properties:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003enetwork.host: 0.0.0.0\nhttp.port: 9200\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eRestart Elasticsearch:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo systemctl restart elasticsearch\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 6: Verify Elasticsearch\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eRun:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ecurl -X GET http://localhost:9200\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eYou should see a JSON response!\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePart:3 Explanation of the Express.js POST Route for Inserting User Data and Sending to Kafka\u003c/strong\u003e\u003c/h2\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eimport express from 'express';\nimport { Kafka } from 'kafkajs';\nimport mysql from 'mysql2/promise'; // MySQL client for Node.js\n\nconst app = express();\napp.use(express.json()); // Parse JSON request bodies\n\n// Kafka producer setup\nconst kafka = new Kafka({\n\u0026nbsp; \u0026nbsp; clientId: 'my-app',\n\u0026nbsp; \u0026nbsp; brokers: ['localhost:9092'], // Replace with your Kafka broker(s)\n});\n\nconst producer = kafka.producer();\n\n// Connect Kafka producer\nasync function connectProducer() {\n\u0026nbsp; \u0026nbsp; await producer.connect();\n}\n\nconnectProducer().catch(console.error);\n\n// MySQL database connection setup\nconst dbConfig = {\n\u0026nbsp; \u0026nbsp; host: 'localhost',\n\u0026nbsp; \u0026nbsp; user: 'root', // Replace with your MySQL username\n\u0026nbsp; \u0026nbsp; password: 'root', // Replace with your MySQL password\n\u0026nbsp; \u0026nbsp; database: 'user', // Replace with your database name\n};\n\nlet connection;\n\nasync function connectDatabase() {\n\u0026nbsp; \u0026nbsp; try {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; connection = await mysql.createConnection(dbConfig);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.log('Connected to MySQL database');\n\u0026nbsp; \u0026nbsp; } catch (error) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.error('Error connecting to MySQL:', error);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; process.exit(1);\n\u0026nbsp; \u0026nbsp; }\n}\n\nconnectDatabase();\n\n// POST route to insert user data\napp.post('/users', async (req, res) =\u0026gt; {\n\u0026nbsp; \u0026nbsp; const userData = req.body;\n\n\u0026nbsp; \u0026nbsp; try {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; // Insert data into MySQL database\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; const { name, email, password } = userData; // Assuming user data has 'name' and 'email' fields\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; const [result] = await connection.execute(\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; 'INSERT INTO user (username, email, password) VALUES (?, ?, ?)',\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; [name, email, password]\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; );\n\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.log('User inserted into database:', result);\n\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; // Send the user data to Kafka topic 'user-topic'\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; await producer.send({\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; topic: 'users-topic', // Replace with your topic\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; messages: [\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; { value: JSON.stringify(userData) },\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; ],\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; });\n\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; res.status(201).json({ message: 'User data inserted into database and sent to Kafka' });\n\u0026nbsp; \u0026nbsp; } catch (error) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.error('Error processing request:', error);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; res.status(500).json({ message: 'Error processing request' });\n\u0026nbsp; \u0026nbsp; }\n});\n\n// Start Express server\napp.listen(3000, () =\u0026gt; {\n\u0026nbsp; \u0026nbsp; console.log('Server running on http://localhost:3000');\n});\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn the given code snippet, an Express.js route (/users) is created to handle\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePOST requests\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e. This route processes user data by first saving it into a MySQL database and then sending the same data to a Kafka topic. Below is a step-by-step explanation of how this route works:\u003c/span\u003e\u003c/p\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e1.\u0026nbsp;Endpoint Definition and Request Handling\u003c/strong\u003e\u003c/h4\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eThe app.post('/users', async (req, res) defines a route that listens for POST requests at the /users endpoint. It uses async/await to handle asynchronous operations such as database insertion and Kafka messaging. The req.body object is used to extract the data sent by the client in the request payload.const userData = req.body;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe\u0026nbsp;userData object contains the user-provided information, typically in JSON format. For example, it might include fields like\u0026nbsp;name. email and password.\u003c/span\u003e\u003c/p\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e2.\u0026nbsp;Inserting User Data into MySQL\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eTo store user data, the route uses a prepared SQL statement to prevent\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eSQL injection attacks\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e. The\u0026nbsp;\u003c/span\u003econnection.execute()\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;function interacts with the database, where\u0026nbsp;name, email and passwords fields are inserted into a\u0026nbsp;user table.\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003econst [result] = await connection.execute(\n    'INSERT INTO user (username, email, password) VALUES (?, ?, ?)',\n    [name, email, password]\n);\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePrepared Statement\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;? placeholders in the SQL query are replaced with actual values (name, email, password) safely.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eDeconstructed User Data\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;name,\u0026nbsp;email, and\u0026nbsp;password fields are extracted from the\u0026nbsp;userData object for better readability and security.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eResult\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;connection.execite()\u0026nbsp;method returns an array, where\u0026nbsp;result contains metadata about the operation, such as the number of rows affected.\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIf the operation succeeds, a log is generated to confirm that the user data was inserted into the database:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003econsole.log('User inserted into database:', result);\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e3.\u0026nbsp;Sending Data to a Kafka Topic\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eAfter successfully storing the data in MySQL, the route sends the same data to a Kafka topic for further processing. Kafka is often used to handle large-scale distributed messaging and stream processing.\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eawait producer.send({\n    topic: 'users-topic', // Replace with your topic\n    messages: [\n        { value: JSON.stringify(userData) },\n    ],\n});\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eKafka Producer\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003ep: The producer object is an instance of Kafka's producer client, which is responsible for sending messages to Kafka topics.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eTopic Name\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;topic field specifies the destination Kafka topic (user-topic in this case). This is where the message will be sent for further processing by Kafka consumers.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eMessage Payload\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;message array contains the data to be sent. Each message is an object with a\u0026nbsp;value field, which holds the serialized user data (converted to JSON using\u0026nbsp;JSON.stringfy(userData)).\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThis mechanism ensures that user data is available for other systems (e.g., analytics, logging, or notifications) in near real-time.\u003c/span\u003e\u003c/p\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e4.\u0026nbsp;Response to the Client\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eIf both the database insertion and Kafka message-sending steps succeed, the server sends a 201 Created response to the client with a success message:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eres.status(201).json({ message: 'User data inserted into database and sent to Kafka' });\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe 201 status code indicates that the request was successfully processed and a new resource was created.\u003c/p\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e5.\u0026nbsp;Error Handling\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eThe try-catch block ensures that errors during either database insertion or Kafka messaging are gracefully handled. If an error occurs, it is logged for debugging purposes, and the client receives a 500 Internal Server Error response:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ecatch (error) {\n    console.error('Error processing request:', error);\n    res.status(500).json({ message: 'Error processing request' });\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThis approach provides transparency to developers and prevents the application from crashing due to unhandled exceptions.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eAfter building the express.js service to insert data in mysql database and message broker. we will create the instance that listening on this producer.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePart 4: Explanation of Kafka Consumer with Elasticsearch Integration\u003c/strong\u003e\u003c/h2\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eimport { Kafka } from 'kafkajs';\nimport { Client } from '@elastic/elasticsearch';\n\n// Kafka consumer setup\nconst kafka = new Kafka({\n\u0026nbsp; \u0026nbsp; clientId: 'express-app',\n\u0026nbsp; \u0026nbsp; brokers: ['192.168.128.207:9092'], // Replace with your Kafka broker(s)\n});\n\nconst consumer = kafka.consumer({ groupId: 'user-group' });\n\n// Elasticsearch client setup\nconst esClient = new Client({\n\u0026nbsp; \u0026nbsp; node: 'http://localhost:9200', // Replace with your Elasticsearch URL\n});\n\n// Kafka consumer processing\nasync function consumeMessages() {\n\u0026nbsp; \u0026nbsp; await consumer.connect();\n\u0026nbsp; \u0026nbsp; await consumer.subscribe({ topic: 'users-topic', fromBeginning: true }); // Replace with your topic\n\n\u0026nbsp; \u0026nbsp; await consumer.run({\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; eachMessage: async ({ topic, partition, message }) =\u0026gt; {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; const userData = JSON.parse(message.value.toString());\n\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; // Insert data into Elasticsearch\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; try {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; const response = await esClient.index({\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; index: 'users', // The Elasticsearch index to use\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; id: message.name,\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; document: userData,\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; });\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.log('User data inserted into Elasticsearch:', response);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; } catch (error) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.error('Error inserting data into Elasticsearch:', error);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; },\n\u0026nbsp; \u0026nbsp; });\n}\n\nconsumeMessages().catch(console.error);\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis code demonstrates how to integrate Kafka as a messaging system and Elasticsearch as a search and data indexing tool in a Node.js application. The overall flow involves consuming messages from a Kafka topic and then indexing the received data into Elasticsearch for further use, such as querying or searching.\u003c/p\u003e\u003cp\u003eTo start, the script imports two essential libraries: KafkaJS and Elasticsearch client. KafkaJS is a JavaScript library used for interacting with Kafka, which is a distributed streaming platform. The Kafka client allows you to create consumers that can listen to Kafka topics and process the messages in real time. On the other hand, the Elasticsearch client facilitates communication with an Elasticsearch cluster, enabling the ability to store and index documents, which can later be queried or analyzed.\u003c/p\u003e\u003cp\u003eThe Kafka consumer is set up by first initializing the Kafka client with a unique clientId (express-app) and specifying the Kafka brokers. These brokers are the Kafka servers where the consumer will connect. The consumer is created with a groupId, which is user-group in this case. The group ID helps manage message consumption across multiple instances of the consumer. When consumers with the same group ID listen to a Kafka topic, Kafka ensures that each partition of the topic is assigned to only one consumer in the group, effectively balancing the load.\u003c/p\u003e\u003cp\u003eNext, the code sets up the Elasticsearch client by specifying the address of the Elasticsearch node (http://localhost:9200). This client will be used to interact with the Elasticsearch service where the user data will be indexed. Elasticsearch is widely used for its powerful search and analytics capabilities, which can handle large volumes of data and provide fast search results.\u003c/p\u003e\u003cp\u003eOnce both Kafka and Elasticsearch clients are set up, the consumeMessages() function is created to handle the actual logic of consuming messages from Kafka. This function first connects to the Kafka cluster and subscribes to the users-topic. By subscribing to the topic, the consumer listens for new messages that are published to that topic. The fromBeginning: true option ensures that the consumer starts processing messages from the very beginning of the topic‚Äôs log, meaning it will consume all the messages from when it first subscribes, not just new messages that arrive after it subscribes.\u003c/p\u003e\u003cp\u003eThe function then uses consumer.run() to begin consuming messages. Each message from the Kafka topic is processed in the eachMessage callback function. Inside this function, the message's value is parsed from a buffer into a JavaScript object (since Kafka messages are typically sent as binary data). The parsed data, which represents user information in this case, is then indexed into Elasticsearch. The esClient.index() method is used to insert this data into the users index in Elasticsearch. A unique identifier for the document is generated using the message's name field. This id ensures that each document in Elasticsearch can be uniquely identified.\u003c/p\u003e\u003cp\u003eIf the data insertion into Elasticsearch is successful, a response from Elasticsearch is logged to the console, confirming that the user data has been indexed. If an error occurs while inserting the data, the error is caught and logged.\u003c/p\u003e\u003cp\u003eFinally, the consumeMessages() function is invoked, and any unhandled errors are caught by console.error() to prevent the application from crashing. This ensures that the consumer will keep running and processing messages as they arrive, continuously feeding new data into Elasticsearch for indexing.\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePart 5: Wrapup what we have been doing\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe architecture discussed in this article aligns well with the CQRS (Command Query Responsibility Segregation) pattern, which is a powerful design pattern that separates the logic of reading data (queries) from the logic of writing data (commands). By implementing Kafka and Elasticsearch in conjunction with MySQL and Express.js, we create a robust system that effectively adheres to the principles of CQRS.\u003c/p\u003e\u003cp\u003eIn this architecture, the write operations (commands) are handled by the /users POST route in the Express.js service. When user data is received, it's inserted into the MySQL database and sent to Kafka. Kafka acts as the message bus, decoupling the data-writing process from the read operations and ensuring that data can be asynchronously processed and consumed by different systems or services.\u003c/p\u003e\u003cp\u003eThe read operations (queries) are efficiently handled by Elasticsearch. After the data is consumed from Kafka and indexed into Elasticsearch, it becomes readily available for fast and scalable querying. Elasticsearch's ability to index and search large volumes of data makes it an excellent fit for handling query-based operations in this architecture.\u003c/p\u003e\u003cp\u003eBy using CQRS, we ensure that the system is optimized for both reading and writing operations, enabling high scalability and responsiveness. Kafka, as the message broker, enables asynchronous communication and allows for horizontal scaling in the system. Meanwhile, Elasticsearch ensures that queries on user data are fast, efficient, and scalable.\u003c/p\u003e\u003cp\u003eThis CQRS-based approach also helps with performance optimization, as read and write concerns are handled separately. It allows for the scaling of each part of the system independently, depending on whether there is a higher load on reading or writing. The separation of concerns promotes better maintainability, flexibility, and scalability, making this architecture ideal for modern, high-traffic applications requiring real-time data processing and analytics.\u003c/p\u003e\u003cp\u003eIn conclusion, by integrating Kafka, Elasticsearch, and Express.js with a CQRS approach, this system architecture offers a scalable, maintainable, and highly performant solution for handling real-time data in applications where reading and writing data need to be optimized separately.\u003c/p\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fcqrs-background-title.png?alt=media\u0026token=dd34dffa-1cc4-4b14-b4e2-f6840555b0e4","image_alt":"Image cover of CQRS Architecture","short_description":"In this lab we will implement simple CQRS architecture pattern using apache kafka as a message broker, elastic search as a search service and mysql database as a command service.","timestamp":"2024-12-07 09:06:28","title":"Building a Simple CQRS Pattern Architecture"},{"blog_id":"e7d3943c-c12a-42c1-9663-2d90449138dc","description":"\u003cp\u003eMicro frontends are an architectural approach that applies the principles of microservices to frontend development. Instead of building a single, monolithic frontend application, the user interface is divided into smaller, independent pieces called \u003cstrong\u003emicro frontends\u003c/strong\u003e, each of which is developed, deployed, and maintained independently.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eThe Problem with Monolithic Frontends\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eA \u003cstrong\u003emonolithic frontend\u003c/strong\u003e refers to a single, large codebase that manages the entire user interface of an application. While this approach works well for small applications, it becomes increasingly challenging to manage and scale as the application and development teams grow. Below are the key problems associated with monolithic frontends:\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003e1. Scaling Teams\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCoordination Overhead\u003c/strong\u003e: In a large team, multiple developers work on the same codebase. This can lead to frequent merge conflicts, delayed pull requests, and dependency issues.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLimited Parallel Development\u003c/strong\u003e: Because the codebase is tightly coupled, teams cannot work independently on different parts of the application without stepping on each other's toes.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cstrong\u003e2. Slower Development Cycles\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSingle Deployment Pipeline\u003c/strong\u003e: In a monolithic frontend, all changes must pass through the same build and deployment pipeline. This means Small changes (e.g., fixing a typo) require deploying the entire application, also A single bug can block the entire release process.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLonger Testing Time\u003c/strong\u003e: The larger the application, the more time and effort it takes to ensure that new changes don‚Äôt break existing functionality.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cstrong\u003e3. High Risk of Changes\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRipple Effect\u003c/strong\u003e: Since everything is interconnected, even small changes in one part of the application can have unintended consequences elsewhere.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRollback Challenges\u003c/strong\u003e: If something goes wrong after deployment, rolling back requires reverting the entire application, not just the problematic component.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cstrong\u003e4. Tech Stack Lock-In\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDifficult to Adopt New Frameworks\u003c/strong\u003e: In a monolithic frontend, the entire application is built using a single framework or library. Upgrading or switching technologies is a monumental task that may require rewriting the entire application. \u003cstrong\u003eExample\u003c/strong\u003e: Migrating from AngularJS to React would involve significant effort and downtime.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNo Flexibility for Teams\u003c/strong\u003e: Teams must stick to the same tech stack, even if certain parts of the application would benefit from newer or more suitable tools.\u003c/li\u003e\u003c/ul\u003e\u003ch1\u003e\u003cstrong\u003eHow Micro Frontends Work\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eMicro frontends operate on the principle of \u003cstrong\u003edivide and conquer\u003c/strong\u003e. Instead of managing one gigantic codebase, you divide your application into smaller units.\u003c/p\u003e\u003cp\u003eThe idea behind micro frontends is simple yet powerful:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Divide\u003c/strong\u003e your application into smaller, self-contained pieces.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2. Conquer\u003c/strong\u003e by developing, testing, and deploying these pieces independently.\u003c/p\u003e\u003cp\u003eInstead of one massive codebase where every change has the potential to disrupt the entire application, you get a collection of smaller, focused units that can evolve at their own pace. Here is the breakdown of how divide and conquer works in microfrontend.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fmonolit_vs_microfrontend.png?alt=media\u0026amp;token=24eba3d8-b7f3-4674-b33e-62daf0517afd\" alt=\"Monolithic Vs Microfrontend\" width=\"720px\"\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eIn a traditional monolithic architecture (left side), all components of a web application are tightly coupled and deployed as a single unit. This includes the web application layer (frontend), integration layer (APIs), and service layer (backend).\u003c/p\u003e\u003cp\u003eMicrofrontends (right side) break down a large web application into smaller, independent frontend applications (also called \"micro-applications\" or \"micro-frontends\"). Each microfrontend is responsible for a specific feature or section of the application. This approach promotes modularity, scalability, and faster development cycles.\u003c/p\u003e\u003cp\u003eThe divide and conquer approach is central to microfrontends. It involves:\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eDividing the Application:\u003c/strong\u003e\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eIdentify Features:\u003c/strong\u003e First, break down the application into distinct features or sections. For example, in the image, we have \"Cart,\" \"Website,\" and \"Payment\" as separate features.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAssign Teams:\u003c/strong\u003e Each feature is assigned to an independent team. This allows teams to work autonomously, focusing on their specific feature.\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003e\u003cstrong\u003eConquering the Features:\u003c/strong\u003e\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eIndependent Development:\u003c/strong\u003e Each team develops its feature as a standalone frontend application. They can use different technologies (React, JAML, etc.) and frameworks as needed.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAPI Integration:\u003c/strong\u003e Teams define clear APIs for their microfrontends to communicate with each other and with the backend services. This ensures loose coupling and flexibility.\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003e\u003cstrong\u003eComposition and Orchestration:\u003c/strong\u003e\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eFrameworks and Tools:\u003c/strong\u003e A framework or library is used to combine the microfrontends into a cohesive user experience. This can be done using techniques like server-side composition (e.g., with a Node.js server) or client-side composition (e.g., with a JavaScript framework like Single-SPA).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRouting and Navigation:\u003c/strong\u003e The framework handles routing and navigation between microfrontends, ensuring a seamless user experience.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eShared Components:\u003c/strong\u003e If necessary, shared components can be developed and used across multiple microfrontends. This promotes consistency and reduces code duplication.\u003c/li\u003e\u003c/ol\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eMicro frontends represent a modern approach to building scalable, resilient, and maintainable frontend applications. By breaking down your application into smaller, independent pieces, you empower your teams to innovate faster and reduce deployment risks.. If you‚Äôve ever felt constrained by the limitations of a monolithic frontend, it might be time to explore the possibilities of micro frontends. They‚Äôre not just a technical solution‚Äîthey‚Äôre a way to rethink how we build for the web.\u003c/p\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fmicrofrontend.webp?alt=media\u0026token=6274c176-9622-4cd5-93be-9ea9e5912bd9","image_alt":"Microfrontend Image Cover","short_description":"Have you ever worked on a massive frontend application where every change felt risky and deploying updates took ages? If so, micro frontends might be the solution you‚Äôve been looking for","timestamp":"2024-12-04 11:23:45","title":"The concept of splitting a frontend into smaller, manageable pieces."},{"blog_id":"4a5ae2d7-0f8f-46b9-b49a-4ff130f22292","description":"\u003cdiv id=\"content-0\"\u003e\u003cp\u003eIn today's rapidly evolving technological landscape, applications are becoming increasingly complex and distributed. To manage this complexity and ensure high availability, reliability, and scalability, organizations are turning to Kubernetes. This powerful container orchestration platform has revolutionized the way we deploy and manage applications.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch1\u003eWhy Kubernetes?\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cp\u003e\u003cstrong\u003e1. Simplified Deployment and Management:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eAutomated Deployment:\u003c/strong\u003e Kubernetes automates the deployment process, eliminating manual intervention and reducing the risk of human error. With a few configuration changes, you can deploy complex applications to multiple environments with ease.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSelf-Healing:\u003c/strong\u003e Kubernetes can automatically detect and recover from failures, ensuring that your applications remain up and running. If a pod fails, Kubernetes will automatically restart it on a different node.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScalability:\u003c/strong\u003e You can easily scale your applications up or down to meet changing demand, without requiring significant manual effort. Whether it's a sudden traffic spike or a planned scaling event, Kubernetes can handle it seamlessly.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp\u003e\u003cstrong\u003e2. Efficient Resource Utilization:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eResource Allocation:\u003c/strong\u003e Kubernetes efficiently allocates resources (CPU, memory) to your applications, maximizing utilization and minimizing waste. It ensures that your applications get the resources they need, while avoiding overprovisioning.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic Scheduling:\u003c/strong\u003e It intelligently schedules pods onto nodes, optimizing resource allocation across the cluster. This ensures that your applications are always running on the most suitable nodes, regardless of their resource requirements.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e3. Enhanced Reliability and Availability:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh Availability:\u003c/strong\u003e Kubernetes ensures high availability by replicating your applications across multiple nodes, providing redundancy and fault tolerance. If one node fails, your application will continue to run on other nodes.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLoad Balancing:\u003c/strong\u003e It automatically distributes traffic across multiple instances of your application, improving performance and reliability. This ensures that no single instance is overwhelmed, and your users get a consistent experience.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e4. Increased Flexibility and Portability:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eContainer-Based:\u003c/strong\u003e Kubernetes is container-based, allowing you to package your applications and their dependencies into portable units. This makes it easy to move your applications between different environments, such as development, testing, and production.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePlatform Agnostic:\u003c/strong\u003e It can run on various infrastructure platforms, including public clouds (AWS, Azure, GCP), private clouds, and on-premises data centers. This gives you the flexibility to choose the best platform for your needs.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch1\u003eKubernetes Architecture\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkubernetes-architecture.png?alt=media\u0026token=cd32c87e-d584-4aec-a83f-f0c20d7d0f5c'/\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003eThis diagram provides a clear overview of the key components in a Kubernetes cluster and how they interact with each other. Let's break down each component:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eControl Plane\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eAPI Server:\u003c/strong\u003e The main entry point for all interactions with the cluster. All requests (e.g., creating a new pod, scaling a deployment) are sent to the API Server.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eetcd:\u003c/strong\u003e A distributed, consistent, highly-available key-value store that stores the entire cluster state. All information about pods, services, deployments, and more is stored here.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eController Manager:\u003c/strong\u003e Manages various controllers responsible for ensuring the cluster is in the desired state. Examples of controllers include Deployment Controller, ReplicaSet Controller, and Job Controller.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScheduler:\u003c/strong\u003e Responsible for scheduling pods to available nodes. It considers various factors like resource availability, affinities, and anti-affinities.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eNode\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eKubelet:\u003c/strong\u003e An agent that runs on each node. Kubelet ensures that the containers specified in pod manifests are running on the node.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ekube-proxy:\u003c/strong\u003e A network proxy that implements network rules for services and load balancing.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eComponent Interactions\u003c/strong\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eUser or Tool:\u003c/strong\u003e When you want to create or manage Kubernetes resources (e.g., using kubectl), you interact with the API Server.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAPI Server:\u003c/strong\u003e Receives the request, validates it, and stores it in etcd.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eController Manager:\u003c/strong\u003e Monitors changes in etcd and takes necessary actions. For example, if the number of replicas for a deployment doesn't match the desired state, the controller will create or delete pods.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScheduler:\u003c/strong\u003e When there's a new pod to be scheduled, the scheduler selects the most suitable node and informs the Kubelet.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eKubelet:\u003c/strong\u003e Receives information from the scheduler and starts running the pod's containers.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ekube-proxy:\u003c/strong\u003e Manages networking to ensure traffic is routed to the correct pods.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch1\u003e\u003cstrong\u003eDeployments: Scale, Update, Rollback\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eImagine you have a simple web application (e.g., a Node.js app) running in a Kubernetes cluster. The application is exposed via a Kubernetes Service, and you want to manage it using a Deployment. Here‚Äôs how you can implement this:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003ch2\u003e1. \u003cstrong\u003eCreating a Deployment\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFirst, you'll create a Deployment to manage your application.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eapiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-web-app\n  template:\n    metadata:\n      labels:\n        app: my-web-app\n    spec:\n      containers:\n        - name: my-web-app\n          image: myusername/my-web-app:1.0\n          ports:\n            - containerPort: 80\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eReplicas\u003c/strong\u003e: This specifies how many pods you want to run.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSelector\u003c/strong\u003e: This defines how to identify the pods managed by this Deployment.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTemplate\u003c/strong\u003e: This describes the pods that will be created.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eDeploying the Application\u003c/strong\u003e: Apply the Deployment with the following command:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl apply -f deployment.yaml\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003ch2\u003e2. \u003cstrong\u003eScaling the Application\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIf you want to handle increased traffic, you can scale your Deployment up or down.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScaling Up\u003c/strong\u003e: To increase the number of replicas to 5:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl scale deployment my-web-app --replicas=5\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003cp\u003eScaling Down: To decrease the number of replicas back to 3:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl scale deployment my-web-app --replicas=3\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003ch2\u003e3. \u003cstrong\u003eUpdating the Application\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWhen you want to update your application (for example, deploying a new version of the image), modify the Deployment:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003espec:\n  template:\n    spec:\n      containers:\n        - name: my-web-app\n          image: myusername/my-web-app:2.0 # Updated version\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cp\u003eApplying the Update: You can update the Deployment by reapplying the configuration:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl apply -f deployment.yaml\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-24\"\u003e\u003cp\u003eKubernetes will perform a rolling update, gradually replacing the old pods with new ones.\u003c/p\u003e\u003ch2\u003e4. \u003cstrong\u003eChecking the Update Status\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo monitor the status of the update, use:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-25\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl rollout status deployment/my-web-app\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-26\"\u003e\u003ch2\u003e5. \u003cstrong\u003eRolling Back an Update\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIf something goes wrong with the new version, you can roll back to the previous version easily:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-27\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl rollout undo deployment/my-web-app\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-28\"\u003e\u003cp\u003eTo check the history of the revisions, you can use:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-29\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl rollout history deployment/my-web-app\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-30\"\u003e\u003ch2\u003e6. \u003cstrong\u003eVerifying the Rollback\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAfter rolling back, you can verify that the previous version is running:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-31\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl get deployments\nkubectl describe deployment my-web-app\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-32\"\u003e\u003ch2\u003e7. \u003cstrong\u003eCreating a Service\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eA Kubernetes \u003cstrong\u003eService\u003c/strong\u003e is used to expose your application, making it accessible from outside the cluster (or within, depending on your requirements). Here's an example of a Service configuration:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-33\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eapiVersion: v1\nkind: Service\nmetadata:\n  name: my-web-app-service\nspec:\n  selector:\n    app: my-web-app\n  ports:\n    - protocol: TCP\n      port: 80       # Port on the Service\n      targetPort: 80 # Port on the container\n  type: LoadBalancer\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-34\"\u003e\u003ch1\u003e\u003cstrong\u003eIn Conclusion:\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eKubernetes architecture is designed to simplify the management of large-scale containerized applications. By understanding its components and interactions, you can effectively leverage Kubernetes to build reliable and scalable applications.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FKubernetes-logo-1024x576.png?alt=media\u0026token=e6f56ef1-e429-4d8f-9597-2d5a01023cf9","image_alt":"Kubernetes Logo","short_description":"In today's fast-paced digital landscape, applications are becoming increasingly complex and distributed. To manage this complexity and ensure high availability, reliability, and scalability, organizations are turning to Kubernetes. This powerful container orchestration platform has revolutionized the way we deploy and manage applications.","timestamp":"2024-11-02 23:48:06","title":"Why You Need Kubernetes: A Comprehensive Guide"},{"blog_id":"6b4113f2-f30c-4e12-a34a-f5c02abbd1cb","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eSpark Architecture\u003c/h1\u003e\u003cp\u003eImagine a Spark application as a bustling city. At the heart of this city is the \u003cstrong\u003eDriver Program\u003c/strong\u003e, which acts like the mayor overseeing everything that happens. The driver program is responsible for running your code, coordinating work, and making key decisions about how tasks should be executed. Like a city planner, it organizes and manages the tasks to be performed by the cluster, breaking down large jobs into smaller units of work. These jobs are then divided into \u003cstrong\u003etasks\u003c/strong\u003e that Spark can run in parallel. The beauty of this architecture lies in its ability to scale, allowing multiple tasks to be completed simultaneously on different data partitions. These tasks are dispatched to \u003cstrong\u003eexecutors\u003c/strong\u003e, which are the hard workers of the cluster, doing the heavy lifting.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FApache-Spark-architecture.png?alt=media\u0026token=ae8b882a-a359-4e63-aad6-a9a9f0a06ff8'/\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cp\u003eExecutors are independent workers spread across the cluster, each taking responsibility for a portion of the work. They are not just performing tasks but are also responsible for caching data, which can speed up future computations by reusing cached data rather than starting from scratch. Much like how a construction team works efficiently by reusing tools and materials at the job site, executors are optimized to perform computations without redundant effort.\u003c/p\u003e\u003cp\u003eTo help you better visualize this, think of a project that requires breaking down and shipping parts to different locations. Each part can be processed independently and reassembled once all pieces are completed. The driver program ensures these tasks are coordinated correctly, and executors handle each piece of the job.\u003c/p\u003e\u003cp\u003eBut what‚Äôs the relationship between \u003cstrong\u003ejobs\u003c/strong\u003e, \u003cstrong\u003etasks\u003c/strong\u003e, and \u003cstrong\u003estages\u003c/strong\u003e? Imagine you're the head of a large construction project, and you need to break down the overall project (the \u003cstrong\u003ejob\u003c/strong\u003e) into manageable tasks for each construction team. A \u003cstrong\u003etask\u003c/strong\u003e in Spark operates on a specific partition of data, much like a construction team focusing on a particular section of the building. Once a group of tasks that don‚Äôt depend on any other data is identified, Spark bundles them into \u003cstrong\u003estages\u003c/strong\u003e. A stage represents a set of tasks that can be executed independently without needing data from elsewhere in the project. However, sometimes a task will need information from another part of the dataset, requiring what‚Äôs known as a \u003cstrong\u003edata shuffle\u003c/strong\u003e. This shuffle is like coordinating deliveries between different construction teams‚Äîan operation that can slow things down due to the necessary data exchange, but essential for the overall completion of the job.\u003c/p\u003e\u003cp\u003eWhat happens when Spark needs to run in different environments? That‚Äôs where \u003cstrong\u003ecluster modes\u003c/strong\u003e come in. There are several ways Spark can be deployed, each suited to different use cases. The simplest is \u003cstrong\u003elocal mode\u003c/strong\u003e, ideal for testing on your own machine. Imagine local mode as running your city‚Äôs planning department with just one employee‚Äîthe driver program manages everything on its own, without help from external workers. This is great for testing, but when you need real performance, it‚Äôs time to deploy Spark in a cluster.\u003c/p\u003e\u003cp\u003eIn a full cluster setup, Spark supports several modes. \u003cstrong\u003eSpark Standalone\u003c/strong\u003e is the quickest way to set up a cluster environment, ideal for smaller projects or when you want complete control over the infrastructure. For those already working in large-scale environments with Hadoop, \u003cstrong\u003eYARN\u003c/strong\u003e is a natural choice. It integrates seamlessly with the broader Hadoop ecosystem, making it easy to manage resources. For greater flexibility and the ability to handle dynamic workloads, \u003cstrong\u003eApache Mesos\u003c/strong\u003e comes into play, providing a more robust partitioning and scaling system.\u003c/p\u003e\u003cp\u003eBut if you‚Äôre looking for a modern, cloud-native approach, \u003cstrong\u003eKubernetes\u003c/strong\u003e offers powerful benefits. Running Spark on Kubernetes is like managing a city that can grow or shrink as needed, using containers to deploy and scale your Spark applications. With Kubernetes, Spark becomes highly portable, making it easy to run your Spark jobs in any cloud environment. You can set up your Spark application inside containers and have them scale automatically based on demand, ensuring smooth processing even as workloads increase.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch1\u003eMastering the Art of Running Apache Spark Applications\u003c/h1\u003e\u003cp\u003eEver wondered how to launch your Apache Spark application into action? Whether you‚Äôre processing mountains of data or just running local tests, Apache Spark‚Äôs flexibility makes it incredibly powerful. But let‚Äôs be real‚Äîunderstanding how to run a Spark application might seem daunting at first. Fear not! Here‚Äôs your step-by-step guide to Spark mastery.\u003c/p\u003e\u003cp\u003eAt the heart of it all is the \u003ccode\u003espark-submit\u003c/code\u003e script. Think of it as Spark‚Äôs personal conductor, ensuring your application runs smoothly across a distributed cluster or right on your local machine. With \u003ccode\u003espark-submit\u003c/code\u003e, you‚Äôve got full control: it lets you specify everything from the cluster manager you want to connect to, to how much memory and CPU cores your application needs. You can also include any additional files or libraries your app requires, making sure all the pieces are in place for a flawless run.\u003c/p\u003e\u003cp\u003eNow, let‚Äôs talk dependencies. In Spark, making sure the driver and executors have access to the right libraries is crucial. If you‚Äôre using Java or Scala, bundling all your code and libraries into a single uber-JAR (or fat JAR) is a common approach. This neat package ensures that everything is shipped out and accessible where it‚Äôs needed. For Python applications‚Äîaka PySpark‚Äîyou‚Äôll want to ensure that each node in your cluster has the exact same Python libraries installed. Imagine trying to run a marathon with mismatched shoes‚Äîit won‚Äôt end well, right? Same idea with your Spark dependencies.\u003c/p\u003e\u003cp\u003eIf you‚Äôre in the mood for a more hands-on, experimental approach, then the \u003cstrong\u003eSpark Shell\u003c/strong\u003e is your playground. This interactive tool lets you dive right into Spark with either Scala or Python, without needing to write and submit an entire application. When you fire up the Spark Shell, it automatically sets up everything for you‚Äîgiving you instant access to Spark‚Äôs APIs. You can run quick computations, play around with datasets, and see the results in real-time, making it perfect for debugging or just satisfying your curiosity.\u003c/p\u003e\u003cp\u003eSo, to sum it all up: running an Apache Spark application is as easy as using \u003ccode\u003espark-submit\u003c/code\u003e to launch your code, bundling your dependencies into an uber-JAR (or ensuring Python libraries are ready), and‚Äîif you're feeling adventurous‚Äîjumping into the Spark Shell for some interactive magic. Spark truly puts the power of distributed computing at your fingertips.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003eWe now trying to submit Apache Spark applications from a python script. This exercise is straightforward thanks to Docker Compose. In this lab, you will:\u003c/p\u003e\u003cul\u003e\u003cli\u003eInstall a Spark Master and Worker using Docker Compose\u003c/li\u003e\u003cli\u003eCreate a python script containing a spark job\u003c/li\u003e\u003cli\u003eSubmit the job to the cluster directly from python (Note: you‚Äôll learn how to submit a job from the command line in the Kubernetes Lab)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch2\u003eInstall a Apache Spark cluster using Docker Compose\u003c/h2\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003egit clone https://github.com/big-data-europe/docker-spark\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cp\u003echange to that directory and attempt to docker-compose up\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ecd docker-spark\ndocker-compose up\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cp\u003eAfter quite some time you should see the following message:\u003c/p\u003e\u003cp\u003e\u003cem\u003eSuccessfully registered with master spark://\u0026lt;server address\u0026gt;:7077\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003ch2\u003eCreate Code\u003c/h2\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport findspark\nfindspark.init()\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField, StructType, IntegerType, StringType\nsc = SparkContext.getOrCreate(SparkConf().setMaster('spark://localhost:7077'))\nsc.setLogLevel(\"INFO\")\nspark = SparkSession.builder.getOrCreate()\nspark = SparkSession.builder.getOrCreate()\ndf = spark.createDataFrame(\n    [\n        (1, \"foo\"),\n        (2, \"bar\"),\n    ],\n    StructType(\n        [\n            StructField(\"id\", IntegerType(), False),\n            StructField(\"txt\", StringType(), False),\n        ]\n    ),\n)\nprint(df.dtypes)\ndf.show()\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003ch2\u003e\u003cstrong\u003eExecute code / submit Spark job\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eNow we execute the python file we saved earlier.\u003c/p\u003e\u003cp\u003eIn the terminal, run the following commands to upgrade the pip installer to ensure you have the latest version by running the following commands.Now we execute the python file we saved earlier.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003erm -r ~/.cache/pip/selfcheck/\npip3 install --upgrade pip\npip install --upgrade distro-info\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003cp\u003ePlease enter the following commands in the terminal to download the spark environment.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ewget https://archive.apache.org/dist/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz \n\u0026\u0026 tar xf spark-3.3.3-bin-hadoop3.tgz \u0026\u0026 rm -rf spark-3.3.3-bin-hadoop3.tgz\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cp\u003eRun the following commands to set up the\u0026nbsp;\u0026nbsp;which is preinstalled in the environment and\u0026nbsp;\u0026nbsp;which you just downloaded.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eexport JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64\nexport SPARK_HOME=/home/project/spark-3.3.3-bin-hadoop3\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003cp\u003eInstall the required packages to set up the spark environment.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-24\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003epip install pyspark\npython3 -m pip install findspark\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-25\"\u003e\u003cp\u003eType in the following command in the terminal to execute the Python script.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-26\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003epython3 submit.py\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-27\"\u003e\u003cp\u003ego to port 8080 to see the admin UI of the Spark master\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-28\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fdemo-spark-submit.png?alt=media\u0026token=b8fe9a3e-6bcf-42f4-82a7-2b1842b11e74'/\u003e\u003c/div\u003e\u003cdiv id=\"content-29\"\u003e\u003cp\u003e\u003cstrong\u003eLook at that!\u003c/strong\u003e You can now see all your registered workers (we‚Äôve got one for now) and the jobs you‚Äôve submitted (just one at the moment) through Spark's slick interface. Want to dig even deeper? You can access the worker‚Äôs UI by heading to port 8081 and see what‚Äôs happening under the hood!\u003c/p\u003e\u003cp\u003eIn this hands-on lab, you've set up your very own experimental Apache Spark cluster using Docker Compose. How cool is that? Now, you can easily submit Spark jobs directly from your Python code like a pro!\u0026nbsp;\u003c/p\u003e\u003cp\u003eBut wait, there‚Äôs more! In our next adventure‚Äîthe Kubernetes lab‚Äîyou‚Äôll unlock the power of submitting Spark jobs right from the command line.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fspark.png?alt=media\u0026token=7a343f5c-0174-4a31-99e1-4943f9e135af","image_alt":"Apache Spark","short_description":"Welcome to an in-depth exploration of Apache Spark‚Äôs architecture! Whether you‚Äôre new to Spark or looking to refresh your understanding, this interactive guide will walk you through the key concepts that power Spark‚Äôs ability to process massive datasets quickly and efficiently.","timestamp":"2024-10-07 05:37:09","title":"Mastering Apache Spark: An Engaging Dive into Its Architecture and Clusters"},{"blog_id":"9f224db5-a76d-4a6f-9f7b-32bfbdf5696f","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eMapReduce: Powering Big Data Processing in Hadoop\u003c/h1\u003e\u003cp\u003eIn the world of Big Data, \u003cstrong\u003eMapReduce\u003c/strong\u003e stands as one of the key mechanisms for efficiently processing enormous datasets across distributed systems. It enables parallel processing of data across multiple computers within a cluster, making data handling at scale fast and reliable.\u003c/p\u003e\u003cp\u003eLet‚Äôs make it easier to grasp and dive into how MapReduce works in a more engaging, reader-friendly way.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch2\u003eWhat Exactly is MapReduce?\u003c/h2\u003e\u003cp\u003eImagine you have a huge task that needs to be done, but you can split it up among several friends. Each of them works on a piece of the task, and when they‚Äôre done, someone else comes along to gather all the pieces, combine them, and finish the job. That‚Äôs essentially how MapReduce works in Hadoop.\u003c/p\u003e\u003cp\u003eIn simple terms:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eMap\u003c/strong\u003e: The data gets divided into smaller, manageable pieces, and each piece is processed independently. The goal here is to transform data into key-value pairs.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eReduce\u003c/strong\u003e: After the data is organized, the pieces with the same key are grouped together. Then, the reduce function steps in to summarize or aggregate the data to produce the final output.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThis parallel processing is what makes MapReduce so powerful in handling large datasets efficiently\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003ch2\u003eSetting the Right Number of Mappers and Reducers\u003c/h2\u003e\u003cp\u003eWhile Hadoop does a lot of the work for you, it‚Äôs useful to know how mappers and reducers are assigned, because it can directly affect performance.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eHow Many Mappers?\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eAutomatically\u003c/strong\u003e: Hadoop typically assigns one mapper per block of data in HDFS (Hadoop Distributed File System). Each block is handled by one mapper.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eManually\u003c/strong\u003e: If needed, you can adjust this manually by configuring the \u003ccode\u003emapreduce.job.maps\u003c/code\u003e parameter in Hadoop settings. For example, if you want smaller or larger chunks of data per mapper, this is where you can tweak it.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e\u003cstrong\u003eHow Many Reducers?\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eManual Configuration\u003c/strong\u003e: Reducers are usually set by you, the developer, when writing the MapReduce job. You specify how many reducers are needed with the \u003ccode\u003emapreduce.job.reduces\u003c/code\u003e setting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic Adjustment\u003c/strong\u003e: Sometimes, Hadoop can adjust the number of reducers based on the data size, but setting it manually is common for optimizing performance.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eMore mappers or reducers can mean more parallel processing, but it‚Äôs important to find the right balance to avoid bottlenecks during the shuffle and sort phase (when data is grouped before reduction).\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch2\u003eThe MapReduce Workflow in Action\u003c/h2\u003e\u003cp\u003eLet‚Äôs look at a real-life example of how MapReduce works, step-by-step, using Hadoop. Let‚Äôs say you want to process a file called \u003ccode\u003ejar.txt\u003c/code\u003e. Here‚Äôs how it works:\u003c/p\u003e\u003ch3\u003e1. \u003cstrong\u003eInput Data to HDFS\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eYou (the client) upload the file \u003ccode\u003ejar.txt\u003c/code\u003e to Hadoop‚Äôs distributed file system (HDFS). Simple enough, but this is where the magic starts.\u003c/p\u003e\u003ch3\u003e2. \u003cstrong\u003eJobTracker \u0026amp; NameNode ‚Äì The Commanders\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe \u003cstrong\u003eJobTracker\u003c/strong\u003e (which coordinates all MapReduce jobs) reaches out to the \u003cstrong\u003eNameNode\u003c/strong\u003e (the master of HDFS) to find out where the file is stored. NameNode provides the metadata, explaining how the file will be divided into blocks and stored across DataNodes.\u003c/p\u003e\u003ch3\u003e3. \u003cstrong\u003eData Replication for Safety\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFor reliability, Hadoop replicates each block of data across multiple DataNodes. So, your file doesn‚Äôt just exist in one place‚Äîit‚Äôs copied to ensure that, even if one node goes down, your data is still accessible.\u003c/p\u003e\u003ch3\u003e4. \u003cstrong\u003eTaskTracker \u0026amp; Block Assignment\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eEach block of data is assigned to a \u003cstrong\u003eTaskTracker\u003c/strong\u003e running on a DataNode. These TaskTrackers are responsible for managing the mappers that will process the blocks. It‚Äôs like sending out your team of workers to handle different pieces of the puzzle.\u003c/p\u003e\u003ch3\u003e5. \u003cstrong\u003eMap Task ‚Äì Processing Begins\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe TaskTrackers execute the map phase by processing the data blocks in parallel. Each mapper works on its own piece of data, transforming it into key-value pairs.\u003c/p\u003e\u003ch3\u003e6. \u003cstrong\u003eIntermediate Results Stored Locally\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOnce the map phase is complete, the intermediate results are saved locally on each DataNode. These results aren‚Äôt final yet‚Äîthey still need to be combined.\u003c/p\u003e\u003ch3\u003e7. \u003cstrong\u003eReduce Task ‚Äì Bringing It All Together\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eNow, the \u003cstrong\u003eReduce\u003c/strong\u003e phase begins. TaskTrackers running the reduce task gather the intermediate results from all mappers. They group the data by key and aggregate it to produce the final output.\u003c/p\u003e\u003ch3\u003e8. \u003cstrong\u003eFinal Output Stored in HDFS\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOnce the reducers finish their job, the final output is written back to HDFS. This means your processed data is now available in the distributed file system, ready for use.\u003c/p\u003e\u003ch3\u003e9. \u003cstrong\u003eJob Completion ‚Äì Success!\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFinally, the JobTracker informs you that the job is complete, and you can access the results in HDFS. Mission accomplished!\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch2\u003eWhy Should You Care About MapReduce?\u003c/h2\u003e\u003cp\u003eMapReduce simplifies the complex task of processing huge datasets by breaking it into smaller chunks and handling everything in parallel. It‚Äôs incredibly efficient and, thanks to data replication, fault-tolerant. Even if a node fails, your job won‚Äôt crash‚Äîthe data is safe, and the job continues on other nodes.\u003c/p\u003e\u003cp\u003eSo, the next time you‚Äôre dealing with a massive dataset, just remember‚ÄîMapReduce is like having an army of helpers, all working together to get the job done fast!\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fmap_reduce.webp?alt=media\u0026token=9675cfce-94c9-4495-9f54-d6f651de19ff","image_alt":"Map reduce","short_description":"Ever wondered how companies handle mountains of data efficiently? Enter MapReduce‚ÄîHadoop‚Äôs superhero when it comes to processing large datasets. Instead of one machine trying to handle everything, MapReduce breaks the work into smaller chunks and distributes it across many machines, making the process faster and more reliable.","timestamp":"2024-09-29 05:50:56","title":"MapReduce: The Magic Behind Processing Big Data in Hadoop"},{"blog_id":"2418e501-daff-473e-b29c-81ba9d65d596","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eHow HDFS Stands Out from Traditional File Systems\u003c/h1\u003e\u003cp\u003eHave you ever wondered how Hadoop handles those massive datasets? It‚Äôs not quite like how your typical computer saves files using NTFS or FAT32. Let's break it down!\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch1\u003eWhat is HDFS?\u003c/h1\u003e\u003cp\u003eHDFS, or Hadoop Distributed File System, is the backbone of Hadoop. It‚Äôs specially built to handle huge volumes of data by spreading it across multiple machines, making it perfect for big data tasks. If you‚Äôve got terabytes or even petabytes of information, HDFS can manage it efficiently.\u003c/p\u003e\u003cp\u003eUnlike regular file systems like \u003cstrong\u003eNTFS\u003c/strong\u003e (which you might find on your Windows laptop), HDFS isn't just about saving files locally. Instead, it breaks your files into chunks (called blocks), and then spreads them out over several machines. This allows Hadoop to process multiple pieces of data at the same time. Pretty neat, right?\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fblock_and_replication.jpg?alt=media\u0026token=7c465fa0-f2ed-444d-814a-1ab9c3e7c944'/\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003ch1\u003eThe Magic of HDFS: Why Blocks and Replication Matter\u003c/h1\u003e\u003cp\u003eHere‚Äôs where HDFS gets really clever: it doesn‚Äôt just save your data in one place. It cuts files into large blocks, then duplicates (or replicates) those blocks across several machines. Why? So that if one machine crashes, your data isn‚Äôt lost. It‚Äôs all about fault tolerance.\u003c/p\u003e\u003cp\u003eThink of it like this: Imagine you‚Äôve got a giant puzzle, but instead of keeping all the pieces in one box, you spread them across three different boxes. That way, even if you lose one box, you‚Äôve still got enough pieces in the other two to complete the picture. That‚Äôs how replication in HDFS works. Each block is typically copied three times (though you can change that number if you need more or fewer backups).\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch1\u003eComparing HDFS to NTFS\u003c/h1\u003e\u003cp\u003eNow, let‚Äôs compare this to NTFS. NTFS is a file system that works great for smaller files and doesn‚Äôt need to worry about distributing data over a network. When you save a file on your laptop, NTFS does its job locally‚Äîno fancy distribution happening here. If your hard drive fails, well, you better hope you‚Äôve got a backup.\u003c/p\u003e\u003cp\u003eOn the other hand, HDFS shines in situations where you're dealing with Big Data. Imagine trying to process a massive log file for an entire year across multiple servers. That‚Äôs when HDFS steps in to spread the workload across dozens or even hundreds of machines, making everything faster and more reliable.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch2\u003eHow Clusters Work in HDFS\u003c/h2\u003e\u003cp\u003eHDFS operates on a \u003cstrong\u003emaster-slave architecture\u003c/strong\u003e, which, despite the name, is simpler than it sounds. Picture a group project where one person (the master) assigns tasks, and the rest (the slaves) carry them out. In Hadoop‚Äôs case, the \u003cstrong\u003eNameNode\u003c/strong\u003e is the master that keeps track of which machines (or \u003cstrong\u003eDataNodes\u003c/strong\u003e) are storing which blocks of data.\u003c/p\u003e\u003cp\u003eBut here's the cool part: even if one of the DataNodes goes offline, you won't lose your data. Thanks to replication, other copies of the data are safe on different nodes. The NameNode makes sure that if something goes wrong, the system automatically reassigns tasks to healthy machines. So, your project continues without missing a beat.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003ch2\u003eBlocks and Replication: The Heart of HDFS\u003c/h2\u003e\u003cp\u003eWhen you upload a file to HDFS, it doesn‚Äôt store the file in one piece. Instead, it breaks it into blocks‚Äîbig ones, usually around 128 MB or 256 MB each (way bigger than what you'd see in NTFS or FAT32). These blocks are then distributed across multiple machines, and each block is typically replicated three times, just in case something goes wrong.\u003c/p\u003e\u003cp\u003eLet‚Äôs say you‚Äôve uploaded a 1 GB file. HDFS would split that file into eight blocks, each around 128 MB. Then, it spreads these blocks out, saving them on different machines. And, for good measure, it replicates each block three times, storing those copies on different nodes. So even if one machine dies, the system still has two copies of every block.\u003c/p\u003e\u003cp\u003eThis is what makes HDFS so powerful‚Äîby distributing both the data and the risk of failure, it allows Hadoop to manage enormous datasets without constantly worrying about losing data due to hardware issues.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003ch2\u003eWhat Happens When You Write Data to HDFS?\u003c/h2\u003e\u003cp\u003eHere‚Äôs how the process goes when you write data to HDFS:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eRequest\u003c/strong\u003e: You, or an application, send a request to HDFS to save some data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCommunication\u003c/strong\u003e: HDFS, through the \u003cstrong\u003eNameNode\u003c/strong\u003e, checks where your data should be stored. It‚Äôs like asking the master project manager where to put each piece of the puzzle.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eBreaking into Blocks\u003c/strong\u003e: The data is then broken down into blocks and sent to different DataNodes, which store those blocks.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eReplication\u003c/strong\u003e: As each block gets stored, it's also replicated across other nodes. If one copy goes missing, the system can recreate it from another.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eConfirmation\u003c/strong\u003e: Once all blocks are safely stored and replicated, HDFS sends a confirmation, saying, ‚ÄúAll good! Your data‚Äôs safely stored.‚Äù\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThis process happens behind the scenes, but it's crucial to keeping your data safe and your big data applications running smoothly.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003ch2\u003eWhat Happens When a DataNode Fails in HDFS?\u003c/h2\u003e\u003cp\u003eImagine you're managing a vast library, and each book is split into multiple copies stored across different branches to ensure none of the information is lost. Now, what if one of the branches burns down? You wouldn't panic, right? Why? Because the other branches still hold copies. This is pretty much how Hadoop handles DataNode failures.\u003c/p\u003e\u003cp\u003eDataNodes store chunks of data, and it's inevitable that sometimes, things will go wrong. Maybe a server breaks down, or there's a network issue. But here‚Äôs the cool part: Hadoop has it all covered.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003ch3\u003eHow Does Hadoop Know a DataNode Failed?\u003c/h3\u003e\u003cp\u003eThink of it like a regular check-in system. Every few seconds, each DataNode sends a simple signal to the \u003cstrong\u003eNameNode\u003c/strong\u003e (the master controller) saying, ‚ÄúHey, I‚Äôm here, and I‚Äôm working!‚Äù These signals are called \u003cstrong\u003eheartbeats\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eBut what if the NameNode doesn‚Äôt hear from a DataNode? If it misses several heartbeats in a row, the NameNode starts to get suspicious. After a certain time, it officially declares, ‚ÄúThis DataNode is down!‚Äù\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003ch3\u003eWhat Happens Next?\u003c/h3\u003e\u003cp\u003eNow, this is where Hadoop shows off its resilience. The moment a DataNode fails, the NameNode updates its metadata. Basically, it marks all the data blocks that were on that failed node as unavailable.\u003c/p\u003e\u003cp\u003eBut remember‚ÄîHadoop doesn‚Äôt store just one copy of your data. It replicates blocks across multiple DataNodes. So, even though one DataNode has failed, the same data is safely stored on other DataNodes. The NameNode immediately gets to work, ensuring that all data remains fully backed up. It checks if any data blocks have dropped below the desired number of replicas (usually 3), and if so, it orders other healthy DataNodes to make fresh copies.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003ch3\u003eBringing a DataNode Back to Life\u003c/h3\u003e\u003cp\u003eLet‚Äôs say the failed DataNode eventually comes back online. It‚Äôs not like Hadoop just forgets about it. First, the NameNode checks whether the data stored there is still relevant. If the system has already made new replicas of those blocks, it might decide to remove the old, now-duplicated data to free up space.\u003c/p\u003e\u003cp\u003eOnce the DataNode starts sending its heartbeats again, it gets reintegrated into the system, and life goes on as if nothing happened. It‚Äôs like having a team member who took a break and is now ready to get back to work!\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cp\u003eIn a nutshell, when a DataNode fails, Hadoop's architecture ensures there's no reason to worry. The NameNode quickly detects the failure, handles it behind the scenes by replicating data to other nodes, and everything continues running smoothly without missing a beat! Pretty smart, right?\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003ch2\u003eWrapping Up\u003c/h2\u003e\u003cp\u003eHDFS might seem complex at first, but once you break it down, it‚Äôs really all about efficiency and reliability. By splitting files into blocks, distributing them across multiple machines, and replicating them for safety, HDFS ensures that your data is always accessible‚Äîeven when things go wrong. So, if you're working with big datasets, HDFS is the perfect solution to keep everything running like clockwork.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fhdfs.jpg?alt=media\u0026token=b609034b-d178-4287-8ed3-4efb0dfa3249","image_alt":"HDFS","short_description":"HDFS, or Hadoop Distributed File System, is the backbone of Hadoop. It‚Äôs specially built to handle huge volumes of data by spreading it across multiple machines, making it perfect for big data tasks.","timestamp":"2024-09-29 05:29:46","title":"Understanding Hadoop Distributed File System (HDFS)"},{"blog_id":"7a76b44a-8bc3-451e-95f0-3ccb1bc23e34","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eWhat is Electron.js?\u003c/h1\u003e\u003cp\u003eElectron.js is an open-source framework that allows developers to build cross-platform desktop applications (Windows, macOS, and Linux) using web technologies like HTML, CSS, and JavaScript. Electron combines Chromium and Node.js so that developers can build desktop applications using the same technology they use for web development. I will discussed about the Modash project that i created too. So other people can use this template and ready to be launch as a producition app. This Modash project using the weird stack such as electron.js, React, Vite and IndexDb. i will tell you why i use this stack.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch1\u003eWhy Use React and Vite?\u003c/h1\u003e\u003cp\u003eReact is a very popular JavaScript library for building user interfaces (UIs). By using React, we can create dynamic and interactive UI components and Vite is a build tool that provides speed and efficiency in frontend project development. Vite offers a fast development experience with hot module replacement (HMR) and a very fast build process.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch1\u003eWhy IndexedDB?\u003c/h1\u003e\u003cp\u003eIndexedDB is a web-based database storage solution that provides larger storage compared to LocalStorage and SessionStorage. IndexedDB is ideal for applications that require large data storage on the client side, such as desktop applications built with Electron.js. we can use other database instead of this indexDb in my template. but for a default i used this indexDB to manage my account login. this is not  a good practice because indexDb can be access trough the Web interface and directly clear the dabatase data. but this can be prevent in prouction by blocking the user to inspect the devtools. Here is the code to prevent user inspect and ruined their production app:\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e if (process.env.NODE_ENV === \"development\") {\n    win.loadURL(\"http://localhost:6969/\");\n    win.webContents.openDevTools();\n  } else {\n    win.loadFile(path.join(__dirname, \"./dist/index.html\"));\n\n    // Remove the menu (including DevTools shortcuts)\n    win.setMenu(null);\n\n    // Disable context menu (right-click) that might have \"Inspect Element\"\n    win.webContents.on('context-menu', (e) =\u0026gt; {\n      e.preventDefault();\n    });\n\n    // Optionally, prevent any manual attempts to open DevTools\n    win.webContents.on('before-input-event', (event, input) =\u0026gt; {\n      if (input.key === 'I' \u0026\u0026 input.control \u0026\u0026 input.shift) {\n        event.preventDefault();\n      }\n    });\n\n    // Make sure DevTools are closed\n    win.webContents.on('did-finish-load', () =\u0026gt; {\n      win.webContents.closeDevTools();\n    });\n  }\n};\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp\u003eThis code snippet is part of an Electron application setup and handles different behaviors based on whether the application is running in development or production mode. In development mode, it loads the application from a local server (\u003ccode\u003ehttp://localhost:6969/\u003c/code\u003e) and opens the Developer Tools for debugging purposes. On the other hand, in production mode, it loads the application from a local HTML file, disables the menu, prevents the context menu from appearing (which might include the \"Inspect Element\" option), and disables the shortcut (\u003ccode\u003eCtrl+Shift+I\u003c/code\u003e) for opening the Developer Tools. Additionally, it ensures that the Developer Tools are closed after the application finishes loading, contributing to a cleaner user interface and enhanced security.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch1\u003eModash\u003c/h1\u003e\u003cp\u003eThis template is used to build the Dashboard application such as inventory list. sales, etc. the default template is using the React router dom for handling and manage the page. the starter kit for css framework i used is a Chakra-UI. You can visit this site to read Chakra-UI docs. \u003ca href=\"https://v2.chakra-ui.com/\" target=\"_blank\"\u003ehttps://v2.chakra-ui.com/\u003c/a\u003e. in this template i already built Home page. and the settings page. You can tweak and modify freely as you need if you want to build the dashboard app using this template. Here is the link of my github repository \u003ca href=\"https://github.com/Barbarpotato/Modash\" target=\"_blank\"\u003ehttps://github.com/Barbarpotato/Modash\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch1\u003eElectron Problem with Production\u003c/h1\u003e\u003cp\u003eSometimes Electron.js has some trouble to delivered as a production app if you used the electron.js and integrating it wirh React environment. i have face some problems in my journey such as error when production if i sed react-router-dom, etc. to prevent react-router-dom crash during a production, you can use the \u0026lt;HashRouter\u0026gt; instead of \u0026lt;Browser Router\u0026gt;. this will solve the issue in electron production app. Here is the example code:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport React from 'react'\nimport ReactDOM from 'react-dom/client'\nimport { HashRouter } from 'react-router-dom'\nimport { ChakraProvider } from '@chakra-ui/react'\nimport theme from './theme/theme.js'\nimport App from './App.jsx'\nimport './index.css'\nimport { AuthProvider } from './hooks/useAuth.jsx'\n\nReactDOM.createRoot(document.getElementById('root')).render(\n  \u0026lt;ChakraProvider theme={theme}\u0026gt;\n    \u0026lt;HashRouter\u0026gt;\n      \u0026lt;AuthProvider\u0026gt;\n        \u0026lt;App /\u0026gt;\n      \u0026lt;/AuthProvider\u0026gt;\n    \u0026lt;/HashRouter\u0026gt;\n  \u0026lt;/ChakraProvider\u0026gt;\n)\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003e\u003ccode\u003eHashRouter\u003c/code\u003e is a type of router provided by React Router DOM, a popular routing library for React applications. The \u003ccode\u003eHashRouter\u003c/code\u003e uses the hash portion of the URL (the part after the \u003ccode\u003e#\u003c/code\u003e symbol) to keep the UI in sync with the URL. This makes it suitable for applications that need to be deployed to servers that don't handle dynamic requests, such as GitHub Pages.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch1\u003eProduction Mode\u003c/h1\u003e\u003cp\u003eMy Template is very ready about hearing the produciton mode. Dont worry about that. if you not optimis about my template during in production environment. You can clone my repository and attempt to make it as a production app. Go to a terminal and type \u003cstrong\u003enpm run prod. \u003c/strong\u003eThis will make 2 folders name dist and out folder. where the dist folder is the build package for the react and the out folder is the application that you are going to install in your device. Here is the package.json file you can look and learn about the template environment:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e{\n  \"name\": \"modash-desktop-app\",\n  \"productName\": \"Modash\",\n  \"description\": \"Modification Dashboard Dekstop App for inventory management purpose\",\n  \"private\": true,\n  \"version\": \"1.0.0\",\n  \"type\": \"module\",\n  \"main\": \"electron.cjs\",\n  \"scripts\": {\n    \"dev\": \"cross-env NODE_ENV=development concurrently \\\"npm:serve\\\" \\\"npm:electron\\\"\",\n    \"prod\": \"npm run build \u0026\u0026 npm run electron-build\",\n    \"serve\": \"vite\",\n    \"build\": \"cross-env NODE_ENV=production vite build\",\n    \"preview\": \"vite preview\",\n    \"electron\": \"wait-on tcp:6969 \u0026\u0026 electron .\",\n    \"electron-build\": \"cross-env NODE_ENV=production electron-builder\"\n  },\n  \"build\": {\n    \"appId\": \"electron-react-vite\",\n    \"mac\": {\n      \"icon\": \"public/download.ico\"\n    },\n    \"win\": {\n      \"target\": [\n        \"nsis\"\n      ],\n      \"icon\": \"public/download.ico\"\n    },\n    \"nsis\": {\n      \"oneClick\": false,\n      \"allowToChangeInstallationDirectory\": true,\n      \"installerIcon\": \"public/download.ico\",\n      \"uninstallerIcon\": \"public/download.ico\",\n      \"uninstallDisplayName\": \"electron-react-vite\"\n    },\n    \"directories\": {\n      \"output\": \"out\"\n    },\n    \"files\": [\n      \"dist/**/*\",\n      \"electron.cjs\",\n      \"electron/**\"\n    ]\n  },\n  \"dependencies\": {\n    \"@chakra-ui/react\": \"^2.8.2\",\n    \"@emotion/react\": \"^11.11.4\",\n    \"@emotion/styled\": \"^11.11.5\",\n    \"framer-motion\": \"^11.2.6\",\n    \"react\": \"^18.2.0\",\n    \"react-dom\": \"^18.2.0\",\n    \"react-icons\": \"^5.2.1\",\n    \"react-router-dom\": \"^6.23.1\"\n  },\n  \"devDependencies\": {\n    \"@types/react\": \"^18.0.28\",\n    \"@types/react-dom\": \"^18.0.11\",\n    \"@vitejs/plugin-react\": \"^4.0.0\",\n    \"builder\": \"^5.0.0\",\n    \"concurrently\": \"^8.0.1\",\n    \"cross-env\": \"^7.0.3\",\n    \"electron\": \"^24.3.1\",\n    \"electron-builder\": \"^24.13.3\",\n    \"eslint\": \"^8.38.0\",\n    \"eslint-plugin-react\": \"^7.32.2\",\n    \"eslint-plugin-react-hooks\": \"^4.6.0\",\n    \"eslint-plugin-react-refresh\": \"^0.3.4\",\n    \"vite\": \"^4.3.2\",\n    \"wait-on\": \"^7.0.1\"\n  }\n}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cimg src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FModash-Display.png?alt=media\u0026token=3b3d3eef-37e4-4c37-91a1-80394b91069e'/\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cp\u003eYou need to install your production applicaiton and once installed you can look the first page will display the login page. You can modify this style as you like. but the default style is just like it. Enjoy Use this template and you can build this with fast. and promises production environment üòÅ.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Felectron.png?alt=media\u0026token=46d654ee-c016-41d2-ab1d-38d5653638a6","image_alt":"Electron.js","short_description":"In the world of application development, Electron.js has become a popular framework for building desktop applications using web technologies. In this blog, we will discuss how to create a desktop application template with Electron.js using React and Vite as the frontend framework, and IndexedDB as the default client-side database.","timestamp":"2024-09-20 14:06:27","title":"Create Dashboard app with Electron.js"},{"blog_id":"bc2c576f-29ce-4513-ab68-b1c336eb9e90","description":"\u003cdiv id=\"content-2\"\u003e\u003cp\u003eElasticsearch is a powerful search and analytics engine that is often used in conjunction with various architectural patterns, including CQRS (Command Query Responsibility Segregation). Here‚Äôs an overview of why Elasticsearch can be particularly useful in the context of CQRS:\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eWhy Use Elasticsearch with CQRS Architecture?\u003c/strong\u003e\u003c/h2\u003e\u003ch3\u003e\u003cstrong\u003e1. Separation of Concerns\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eIn CQRS, the system is divided into two parts:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCommand Side\u003c/strong\u003e: Handles writes and updates to the system.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eQuery Side\u003c/strong\u003e: Handles reads and queries from the system.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eElasticsearch fits naturally into the Query Side of CQRS. It‚Äôs designed for fast searches and complex queries, which aligns perfectly with the need for efficient read operations in CQRS. By using Elasticsearch, you can offload complex search and analytics queries from your primary database, allowing it to focus on handling writes and updates.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003e2. Scalability\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eElasticsearch is built to handle large volumes of data and high query loads. This makes it an excellent choice for the Query Side in CQRS where you might need to execute complex searches, aggregations, and filtering on large datasets. Elasticsearch‚Äôs distributed nature allows it to scale horizontally, handling increasing amounts of read traffic without a significant performance hit.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003e3. High Performance\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eElasticsearch is optimized for search and analytics, providing low-latency responses even for complex queries. It uses inverted indexing to quickly retrieve relevant documents, which is beneficial for applications requiring fast, real-time search capabilities. This performance advantage is critical in the Query Side of CQRS, where read operations need to be efficient and responsive.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003e4. Flexibility in Querying\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eElasticsearch supports a wide range of query types, including full-text search, filtering, faceting, and aggregations. This flexibility allows you to perform complex and nuanced queries that are often needed for analytics and reporting. In CQRS, where the Query Side might need to provide various views and insights from the data, Elasticsearch can cater to these diverse querying needs.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fcqrs.png?alt=media\u0026token=6eab7b0b-37d8-49f2-9137-27dadd766c96'/\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch1\u003e\u003cstrong\u003eElasticsearch Configuration\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eBy default, Elasticsearch comes with a feature called X-Pack, which is an additional plugin provided by Elastic. However, since X-Pack is not open-source, you can disable it if it's not needed. All Elasticsearch configurations can be found in the \u003ccode\u003econfig/elasticsearch.yml\u003c/code\u003e file. Elasticsearch uses YAML format for its configurations, making it easy to manage.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch1\u003e\u003cstrong\u003eRunning Elasticsearch\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTo run Elasticsearch, simply open a terminal and execute the following command:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e./bin/elasticsearch\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003eOnce it's running, Elasticsearch will be accessible on port 9200 according to the \u003ccode\u003ehttp.port\u003c/code\u003e setting in the configuration file. To stop the Elasticsearch application, you can use the \u003ccode\u003eCtrl + C\u003c/code\u003e key combination.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ePUT /index_name\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cp\u003eThe rules for an index name are: it must be lowercase, cannot contain special characters except for \u003ccode\u003e-\u003c/code\u003e, \u003ccode\u003e+\u003c/code\u003e, and \u003ccode\u003e_\u003c/code\u003e (and these cannot be at the beginning), and it cannot exceed 255 bytes.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003ch2\u003e\u003cstrong\u003eElasticsearch Client\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eElasticsearch communicates using RESTful API, which means we can use HTTP to interact with it. This makes Elasticsearch very flexible and easy to learn, as well as easy to integrate with other applications.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eFlexible Schema\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eUnlike relational databases, Elasticsearch allows you to insert data into an index without needing to define the schema first. This schema is very flexible, but once it's established, you cannot change the data type of existing fields; you can only add new attributes. For example, if you create an \u003ccode\u003eage\u003c/code\u003e attribute with a \u003ccode\u003enumber\u003c/code\u003e type, you cannot change it to a \u003ccode\u003estring\u003c/code\u003e type later on.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003ePrimary Key in Elasticsearch\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWhen creating a document in Elasticsearch, you are required to include a primary key or ID. Unlike relational databases, in Elasticsearch, the primary key must use the \u003ccode\u003e_id\u003c/code\u003e field and can only consist of a single field with a string type.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eInteracting with Elasticsearch Using API\u003c/strong\u003e\u003c/h2\u003e\u003ch3\u003eCreating an Index\u003c/h3\u003e\u003cp\u003eIn Elasticsearch, there is no concept of a database like in RDBMS. You can directly create an index (similar to a table in a database). A common practice is to use the application name as a prefix for the index name, such as \u003ccode\u003emyapp_users\u003c/code\u003e. This prevents index name conflicts when Elasticsearch is used for multiple applications.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ePUT /index_name\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cp\u003eThe rules for an index name are: it must be lowercase, cannot contain special characters except for \u003ccode\u003e-\u003c/code\u003e, \u003ccode\u003e+\u003c/code\u003e, and \u003ccode\u003e_\u003c/code\u003e (and these cannot be at the beginning), and it cannot exceed 255 bytes.\u003c/p\u003e\u003ch3\u003eDeleting an Index\u003c/h3\u003e\u003cp\u003eTo delete an index, you can use the DELETE HTTP method. Deleting an index will automatically remove all data within it.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eDELETE /index_name\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003ch3\u003eAdding Data with Create API\u003c/h3\u003e\u003cp\u003eTo add data to Elasticsearch, you can use the Create API. This API is safe, meaning if the document with the specified \u003ccode\u003e_id\u003c/code\u003e doesn't exist, it will be saved as a new document. However, if it already exists, a conflict error will occur.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ePOST /index_name/_create/id\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003ch3\u003eRetrieving Data with Get API\u003c/h3\u003e\u003cp\u003eAfter saving data, you can retrieve it using the Get API. This API returns the data along with its metadata, such as \u003ccode\u003e_id\u003c/code\u003e, index name, document version, etc.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eGET /index_name/_doc/id\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003ch3\u003eMulti Get API\u003c/h3\u003e\u003cp\u003eElasticsearch also provides a Multi Get API to retrieve multiple documents at once. This is useful when you need to fetch data from multiple indexes in a single API call.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ePOST /_mget\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003ch3\u003eSearching Data with Search API\u003c/h3\u003e\u003cp\u003eTo search for documents without using \u003ccode\u003e_id\u003c/code\u003e, you can use the Search API. This is a very powerful and complex API that allows you to perform highly specific queries.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ePOST /index_name/_search\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003ch3\u003ePagination and Sorting\u003c/h3\u003e\u003cp\u003eThe Search API also supports pagination with the \u003ccode\u003efrom\u003c/code\u003e parameter to specify the starting document and the \u003ccode\u003esize\u003c/code\u003e parameter to specify the number of documents in the response. Additionally, you can sort the search results using the \u003ccode\u003esort\u003c/code\u003e parameter.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-24\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ePOST /index_name/_search\n{\n  \"from\": 0,\n  \"size\": 10,\n  \"sort\": [\n    { \"field_name\": \"asc\" }\n  ]\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-25\"\u003e\u003ch1\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eElasticsearch is a powerful and flexible tool for search and data analysis. With a basic understanding of configuration, running the application, and interacting with it using APIs, you can start leveraging Elasticsearch for various use cases such as full-text search, log management, and more. In the next article, we'll dive deeper into optimization and advanced features in Elasticsearch.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FElastic-Search.webp?alt=media\u0026token=419c2bf2-cb3e-4582-a48f-eec9da861459","image_alt":"Intro Elastic search","short_description":"Elasticsearch is a powerful and scalable search engine built on Apache Lucene, commonly used for full-text search, data analysis, and log management.","timestamp":"2024-09-20 14:05:37","title":"Introduction to Elastic Search"},{"blog_id":"4e309d73-dc31-47b8-816a-901fd092368d","description":"\u003cdiv id=\"content-3\"\u003e\u003ch1\u003eIntroduction\u003c/h1\u003e\u003cp\u003eIn this article, we will explore how to integrate MySQL with a Flask application using the\u0026nbsp;\u003ccode\u003eflask_mysqldb\u003c/code\u003e\u0026nbsp;library. This integration allows us to efficiently manage database operations within our Flask web applications. In this lab we will asumme that we are already know to installed the flask application. and we will skip ahead and jump to the sql connection.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eThere is a Prerequisites to running this operations:\u003c/p\u003e\u003cp\u003eBefore starting, ensure you have the following installed:\u003c/p\u003e\u003cul\u003e\u003cli\u003ePython (3.x recommended)\u003c/li\u003e\u003cli\u003eFlask\u003c/li\u003e\u003cli\u003eMySQL Server\u003c/li\u003e\u003cli\u003e\u003ccode\u003eflask_mysqldb\u003c/code\u003e\u0026nbsp;library (\u003ccode\u003epip install flask-mysqldb\u003c/code\u003e)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf we already fulfilled the prerequisites, we will going to install the mysql to the flask application:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003epip install flask-mysqldb\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cp\u003eOnce the installation is completed, we will initiate all the MySql configuration. in this lab we will going to initiate the configurations file trough the \u003cstrong\u003e\u003cu\u003eapp.py\u003c/u\u003e\u003c/strong\u003e file. This is not recommended if the application continue grows to large scale. but for this demonstration, we are going to do that.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003ch1\u003eConfigure the MYSQL Connections\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003efrom flask import Flask, request, jsonify\nfrom flask_mysqldb import MySQL\n\napp = Flask(__name__)\napp.config['MYSQL_HOST'] = 'localhost'\napp.config['MYSQL_USER'] = 'username'\napp.config['MYSQL_PASSWORD'] = 'password'\napp.config['MYSQL_DB'] = 'database_name'\n\nmysql = MySQL(app)\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cp\u003ein the app.py, im importing the flask_mysqldb that we are already installed so we can use it in the app.py file later. after import the module. initiate the mysql configuration just like above. fill the neccessary field such as the password for db, the host user, etc. depends non your localhost mysql server.\u003c/p\u003e\u003cp\u003eit will be good if we try to running the flask server application and there is no anomaly when we are running it. when there is nothing error when we are running the flask server. that means that out mysql connection configurations form the flask is successful. and we will continue to do some basic mysql operations in this flask server.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003ch1\u003eCREATE OPERATIONS\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eapp.route('/insert', methods=['POST'])\ndef insert():\n    cur = mysql.connection.cursor()\n    data = request.get_json()\n    name = data['name']\n    age = data['age']\n    cur.execute(\"INSERT INTO users (name, age) VALUES (%s, %s)\", (name, age))\n    mysql.connection.commit()\n    cur.close()\n    return jsonify({'message': 'Data inserted successfully'})\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003cp\u003eThis code snippet defines a route in a Flask application (\u003ccode\u003e/insert\u003c/code\u003e) that handles POST requests. When a POST request is made to this endpoint, the function \u003ccode\u003einsert()\u003c/code\u003e is executed.\u003c/p\u003e\u003cp\u003eInside \u003ccode\u003einsert()\u003c/code\u003e, it establishes a connection to the MySQL database using \u003ccode\u003eflask_mysqldb\u003c/code\u003e, extracts JSON data containing \u003ccode\u003ename\u003c/code\u003e and \u003ccode\u003eage\u003c/code\u003e from the request body, and inserts this data into the \u003ccode\u003eusers\u003c/code\u003e table using an SQL \u003ccode\u003eINSERT\u003c/code\u003e statement.\u003c/p\u003e\u003cp\u003eAfter committing the transaction to the database and closing the cursor, it returns a JSON response confirming successful data insertion. This functionality demonstrates how to integrate MySQL database operations seamlessly into a Flask web application for handling data input.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003ch1\u003eREAD OPERATIONS\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e@app.route('/users', methods=['GET'])\ndef users():\n    cur = mysql.connection.cursor()\n    cur.execute(\"SELECT * FROM users\")\n    results = cur.fetchall()\n    cur.close()\n    users_list = []\n    for row in results:\n        user = {\n            'id': row[0],\n            'name': row[1],\n            'age': row[2]\n        }\n        users_list.append(user)\n    return jsonify({'users': users_list})\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cp\u003eLet brak down the code piece by piece!\u003c/p\u003e\u003cp\u003eSo This code snippet defines a route in a Flask application (\u003ccode\u003e/users\u003c/code\u003e) that handles GET requests. When a GET request is made to this endpoint, the function\u0026nbsp;\u003ccode\u003eusers()\u003c/code\u003e\u0026nbsp;is executed.\u003c/p\u003e\u003cp\u003eInside\u0026nbsp;\u003ccode\u003eusers()\u003c/code\u003e, it establishes a connection to the MySQL database using\u0026nbsp;\u003ccode\u003eflask_mysqldb\u003c/code\u003e, executes an SQL\u0026nbsp;\u003ccode\u003eSELECT\u003c/code\u003e\u0026nbsp;query to fetch all records from the\u0026nbsp;\u003ccode\u003eusers\u003c/code\u003e\u0026nbsp;table, and fetches all results using\u0026nbsp;\u003ccode\u003ecur.fetchall()\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eAfter retrieving the results, it closes the database cursor to release resources. Then, it iterates through the fetched results to construct a list of dictionaries (\u003ccode\u003eusers_list\u003c/code\u003e), where each dictionary represents a user with keys\u0026nbsp;\u003ccode\u003eid\u003c/code\u003e,\u0026nbsp;\u003ccode\u003ename\u003c/code\u003e, and\u0026nbsp;\u003ccode\u003eage\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eFinally, it returns a JSON response containing the list of users in the format\u0026nbsp;\u003ccode\u003e{'users': users_list}\u003c/code\u003e. This functionality demonstrates how to retrieve and format data from a MySQL database using Flask, making it accessible via an API endpoint. Very Simple is in it?\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003cp\u003eBy following these steps, you can effectively integrate MySQL into your Flask applications using \u003ccode\u003eflask_mysqldb\u003c/code\u003e. This setup enables you to perform essential database operations seamlessly within your web development projects.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fmysql_flask.png?alt=media\u0026token=9890d7a0-79cc-4ed1-beae-0b916b9f72e1","image_alt":"SQL + Flask","short_description":"This lab will cover the installation and usage of the flask_mysqldb library. It will include instructions on installing the library, initializing the database in app.py, and providing examples of how to post data and read data from the database.","timestamp":"2024-09-20 11:57:42","title":"Integrate Mysql to Flask"},{"blog_id":"e27f5bbc-f1be-4013-89da-312a09e6f3c0","description":"\u003ch1\u003eOptimizing Your Content for Top-notch SEO\u003c/h1\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\u003cp\u003eSEO (Search Engine Optimization) is a crucial practice in the digital world to enhance the visibility and ranking of your web pages on search engines like Google. By understanding the fundamental principles of SEO, you can increase the likelihood of your content appearing in search results, attract more visitors, and enhance your online presence.\u003c/p\u003e\u003ch2\u003e1. Understanding Keywords\u003c/h2\u003e\u003cp\u003eThe key to SEO lies in keywords or keyword phrases that potential visitors use when searching for information on search engines. Conduct keyword research to identify terms relevant to your business or topic. Utilize tools like Google Keyword Planner to determine keywords with high search volume.\u003c/p\u003e\u003ch2\u003e2. High-Quality Content\u003c/h2\u003e\u003cp\u003eSearch engines prioritize content that provides value to users. Ensure your content is informative, relevant, and meets the needs of your audience. Use engaging writing styles and avoid duplicate content. High-quality content not only captivates readers but is also favored by search engines.\u003c/p\u003e\u003ch2\u003e3. SEO-Friendly URL Structure\u003c/h2\u003e\u003cp\u003eIt's crucial to have URLs that are easily understood by both humans and search engines. Use a clear and descriptive URL structure that is concise and relevant to the content. For example: \u003ccode\u003ewww.examplewebsite.com/seo-article\u003c/code\u003e.\u003c/p\u003e\u003ch2\u003e4. Heading and Subheading Usage\u003c/h2\u003e\u003cp\u003eBreak down your content into easily digestible sections using headings (H1, H2, H3, etc.) and subheadings. This helps search engines understand the structure and hierarchy of your content. Make sure to include keywords in headings to provide additional signals to search engines.\u003c/p\u003e\u003ch2\u003e5. Image Optimization\u003c/h2\u003e\u003cp\u003eImages can enhance the visual appeal of your content but also require SEO optimization. Ensure each image has a descriptive alt attribute containing relevant keywords. Image file sizes should also be minimized to speed up page loading times.\u003c/p\u003e\u003ch2\u003e6. Quality Backlinks\u003c/h2\u003e\u003cp\u003eGetting backlinks from high-quality websites can boost your authority and SEO ranking. Build partnerships with other websites in your industry and engage in mutually beneficial link exchanges.\u003c/p\u003e\u003ch2\u003eConclusion\u003c/h2\u003e\u003cp\u003eOptimizing your content for SEO requires attention to detail and a commitment to providing added value to users. By implementing proper SEO practices, you can improve search engine rankings, attract more traffic, and strengthen your online presence.\u003c/p\u003e","image":"https://itbox.id/wp-content/uploads/2023/02/SEO-1_11zon.jpg","image_alt":"SEO Cover Image","short_description":"SEO stands for ‚Äúsearch engine optimization.‚Äù In simple terms, SEO means the process of improving your website to increase its visibility in Google, Microsoft Bing, and other search engines whenever people search for: Products you sell. Services you provide.","timestamp":"2024-09-20 11:37:18","title":"Search Engine Optimization"},{"blog_id":"14d690d3-2201-43e4-b936-e06564ab67e2","description":"\u003ch1\u003eReact PWA Fundamental\u003c/h1\u003e\u003cp\u003eA Progressive Web App is a type of web application that combines the best features of web and mobile applications. PWAs can be accessed through a web browser but can also be installed on a user's device like traditional mobile apps. Some key features of PWAs include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe ability to work offline\u003c/li\u003e\u003cli\u003eFast performance\u003c/li\u003e\u003cli\u003eResponsive display on various devices\u003c/li\u003e\u003cli\u003eAccess through an icon on the device's home screen\u003c/li\u003e\u003c/ul\u003e\u003cp\u003ein this case we will be focused on how to make our react applicaion running on offline, get the requested react module from cache instead from the server. we will be learn about how to cache the customs api fetch that can be store to our react application.\u003c/p\u003e\u003ch2\u003eWhat Are Service Workers?\u003c/h2\u003e\u003cp\u003eService Workers are a crucial part of PWAs. They are JavaScript files that run in the background and enable features like offline caching, push notifications, and background sync. Service Workers act as intermediaries between your web application and the network, allowing you to control how your PWA behaves in various scenarios.\u003c/p\u003e\u003ch2\u003eVITE React PWA\u003c/h2\u003e\u003cp\u003eIn Vite React Project, there is some special configuration needed to applied the Progressive Web App. Below is the step-by-step to configure the PWA.\u003c/p\u003e\u003ch3\u003eInstalling vite-plugin-pwa\u003c/h3\u003e\u003cp\u003eFirst we need to install the vite-plugin-pwa plugin, just add it to your project as a dev dependency:\u003c/p\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003enpm install -D vite-plugin-pwa\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNote: to running the implementation of PWA, we need to to build our react vite project, then running the react vite project trough npm run preview. There is some extra configuration to make implementation of PWA running in development mode.\u003c/p\u003e\u003ch3\u003eConfiguring vite-plugin-pwa\u003c/h3\u003e\u003cp\u003eEdit your vite.config.js / vite.config.ts file and add the vite-plugin-pwa:\u003c/p\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport { defineConfig } from 'vite'\nimport { VitePWA } from 'vite-plugin-pwa'\nimport react from '@vitejs/plugin-react'\n\n// https://vitejs.dev/config/\nexport default defineConfig({\n  plugins: [\n    react(),\n    VitePWA({\n      registerType: 'autoUpdate',\n      workbox: {\n        globPatterns: ['**/*.{js,css,html,ico,png,svg}']\n      }\n    })\n  ],\n})\u003c/code\u003e\u003c/pre\u003e\u003ch3\u003eCache External Resources\u003c/h3\u003e\u003cp\u003eIf you have some additional resource like font and css, you must include them into the service workerpre-cache and so your application will work when offline. But in this scenario, we will trying to use some free-api named: https://jsonplaceholder.typicode.com. to fetch the data from it and then stored it to the cache browser. so it can be rendered to a front-end page without the network traffic. The implementation is very easy. we need to addd some property in workbox object named :runtimeCaching. Below the example of how to use it:\u003c/p\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e runtimeCaching: [\n          {\n            urlPattern: ({ url }) =\u0026gt; {\n              return url.pathname.match('/posts/1')\n            },\n            handler: 'CacheFirst',\n            options: {\n              cacheName: 'api-cache',\n              cacheableResponse: {\n                statuses: [0, 200]\n              }\n            }\n          }\n        ]\u003c/code\u003e\u003c/pre\u003e\u003ch2\u003eResult Excercise\u003c/h2\u003e\u003cp\u003eBelow is the result example of how the PWA can running the application without the network traffic and requested from the server.\u003c/p\u003e\u003cimg src='https://raw.githubusercontent.com/Barbarpotato/React-PWA-Fundamental/main/git-images/Result.png?token=GHSAT0AAAAAABT2NYLCZWUIB25VOK65BHRIZI6WNAA'/\u003e\u003ch2\u003eService Worker without PWA capabilities\u003c/h2\u003e\u003cp\u003eSometimes you don't need the full blown PWA functionality like offline cache and manifest file, but need simple custom Service Worker.\u003c/p\u003e\u003ch3\u003eSetup the Service Worker\u003c/h3\u003e\u003cp\u003eYou can first check the browsers are supporting the service worker by create the script like below:\u003c/p\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e  \u0026lt;script\u0026gt;\n    if ('serviceWorker' in navigator) {\n      window.addEventListener('load', () =\u0026gt; {\n        navigator.serviceWorker.register('/src/serviceWorker.js').then((reg) =\u0026gt; {\n          console.log('Worker Registered!')\n        }).catch(err =\u0026gt; {\n          console.log('Error in service Worker', err)\n        })\n      })\n    }\n  \u0026lt;/script\u0026gt;\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis this code is responsible for registering a service worker for your web application.\u003c/p\u003e\u003ch2\u003eOffline Caching\u003c/h2\u003e\u003cp\u003eIf the services worker is available (it whill show the Worker Registered in your broswer console). Now let's create the serviceWorker.js file in public directory. We can squeeze the serviceWorker file by create some eventlistener that installed some assests from server to Cache Storage. So the client is not calling the resource from the server anymore instead calling from the client browser cache data.\u003c/p\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eself.addEventListener('install', (event) =\u0026gt; {\n    event.waitUntil(\n        caches.open('PWA-Cache').then((caches) =\u0026gt; {\n            console.log('Opened Cache')\n            return caches.addAll([\n                './assets/react.svg',\n                '/vite.svg'\n            ])\n        })\n    )\n})\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe purpose of this install event handler is to cache these specified assets when the service worker is first installed. Once the assets are cached, they can be served from the cache even if the user is offline, providing offline access to these resources. This is a fundamental step in building Progressive Web Apps (PWAs) that work seamlessly offline.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eAfter installing all assest from the server to the client. we need to tell the browser that whenever we fetch the data, we need to check the browser cache data first before we calling the server resource. if the client request it is same as the data from a data cache browser, then just use the cache browser data. Below is the example of how the explanation above implemented in javascript:\u003c/p\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eself.addEventListener('fetch', (event) =\u0026gt; {\n    event.respondWith(\n        caches.match(event.request).then((response) =\u0026gt; {\n            if (response) {\n                // Cache hit, return the response\n                return response;\n            }\n            // Not found in cache, fetch from the network\n            return fetch(event.request);\n        })\n    );\n});\u003c/code\u003e\u003c/pre\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Freact-pwa.png?alt=media\u0026token=2b264f65-ddf9-4b4a-afa8-efb39cb13c3d","image_alt":"Progressive Web App","short_description":"A Progressive Web App is a type of web application that combines the best features of web and mobile applications. PWAs can be accessed through a web browser but can also be installed on a user's device like traditional mobile apps.","timestamp":"2024-09-20 11:37:12","title":"React PWA Fundamental"},{"blog_id":"9c490813-d5d6-4e27-8cc8-31f115046c0e","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eWhat is GraphQL?\u003c/h1\u003e\u003cp\u003eGraphQL is a query language for your API, and it's designed to request and deliver exactly the data that a client needs. Unlike REST, where the server determines what data is returned, GraphQL puts the control in the hands of the client. This allows for more efficient data retrieval, reduces over-fetching and under-fetching, and enables clients to request multiple resources in a single query.\u003c/p\u003e\u003ch1\u003eHow Does GraphQL Work?\u003c/h1\u003e\u003cp\u003eGraphQL is based on a strong schema that defines the types and operations available in your API. Clients can request data by specifying what they need, and the server responds with only the requested data in a structured format (typically JSON). The server resolves the query by matching it to the available types and fields in the schema.\u003c/p\u003e\u003ch1\u003eSetting Up GraphQL with Node.js, Express, and graphql-express\u003c/h1\u003e\u003cp\u003eThis Content will guide you through the process of setting up a GraphQL server on the backend using Node.js, Express, and the graphql-express package. GraphQL is a powerful query language that allows you to request and deliver data with precision, and this setup will enable you to create a flexible API.\u003c/p\u003e\u003ch1\u003ePrerequisites\u003c/h1\u003e\u003cp\u003eBefore you get started, ensure you have the following prerequisites:\u003c/p\u003e\u003col\u003e\u003cli\u003eNode.js installed on your machine.\u003c/li\u003e\u003cli\u003eA basic understanding of JavaScript and Express.\u003c/li\u003e\u003c/ol\u003e\u003ch1\u003eBasic Installation \u0026amp; Usage\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cul\u003e\u003cli\u003eCreate a New Node.js Project: If you don't already have a Node.js project, create a new directory for your project and run npm init to initialize a new Node.js project.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cul\u003e\u003cli\u003eInstall Dependencies: You'll need to install the required packages using npm. Run the following command to install Express, GraphQL, and graphql-express:\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003enpm install express graphql express-graphql\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cul\u003e\u003cli\u003eCreate a Server File: Create a new JavaScript file (e.g., app.js) in your project directory.\u003c/li\u003e\u003cli\u003eImport Dependencies: In server.js, import the necessary packages and set up the Express app.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst express = require('express');\nconst { graphqlHTTP } = require('express-graphql');\nconst { buildSchema } = require('graphql');\n\nconst app = express();\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cp\u003eCreate Schema Folder and create schema.js file. In Schema file there is some component that you need to understand:\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eCreate GraphQL Object Type\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eObject Type is a fundamental building block used to define the structure of the data that can be queried from a GraphQL API. It represents a type of object that can be retrieved or manipulated through the API. Object Types play a crucial role in modeling the data and defining the shape of the response that clients can request. Below is the example of how to make Object Type of GraphQL:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst BookType = new GraphQLObjectType({\n    name: 'book',\n    fields: () =\u0026gt; ({\n        id: { type: GraphQLString },\n        name: { type: GraphQLString },\n        genre: { type: GraphQLString }\n    })\n});\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003ch2\u003eCreate Entry Point to the GraphQL Schema\u003c/h2\u003e\u003cp\u003eQuery is an operation type in GraphQL used to read or retrieve data from the server. They are created using the RootQuery Object Type and include fields that can be accessed by clients. Below the example code:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst RootQuery = new GraphQLObjectType({\n    name: 'RootQueryType',\n    fields: {\n        book: {\n            type: BookType,\n            args: { id: { type: GraphQLString } },\n            resolve(parent, args) {\n                // code to get data from db\n                return books.filter(object =\u0026gt; object.id === args.id)[0]\n            }\n        }\n    }\n});\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cp\u003eThis code is defining a RootQuery type in GraphQL, which serves as the entry point for querying data in the schema.\u003c/p\u003e\u003col\u003e\u003cli\u003econst RootQuery = new GraphQLObjectType({ ... }): This line creates a new GraphQL Object Type called RootQuery. The RootQuery type is special in GraphQL, as it is the starting point for all read (query) operations. It defines the fields that clients can query from the root of the schema.\u003c/li\u003e\u003cli\u003ename: 'RootQueryType': This sets the name for the RootQueryType. In this case, it's named \"RootQueryType.\"\u003c/li\u003e\u003cli\u003efields: { ... }: Here, you define the available fields within the RootQuery. Each field represents a possible query that clients can make.\u003c/li\u003e\u003cli\u003ebook: { ... }: This defines a field called \"book\" within the RootQuery. Clients can use this field to query information about books.\u003c/li\u003e\u003cli\u003etype: BookType: The type field specifies the data type that will be returned by the \"book\" query. In this case, it's set to the BookType, indicating that when a client queries \"book,\" they will receive data structured according to the BookType.\u003c/li\u003e\u003cli\u003eargs: { id: { type: GraphQLString } }: The args field specifies the arguments that can be provided with the \"book\" query. In this case, there's one argument named \"id,\" which is of type GraphQLString. It means that clients need to provide an \"id\" when querying for a book.\u003c/li\u003e\u003cli\u003eresolve(parent, args) { ... }: The resolve function is where you specify how to fetch the actual data when a client makes a query for \"book.\"\u003c/li\u003e\u003cli\u003ereturn books.filter(object =\u0026gt; object.id === args.id)[0]: Within the resolve function, you see code to fetch the data. In this case, it's looking through an array called \"books\" to find a book with an \"id\" that matches the one provided by the client. The filter method is used to find the matching book, and [0] is added to return the first matching result. This result will be returned to the client in the shape of a BookType.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eFinally, the code exports a new GraphQLSchema instance with the RootQuery as the query root. This makes the book query available for use in your GraphQL API.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003emodule.exports = new GraphQLSchema({\nquery: RootQuery\n});\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cul\u003e\u003cli\u003eSet Up GraphQL Middleware to your app.js: Use the graphqlHTTP middleware to create a GraphQL endpoint for your Express app.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eapp.use('/graphql', graphqlHTTP({\n    schema: schema,\n    graphiql: true\n}))\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003cp\u003eby using the graphiql property to your middleware, the backend service will provide the graphql development interface that can be access for demo and simulate accessing the different variant of our http request.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003cul\u003e\u003cli\u003eStart the Server: Start the Express server on a port of your choice.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eapp.listen(4000, () =\u0026gt; {\n    console.log(`Now Listening on Post 4000`)\n})\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cul\u003e\u003cli\u003eTesting Your Queries in GrapiQL: You can access the /graphql endpoint in your browser, and the interface will be like this:\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cimg src='https://github.com/Barbarpotato/GraphQL-Fundamental/raw/main/images/graphiql-example1.png'/\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003ch2\u003eList Type\u003c/h2\u003e\u003cp\u003eSo far we have already built the relationship between the book and who is the author from the book. Now we want to build the relationship between the author and the book. we want to know if some author are called trough the request, we want to know what books they are created. in other words we call it\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eone to many\u003c/code\u003e\u0026nbsp;relationship if we are on the relational database environment, which is one author can have many books they created. Below is the example of how to implement it in GraphQL:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e// we added additional object for the one to many relationship display purposes.\nconst books = [\n    { name: 'Name of thw Wind', genre: 'Fantasy', id: '1', authorId: '1' },\n    { name: 'The Final Empire', genre: 'Fantasy', id: '2', authorId: '2' },\n    { name: 'The Long Earth', genre: 'Sci-Fi', id: '3', authorId: '3' },\n    { name: 'The Hero Of Ages', genre: 'Fantasy', id: '4', authorId: '2' },\n    { name: 'The Colourof Magic', genre: 'Fantasy', id: '5', authorId: '3' },\n    { name: 'The Loght Fantastic', genre: 'Fantasy', id: '6', authorId: '3' }\n];\n\nconst authors = [\n    { name: 'Patrick Bateman', age: 29, id: '1' },\n    { name: 'Bruce Wayne', age: 33, id: '2' },\n    { name: 'Peter Parker', age: 25, id: '3' }\n]\n\nconst AuthorType = new GraphQLObjectType({\n    name: 'author',\n    fields: () =\u0026gt; ({\n        id: { type: GraphQLID },\n        name: { type: GraphQLString },\n        age: { type: GraphQLInt },\n        book: {\n            type: new GraphQLList(BookType),\n            resolve(parent, args) {\n                return books.filter(object =\u0026gt; object.authorId === parent.id)\n            }\n        }\n    })\n});\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-24\"\u003e\u003cul\u003e\u003cli\u003eIn above code, dont forget that we are going to return multiple object from the book fields, which we are need the\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eGraphQLList\u003c/code\u003e\u0026nbsp;imported from the\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003egraphql\u003c/code\u003e\u0026nbsp;instance.\u003c/li\u003e\u003cli\u003ewe are returning the processed data from the authorType. This data will be processed again in resolve function book field.\u003c/li\u003e\u003cli\u003ethe result query if we success build this one to many relations:\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-25\"\u003e\u003cimg src='https://github.com/Barbarpotato/GraphQL-Fundamental/raw/main/images/type-list.png'/\u003e\u003c/div\u003e\u003cdiv id=\"content-26\"\u003e\u003ch2\u003eAll Objects\u003c/h2\u003e\u003cp\u003eFor some cases, we need to return all list of books, or all list of author that we want to the client. To do this we just added some field in the\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eRootQuery\u003c/code\u003e\u0026nbsp;of our Schema:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-27\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst RootQuery = new GraphQLObjectType({\n    name: 'RootQueryType',\n    fields: {\n        ...,\n        ...,\n        books: {\n            type: new GraphQLList(BookType),\n            resolve(_parent, _args) {\n                return books\n            }\n        },\n        authors: {\n            type: new GraphQLList(AuthorType),\n            resolve(_parent, _args) {\n                return authors\n            }\n        }\n    }\n});\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-29\"\u003e\u003cp\u003eThe result output from the graphiql will be like this:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-30\"\u003e\u003cimg src='https://github.com/Barbarpotato/GraphQL-Fundamental/raw/main/images/all-objects.png'/\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgraphql.png?alt=media\u0026token=bdb93458-f903-44e3-bb96-0086a7f78a6e","image_alt":"Graph QL - Darma Labs","short_description":"GraphQL is a powerful query language for your API, and it provides a more efficient, powerful, and flexible alternative to the traditional REST API.","timestamp":"2024-09-20 11:37:06","title":"GraphQL Fundamental"},{"blog_id":"978cd40e-18da-47a6-a4fe-917bec69dea1","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003e\u003cstrong\u003eReal-World Case\u003c/strong\u003e\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003cimg style='width:720px;' src='https://raw.githubusercontent.com/Barbarpotato/React-Lazy-Load/main/git-image/Scenario.png'/\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cp\u003eWhen building a dashboard application. This application has high complexity and requires a large size to be loaded by application users in the future. For example, the dashboard application has several different features, depending on the position of each application user. for example, users with the admin position have a different features in the application compared to users with other positions, for example Sales. When Sales enters a dashboard application. Of course, the features contained in admin will not appear for users with sales positions, BUT users with sales positions will still load the features contained in admin even though they are not used. Vice Versa, This affects the performance, speed and efficiency of an application running.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eSolution\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo asnwer the prbolem above, we will be use\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eLazy Load\u003c/code\u003e\u0026nbsp;technique, you can optimize the initial loading time and improve the performance of your React application by dynamically loading components as needed.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eUsage\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eHere are the steps on how to use React Lazy Load in your project:\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eCreate the routes for the web page.\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWe have 3 different pags for this case, Main page which can be access for all the role position (Admin, Sales). we have the sales component which can be access by sales person, and the last page is the admin page, where it can only be access by the admin person.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport { Routes, Route } from 'react-router-dom'\nimport Sales from './pages/Sales'\nimport Home from './pages/Home'\nimport Admin from './pages/Admin'\nimport './App.css'\n\nfunction App() {\n\n  return (\n    \u0026lt;Routes\u0026gt;\n      \u0026lt;Route index element={\u0026lt;Home /\u0026gt;} /\u0026gt;\n      \u0026lt;Route path='/sales' element={\u0026lt;Sales /\u0026gt;} /\u0026gt;\n      \u0026lt;Route path='/admin' element={\u0026lt;Admin /\u0026gt;} /\u0026gt;\n    \u0026lt;/Routes\u0026gt;\n  )\n}\n\nexport default App\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cp\u003eIf we dont implement the lazy load technique\u0026nbsp;\u0026nbsp;our react application\u0026nbsp;it will be load\u0026nbsp;\u0026nbsp;three pages\u0026nbsp;which are Main, Sales\u0026nbsp;\u0026nbsp;the Admin page. If we implement the lazy load, whenever we access the Sales Page, it will be load the Main\u0026nbsp;\u0026nbsp;the Sales Page, Vice Versa\u003c/p\u003e\u003ch2\u003eApplied the Lazy Load to your React App\u003c/h2\u003e\u003cp\u003eAfter build the different page\u0026nbsp;\u0026nbsp;route, we can implement the lazy load by using the lazy\u0026nbsp;\u0026nbsp;Suspense\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport { Routes, Route } from 'react-router-dom'\nimport { lazy, Suspense } from 'react'\nimport Home from './pages/Home'\nimport './App.css'\n\nconst Admin = lazy(() =\u0026gt; import('./pages/Admin'))\nconst Sales = lazy(() =\u0026gt; import('./pages/Sales'))\n\nfunction App() {\n\n  return (\n    \u0026lt;Routes\u0026gt;\n\n      \u0026lt;Route index element={\u0026lt;Home /\u0026gt;} /\u0026gt;\n\n      \u0026lt;Route path='/sales' element={\n        \u0026lt;Suspense fallback={\u0026lt;div\u0026gt;Loading Content...\u0026lt;/div\u0026gt;}\u0026gt;\n          \u0026lt;Sales /\u0026gt;\n        \u0026lt;/Suspense\u0026gt;} /\u0026gt;\n\n      \u0026lt;Route path='/admin' element={\n        \u0026lt;Suspense fallback={\u0026lt;div\u0026gt;Loading Content...\u0026lt;/div\u0026gt;}\u0026gt;\n          \u0026lt;Admin /\u0026gt;\n        \u0026lt;/Suspense\u0026gt;} /\u0026gt;\n\n    \u0026lt;/Routes \u0026gt;\n  )\n}\n\nexport default App\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch2\u003e\u003cstrong\u003eImplementation Result\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eNow, if you go to the Dev Console -\u0026gt; Source. You need to check the page source file. and then check the src folder -\u0026gt; pages . We cannot see our Admin and Sales Component. If we accessing the admin route or the sales route, it will appear in the source dev console. which means that our lazy load implement successfully.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cimg style='width:720px;' src='https://raw.githubusercontent.com/Barbarpotato/React-Lazy-Load/main/git-image/lazy-load.png'/\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Flazy-load.png?alt=media\u0026token=94d1a55f-c578-455a-9623-b561272a9f99","image_alt":"Lazy Load React - Darma","short_description":"React Lazy Load is a technique used to load React components lazily, i.e., only when they are needed. This helps reduce the initial bundle size and improve the performance of your React application. You can easily implement lazy loading in your React project using React.lazy() along with Suspense.","timestamp":"2024-09-20 11:36:59","title":"React Lazy Load"},{"blog_id":"c4973cc3-3c29-4fcb-9651-5d921b02eed2","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eWhat is ETL?\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003eETL\u003c/strong\u003e stands for \u003cstrong\u003eExtract, Transform, Load\u003c/strong\u003e. It is a crucial process in data management and integration, typically used in data warehousing and data analytics. Here's a brief overview:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eExtract\u003c/strong\u003e:\u003c/li\u003e\u003cli class=\"ql-indent-1\"\u003eThis step involves retrieving data from various sources such as databases, files, APIs, or other systems. The goal is to gather raw data for further processing.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTransform\u003c/strong\u003e:\u003c/li\u003e\u003cli class=\"ql-indent-1\"\u003eOnce the data is extracted, it often needs to be cleaned, formatted, and transformed to fit the desired structure or schema. This step may include filtering, sorting, aggregating, and converting data to ensure consistency and compatibility.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLoad\u003c/strong\u003e:\u003c/li\u003e\u003cli class=\"ql-indent-1\"\u003eThe final step is to load the transformed data into a target system, such as a database, data warehouse, or data lake. This makes the data available for querying, analysis, and reporting.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch1\u003eETL using Shell Scripts\u003c/h1\u003e\u003cp\u003eIn this lab we will covering about read and extract data from various delimited thing using shell command. We'll understand how to handle file input, parse the data, and prepare it for further processing. Master the techniques for transforming text data using powerful shell utilities like tr command. and finally load the data to the database. in this lab we will use PostgreSQL. \u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch1\u003e\u003cstrong\u003eExercise 1 - Extracting data using 'cut' command\u003c/strong\u003e\u003c/h1\u003e\u003ch2\u003eExtracting characters\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eecho \"database\" | cut -c1-4\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cp\u003eYou should get the string ‚Äòdata‚Äô as output.\u003c/p\u003e\u003cp\u003eThe command below shows how to extract 5th to 8th characters.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eecho \"database\" | cut -c5-8\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cp\u003eYou should get the string ‚Äòbase‚Äô as output.\u003c/p\u003e\u003cp\u003eNon-contiguous characters can be extracted using the comma.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003ch2\u003eExtracting fields/columns\u003c/h2\u003e\u003cp\u003eWe can extract a specific column/field from a delimited text file, by mentioning\u003c/p\u003e\u003cul\u003e\u003cli\u003ethe delimiter using the\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003e-d\u003c/code\u003e\u0026nbsp;option, or\u003c/li\u003e\u003cli\u003ethe field number using the\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003e-f\u003c/code\u003e\u0026nbsp;option.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe /etc/passwd is a ‚Äú:‚Äù delimited file.\u003c/p\u003e\u003cp\u003eThe command below extracts usernames (the first field) from /etc/passwd.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ecut -d\":\" -f1 /etc/passwd\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cimg  style='width:600px; height:100%'src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fetl_using_shell_script_img_1.png?alt=media\u0026token=7a3e427e-002d-4d12-875f-8f8160cc984d'/\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003ch1\u003e\u003cstrong\u003eExercise 2 - Transforming data using 'tr'\u003c/strong\u003e\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003ch2\u003eTranslate from one character set to another\u003c/h2\u003e\u003cp\u003eThe command below translates all lower case alphabets to upper case.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eecho \"Shell Scripting\" | tr \"[a-z]\" \"[A-Z]\" echo \"Shell Scripting\" | tr \"[:lower:]\" \"[:upper:]\" echo \"Shell Scripting\" | tr \"[A-Z]\" \"[a-z]\"\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003ch1\u003ePrepared the Database for the practice\u003c/h1\u003e\u003cp\u003eIn this exercise we will create a table called\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003eusers\u003c/code\u003e\u0026nbsp;in the PostgreSQL database using PostgresSQL CLI. This table will hold the user account information.\u003c/p\u003e\u003cp\u003eThe table\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003eusers\u003c/code\u003e\u0026nbsp;will have the following columns: uname, uid, home\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ecreate table users(username varchar(50),userid int,homedirectory varchar(100));\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003ch1\u003eCreate the Shell Script File\u003c/h1\u003etouch csv2db.sh. Then Open the file in the editor. Copy and paste the following lines into the newly created file.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# This script # Extracts data from /etc/passwd file into a CSV file. # The csv data file contains the user name, user id and # home directory of each user account defined in /etc/passwd # Transforms the text delimiter from \":\" to \",\". # Loads the data from the CSV file into a table in PostgreSQL database.\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003cp\u003eWe need to add lines of code to the script that will xtract user name (field 1), user id (field 3), and home directory path (field 6) from /etc/passwd file using the\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003ecut\u003c/code\u003e\u0026nbsp;command. \u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ecut -d\":\" -f1,3,6 /etc/passwd\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cp\u003ethen run the script. -\u0026gt; bash csv2db.sh\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cp\u003elet us change the last line of command so it not directly printed trough the terminal instead we will save the output to .txt file\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ecut -d\":\" -f1,3,6 /etc/passwd \u0026gt; extracted-data.txt\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-24\"\u003e\u003cimg style='width:720px; heigth:100%;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fetl_using_shell_script_img_2.png?alt=media\u0026token=3b31b28f-40bf-4f3e-952a-4d0607d133c6'/\u003e\u003c/div\u003e\u003cdiv id=\"content-25\"\u003e\u003cp\u003eThe extracted columns are separated by the original ‚Äú:‚Äù delimiter. You need to convert this into a ‚Äú,‚Äù delimited file. Add the below lines at the end of the script and save the file.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-26\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003etr \":\" \",\" \u0026lt; extracted-data.txt \u0026gt; transformed-data.csv\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-27\"\u003e\u003cp\u003eRun the script and then it automatically save the .csv file to your system. you can read it from the terimnal by calling this command: cat transformed-data.csv\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-29\"\u003e\u003ch3\u003eTo load data from a shell script, you will use the\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003epsql\u003c/code\u003e\u0026nbsp;client utility in a non-interactive manner. This is done by sending the database commands through a command pipeline to\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003epsql\u003c/code\u003e\u0026nbsp;with the help of\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003eecho\u003c/code\u003e\u0026nbsp;command.\u003c/h3\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-30\"\u003e\u003cp\u003ePostgreSQL command to copy data from a CSV file to a table is\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003eCOPY\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eThe basic structure of the command which we will use in our script is\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-31\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eCOPY table_name FROM 'filename' DELIMITERS 'delimiter_character' FORMAT;\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-32\"\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-33\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eecho \"\\c template1;\\COPY users FROM '/home/project/transformed-data.csv' DELIMITERS ',' CSV;\" | psql --username=postgres --host=localhost\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-34\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eecho '\\c template1; \\\\SELECT * from users;' | psql --username=postgres --host=localhost\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-35\"\u003e\u003cimg style='width:720px; height:100%;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fetl_using_shell_script_img_3.png?alt=media\u0026token=ff23b79e-4971-4f41-9060-58b47f009358'/\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fetl_using_shell_scripts.png?alt=media\u0026token=e81bec9a-d988-429f-954c-eab34e3d0e11","image_alt":"ETL Using bash Script","short_description":"In this lab, we perform ETL (Extract, Transform, Load) using shell scripts. Learn to extract data from delimited files, transform text data with cut and tr, and load it into a PostgreSQL database. This hands-on approach will gain essential skills for automating data management and integration.","timestamp":"2024-09-20 11:36:39","title":"ETL using shell scripts"},{"blog_id":"0e1dd5fa-3510-411f-9dc9-85938efe6af9","description":"\u003cdiv id=\"content-1\"\u003e\u003ch2\u003eBig-O notation\u003c/h2\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eEvaluating an application's performance ensures that the code written is good and fit for purpose. The question is how do we evaluate efficiency? When we measure electricity, we use kilowatt-hours, which means how many kilowatts an appliance will use if it runs for an hour. The appliance will not always run for an hour, and it may have different requirements depending on the setting used, it is more of a general rule-of-thumb for evaluating cost.\u003c/p\u003e\u003cp\u003eWhen evaluating coding solutions, Big-O notation is used. So, Big-O notation is the kilowatt hour of code evaluation. It can be applied to measuring how much time a piece of code will take or how much space it will use in memory. Not all processors will run at the same speed, so instead of timing an application, you count the number of instructions an application initiates.\u003c/p\u003e\u003cp\u003eWhich measurement reflects the quickest possible execution of some code? Let's explore which measurement reflects the quickest possible execution of some code.\u003c/p\u003e\u003cp\u003e\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(1)\u003c/code\u003e\u0026nbsp;You use a constant time algorithm that takes O(1) (O-of-one) time to compute. This determines that it will only take one computation to complete a task. An example of this is to print an item from an array.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# An array with 5 numbers \narray = [0,1,2,3,4]\n\n# retrieve the number found at index location 3 \nprint(array[3]) \u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp\u003eIn this instance, no matter how many values exist in the array, the approach has a Big-O of one. This means that running this code is considered O(1).\u003c/p\u003e\u003cp\u003e\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(n)\u003c/code\u003e\u0026nbsp;Next, let's explore an example of O(n). Taking the same array, an if statement is written that looks for the number 5. To establish that 5 is not there, it has to check every item in the array.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# An array with 5 numbers \narray = [0,1,2,3,4] \n\nif 5 in array:\n    print(\"five is alive\")\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# an array with 10 numbers \narray = [0,1,2,3,4,6,7,8,9,10]\n\nif 5 in array:\n    print(\"five is still alive\")\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cp\u003e\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(log n)\u003c/code\u003e\u0026nbsp;This search is less intensive than O(n) but more work than O(1). O(log n) is a logarithmic search and it will increase as new inputs are added but these inputs only offer marginal increases. An excellent example of this in action is a binary search. Binary search is covered in more detail later in the course.\u003c/p\u003e\u003cp\u003eNow, imagine playing a guessing game with the following prompts: too high, too low, or correct. You are given a range of 100 to 1. You may decide to approach the problem systematically. First, you guess 50 ‚Äì too high. So, you guess 25 ‚Äì which is too high. You may choose then to go 12 or 13. What is happening here is that you are halving the search space with each guess.\u003c/p\u003e\u003cp\u003eSo, while the input to this function was 100 using a binary search approach, you should come upon the answer in under 5 or 6 guesses. This solution would have a time complexity of O(log n). Even if n (the range of numbers entered) is ten times bigger. It will not take ten times as many guesses.\u003c/p\u003e\u003cp\u003eHere is a breakdown of those steps on the array.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003earray = [0,1,2,3,4,6,7,8,9,10]\n\nprint(\"##Step One\")\nprint(\"Array\")\nprint(array)\nmidpoint = int(len(array)/2)\nprint(\"the midpoint at step one is: \" , array[midpoint])\n\nprint()\n\nprint(\"##Step Two\")\narray = array[:midpoint] # 6 is the midpoint of the array \nprint(\"Array\")\nprint(array)\n# running this shows the numbers left to check \n# is 5 \u0026lt; 3 \n# no \n# so discard the left hand side \n\n# so the array is halved again \nmidpoint=int(len(array)/2)\nprint(\"the midpoint is: \",  array[midpoint])\n\nprint()\nprint(\"##Step Three\") \narray = array[midpoint:] # so the array is halved at the midpoint\nprint(array)# check for the midpoint \nmidpoint=int(len(array)/2)\nprint(\"the midpoint is: \" , array[midpoint])\n# is 4 \u0026lt; 5 \n# yes look to the right\n\nprint()\nprint(\"##Step Four\") \nprint(array[midpoint:]) \n# check for the midpoint \narray = array[midpoint:] # so the array is halved at the midpoint\nmidpoint=int(len(array)/2)\n\n\n\nprint()\nprint(\"##Step Five\") \narray = array[midpoint:] \nprint(array)\nprint(\"only one value to check and it is not 5\")\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cp\u003eYou will notice that to determine if 5 is present, it took 5 steps. That is a big-O score of O(5). You can see that this is bigger than O(1) but smaller than O(n). Now, what happens when the array is extended to 100? When looking for a number in an array of 10, it took 5 guesses. Looking at an array of 100 will not take 50 guesses; it will take no more than 10. Equally, if the list is extended to 1000, the guesses will only go up to 15-20.\u003c/p\u003e\u003cp\u003eFrom this, we can see that it is not O(1) because the answer is not immediate. It is not big-O(n) because the number of guesses does not go up with the size n of the array. So here, one says that the complexity is O(log(n)).\u003c/p\u003e\u003cp\u003eTo gain greater insight into how the log values are only a gradual rise, look at a log table up to 100,000,000. This lens shows that O(log n) incurs only a minimal processing cost. Running a binary search on an array with any n values will, in a worst-case scenario, always make the number of computations found in the log values column.\u003c/p\u003e\u003cp\u003e\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(n^2)\u003c/code\u003e\u0026nbsp;is heavy on computation. This is quadratic complexity, meaning that the work is doubled for every element in the array. An excellent way to visualize this is to consider that you have a variety of arrays. In keeping with the earlier example, let's explore the following code:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003enew_array=[] # an array to hold all of the results \n# array with five numbers \narray = [0,1,2,3,4]\nfor i in range(len(array)): # the array has five values, so this is n=5 \n    for j in range(len(array)): # still the same array so n = 5 \n        new_array.append(i*j) # every computation made is stored here \n\nprint(len(new_array)) #how big is this new array ? \u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cp\u003eThe first loop wi ll equal the number of elements input, n. The second loop will also look at the number of input elements, n. So, the overall complexity of running this approach can be said to be n*n which is n^2 (n-squared). To find out how many computations were made, you have to print out the number of times n was used in the loop as below.\u003c/p\u003e\u003cp\u003eIf you know that the array has 25 elements, then you understand the principles of calculating Big-O notation. To further test your knowledge, how many computations would be required if n = 6? Meaning the array had 6 values? The answer is 6 x 6 so 36.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003ch1\u003eWorking with Space Complexity\u003c/h1\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eSome Algorithms like\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003ehash tables\u003c/code\u003e\u0026nbsp;provide very fast lookups in\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(10) time\u003c/code\u003e. However, to work efficiently, they must have a lookup every element stored. This results in a space complexity of O(n). The big O-notation for space complexity is the same as for the time O(1), O(log log n), O(log n) and so on. In all these notations\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003en\u003c/code\u003e\u0026nbsp;refers to the size of the input. This is often measured in bytes.\u003c/p\u003e\u003cp\u003eDifferent languages have different memory costs associated with them. In java for instance, an integer requires 4 bytes of memory. A blank array will consume 12 bytes for the header object and an additional 4 bytes for padding. Thus, if n refers to an array of integers size 4, then the total memory requirement is 32 bytes of memory.\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003e(4 * 4) + 12 + 4 = 32\u003c/code\u003e\u003c/p\u003e\u003cp\u003eWhen discussing space complexity, you have to consider what the increase in input size has on the overall usage. Space Compexity is the total of auxiliary space + input space The space complexity of a problem can be broken into two sections namely auxiliary and input space:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAuxiliary space is the space required to hold all data required for the solution. It refers to the temporary space needed to compute a given solution.\u003c/li\u003e\u003cli\u003eInput space refers to the space required to add data to the function, algorithm, application or system that you are evaluating.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst a = 1;\nconst b = 10;\nconst c = 100;\nconst sum = (a, b, c) =\u0026gt; a + b + c;\nconst d = sum(a, b, c);\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cp\u003eIn the above function sum, we can resolve the space complexity required by the memory requirement. Given that all arguments in the above example are integers and the return value is an integer of set size, we know this will be constant.\u003c/p\u003e\u003cp\u003eGiven the Number type in JavaScript is 64-bit (8 bytes), we can resolve that the memory requirement for a, b and c is (24). Therefore, the function sum has\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(1)\u003c/code\u003e\u0026nbsp;constant space complexity given that we know the constant requirement of 24 bytes of data space for this function.\u003c/p\u003e\u003cp\u003eLooking at an example in C that requires a dynamic amount of memory in an array:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eint sum(int a[], int n) {\n    int x = 0;      // 4 bytes for x\n    for(int i = 0; i \u0026lt; n; i++)  // 4 bytes for i\n    {\n        x  = x + a[i];\n    }\n    return(x);\n}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003cp\u003eFirstly, we know that int types in C require 4 bytes of space. Here we note that 4*n bytes of space is required for the array a[] of integers. The remaining variables x, n, i and the return value each require a constant 4 bytes each of memory give that they are integers.\u003c/p\u003e\u003cp\u003eThis gives us a total memory requirement of (4n + 12). This itself is\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(n)\u003c/code\u003e\u0026nbsp;linear space complexity since the memory requires linearly increases with input value n.\u003c/p\u003e\u003cp\u003eWhat is important to note with this example is that the auxiliary space required for the above sum function is actually\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(1)\u003c/code\u003e\u0026nbsp;constant given that the auxiliary variables are only x and i which totals a (8) memory requirement (constant).\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fcoding_interview.png?alt=media\u0026token=1e7a8927-e196-4b59-a0bd-202c9d190484","image_alt":"Coding interview Preparation","short_description":"In this reading, We will explore a worked example of a piece of code written in Python, along with how you would evaluate it using Big-O notation.","timestamp":"2024-09-20 11:35:52","title":"Working with Time \u0026 Space Complexity "},{"blog_id":"59ebedc0-f362-4c2b-a7ee-a5fd8db2bc29","description":"\u003cdiv id=\"content-0\"\u003e\u003cp class=\"ql-align-justify\"\u003eApache Airflow is an open-source platform designed for orchestrating and managing complex workflows and data pipelines. It allows users to programmatically author, schedule, and monitor workflows through a rich web-based user interface. Airflow uses directed acyclic graphs (DAGs) to represent workflows, making it highly flexible and scalable for a wide range of use cases. With built-in support for dynamic pipeline generation, Airflow enables users to create workflows that adapt to changes in data and business logic. Additionally, its extensible architecture allows integration\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003cimg src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_architecture.png?alt=media\u0026token=2ff31468-d678-4134-b46c-4f0da4f4ff93'/\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp class=\"ql-align-justify\"\u003eApache Airflow's architecture is designed to be modular and scalable, consisting of several key components: the Scheduler, the Executor, the Web Server, and the Metadata Database. The Scheduler handles the scheduling of tasks, ensuring they run at the correct times and in the correct order. The Executor runs the tasks, which can be distributed across multiple workers for scalability. The Web Server provides a user-friendly interface for managing and monitoring workflows, while the Metadata Database stores the state of tasks and workflows.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eWorkflows in Airflow are defined as Directed Acyclic Graphs (DAGs), where each node represents a task, and edges define the dependencies between tasks. This structure allows for clear visualization of the workflow and its dependencies. Tasks within a DAG can range from simple data processing steps to complex data transfer operations, and they can be scheduled to run at specific intervals or triggered by external events. Airflow supports a wide range of operators for different tasks, including BashOperator for running bash scripts, PythonOperator for executing Python code, and many more for interacting with various data systems and services. This flexibility and the ability to create dynamic, code-driven workflows make Airflow a powerful tool for orchestrating data pipelines and workflows.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eIn this lab we will learn how to install apache airflow and create some basic DAG to perform ETL task. First we want to make sure our apache airflow in in our local machine for demonstration. We will install using docker and docker compose for it. Lets create new folder and named it as we want. In this case im going to create folder name apache_airflow. Then lets\u0026nbsp;create a Dockerfile inside that folder and fill the file with this code:\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eFROM apache/airflow:latest\nUSER root\nRUN apt-get update \u0026\u0026 \\\n    apt-get -y install git \u0026\u0026 \\\n    apt-get clean\n\nUSER airflow\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cp class=\"ql-align-justify\"\u003eOverall, this Dockerfile snippet customizes the Airflow Docker image by adding Git, which can be useful for tasks such as cloning repositories or managing code directly within the Airflow environment. Lets run this docker file. If the build image is successful, we then jump to the next step where we are going to create a new file docker compose to use our image and create a volums for our data. Below is the example of how we build our compose to run apache airflow:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eversion: '3'\nservices:\n  apacheairflow:\n    image: apacheairflow:latest\n\n    volumes:\n      - ./airflow:/opt/airflow\n\n    ports:\n      - \"8080:8080\"\n    command: airflow standalone\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp class=\"ql-align-justify\"\u003eAfter this, we can access our apache airflow in our local machine by accessing trough \u003ca href=\"http://localhost:8080/\" target=\"_blank\"\u003ehttp://localhost:8080\u003c/a\u003e. Dont forget to build up our compose file. By running docker-compose up -d. We can type our username Admin and we wil get our password from our folder generated from /airflow/stand_alone_admin_password.txt.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eNow we can create our DAG file in our /airflow folder. Create new folder name dags and fill the python file inside of it. The name is our wishes.\u0026nbsp;Then we can create a simple task from it. But we need to import some thing like:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# Import the libraries\nfrom datetime import timedelta\n# The DAG object; we'll need this to instantiate a DAG\nfrom airflow.models import DAG\n# Operators; you need this to write tasks!\nfrom airflow.operators.python import PythonOperator\n\n# This makes scheduling easy\nfrom airflow.utils.dates import days_ago\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cp class=\"ql-align-justify\"\u003eNow we can define our function, DAG Argument, DAG Definitions, Define the task, and create a task pipeline from this file.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eIn this case. I just want to grab some data from the api and save it to json file.Lets create our functions first called extract here is the code for the extract data from the api random jokes:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003edef extract():\n    response = requests.get('https://official-joke-api.appspot.com/random_joke')\n    joke = response.json()\n\n    # Define the path to the JSON file\n    file_path = '/opt/airflow/jokes/joke.json'\n    \n    try:\n        # Ensure the directory exists\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        \n        # Read existing jokes from the JSON file if it exists\n        if os.path.exists(file_path):\n            with open(file_path, 'r') as f:\n                # Try to load the existing jokes, handle if file is empty\n                content = f.read()\n                jokes = json.loads(content) if content else []\n        else:\n            jokes = []\n\n        # Append the new joke to the list\n        jokes.append(joke)\n        \n        # Write the updated list of jokes to the JSON file\n        with open(file_path, 'w') as f:\n            json.dump(jokes, f, indent=4)\n        \n        print(f\"Joke saved to {file_path}: {joke['setup']} - {joke['punchline']}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n        raise\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cp\u003eAfter that we can create our DAG Arguments and DAG Definitions:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# You can override them on a per-task basis during operator initialization\ndefault_args = {\n    'owner': 'Darmawan',\n    'start_date': days_ago(0),\n    'email': ['darmawanjr88@gmail.com'],\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Define the DAG\ndag = DAG(\n    'jokes-callable-dag',\n    default_args=default_args,\n    description='My first DAG',\n    schedule_interval=timedelta(days=1),\n)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cp\u003eThen we can define the task named execute_extract to call the extract function\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cp\u003eAnd build the the task pipeline for now we just have a single task when we have multiple task we can call our defined task like a sequence call like task_1 \u0026gt;\u0026gt; task_2 \u0026gt;\u0026gt; task_3 and so on. Now we can go to the web UI and triggering the DAG that we build.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cimg src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_1.png?alt=media\u0026token=1b05157a-20dc-4748-bb40-ff9bbaeb23be'/\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cp\u003eHorray we dit it! We can create a simple task from the apache airflow. We can make another complex task from our idea and our problems that we faced from our life.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_python.jpg?alt=media\u0026token=f6c31e13-c63f-49c5-b0ad-b54ec8509560","image_alt":"Apache airflow with Python","short_description":"In this lab, you will explore the Apache Airflow web user interface (UI). You will then create a Direct Acyclic Graph (DAG) using PythonOperator and finally run it through the Airflow web UI.","timestamp":"2024-09-20 11:35:46","title":"Create a DAG for Apache Airflow with Python Operator"},{"blog_id":"cfb72d72-4754-48f1-b815-52edb986d17b","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1 class=\"ql-align-justify\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h1\u003e\u003cp class=\"ql-align-justify\"\u003eApache Kafka is an event streaming platform that helps in moving and storing large amounts of data in real-time. It is like a central hub where different sources of data can send their events, and these events can be consumed by various applications or systems.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eThink of it as a postal service. Imagine you have different people sending letters from different locations, and you want all these letters to be collected in one place so that anyone who needs them can access them. Apache Kafka acts as that central place where all the letters (events) are collected and stored. It ensures that the events are delivered reliably and can be accessed by multiple applications or systems.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eKafka is highly scalable, meaning it can handle a large volume of data and process it quickly. It is also fault-tolerant, which means it can recover from failures and ensure that no data is lost. Additionally, Kafka is open source, so it can be used for free and customized according to specific needs.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch1 class=\"ql-align-justify\"\u003e\u003cstrong\u003eArchitecture\u003c/strong\u003e\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_kafka_architecture.png?alt=media\u0026token=4f637bf3-00ec-4dc6-9fb9-9f5602e9d18a'/\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp class=\"ql-align-justify\"\u003eThe architecture of Apache Kafka consists of several key components that work together to enable the efficient streaming and processing of data. Here is a simplified explanation of the architecture and its components:\u003c/p\u003e\u003col\u003e\u003cli class=\"ql-align-justify\"\u003eProducers: Producers are the sources of data in Kafka. They send events or messages to Kafka topics. Topics are like categories or channels where events are organized and stored.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eTopics: Topics are the central entities in Kafka. They represent a specific category or stream of events. Producers send events to specific topics, and consumers subscribe to topics to receive those events.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eBrokers: Brokers are the servers in the Kafka cluster. They receive events from producers, store them, and distribute them to consumers. Each broker can handle a specific amount of data and provides fault tolerance by replicating data across multiple brokers.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eConsumers: Consumers are the applications or systems that subscribe to topics and receive events from Kafka. They process the events according to their specific requirements.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003ePartitions: Topics are divided into partitions, which are individual ordered sequences of events. Each partition is stored on a specific broker. Partitioning allows for parallel processing and scalability.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eConsumer Groups: Consumer groups are a way to scale the consumption of events. Multiple consumers can be part of a consumer group, and each consumer within the group will receive a subset of the events from the topic partitions.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eZooKeeper (deprecated in newer versions): ZooKeeper is a distributed coordination service that was used in older versions of Kafka for managing the cluster and maintaining metadata. However, in newer versions, Kafka Raft (KRaft) is used to eliminate the dependency on ZooKeeper.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eKafka Connect: Kafka Connect is a framework for importing and exporting data to and from Kafka. It allows seamless integration with external systems and data sources.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eKafka Streams: Kafka Streams is a library that enables stream processing of data within Kafka. It allows developers to build real-time applications and perform transformations, aggregations, and analytics on the data.\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch1 class=\"ql-align-justify\"\u003e\u003cstrong\u003eCore Component\u003c/strong\u003e\u003c/h1\u003e\u003cp class=\"ql-align-justify\"\u003eHere's a breakdown of the core components of Kafka:\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e1. Brokers: These are dedicated servers that receive, store, process, and distribute events. They are like the central hub for all the events.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e2. Topics: These are containers or databases of events. Each topic stores specific types of events, such as logs, transactions, or metrics.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e3. Partitions: Topics are divided into different partitions, which are like smaller sections within a topic. This helps with scalability and performance.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e4. Replications: Partitions are duplicated and stored in different brokers. This ensures fault tolerance and allows for parallel processing of events.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e5. Producers: These are client applications that publish events into topics. They can associate events with a key to ensure they go to the same partition. 6. Consumers: These are client applications that subscribe to topics and read events from them. They can read events as they occur or go back and read events from the beginning. To build an event streaming pipeline, we create topics, publish events using producers, and consume events using consumers. We can also use the Kafka command-line interface (CLI) to manage topics, producers, and consumers. Kafka provides a powerful and scalable solution for processing and analyzing streams of events.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch1 class=\"ql-align-justify\"\u003e\u003cstrong\u003eInstallation\u003c/strong\u003e\u003c/h1\u003e\u003cp class=\"ql-align-justify\"\u003eIn this lab we wil going to test the apache kafka using the docker compose. Here is the file of the docker compose, so you can test it immediately without thinking how to installed it.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eversion: '3.8'\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:latest\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n    ports:\n      - \"2181:2181\"\n\n  kafka:\n    image: confluentinc/cp-kafka:latest\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003eWe can now running this docker compose by type in the terminal : docker-compose up -d. and then wait for all the setup to be completed. Once we completed we can now access the kafka container using this command:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003edocker exec -it \u0026lt;kafka-container-id\u0026gt; bash\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cp\u003eNow we are inside of the container. Now we can create a topic inside this apache kafka. we simply create a topic named test. Here is the command in the terminal:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekafka-topics --create --topic test --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cp\u003eOnce we create a topic now we can prepare other 2 terminal for the apache kafka demonstration. where the one terminal is used fot the producer and the other one is going to be a consumer. Lets begin with the first termnal and type:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekafka-console-producer --topic test --bootstrap-server localhost:9092\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cp\u003eNow our producer topic is available in this terminal. we can type a few messages and hit\u0026nbsp;\u003ccode\u003eEnter\u003c/code\u003e\u0026nbsp;after each message. In the second terminal we are going to make the consumer. here is the command:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003edocker exec -it \u0026lt;kafka-container-id\u0026gt; kafka-console-consumer --topic test --bootstrap-server localhost:9092 --from-beginning\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cp\u003eNow we should see the messages we produced in the previous step. Here is the output of what he have been doing so far\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_1.png?alt=media\u0026token=09d39473-77b7-46a3-ac9d-2dd44b252e1a'/\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_2.png?alt=media\u0026token=b3885d54-45ce-41a7-8b43-3eccc3c52241'/\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eWe've successfully installed Apache Kafka using Docker and tested it by producing and consuming messages. This setup can be expanded with additional configurations and services as needed for our use case.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_kafka.png?alt=media\u0026token=50fe3034-c769-4d0b-b54c-5c0ad0bbd0e5","image_alt":"Apache Kafka","short_description":"In this lab we will be covering about what is the apache kafka. How it works and little bit cover about the installation and the basic workflow demonstration","timestamp":"2024-09-20 11:35:12","title":"Introduction to Apache Kafka"},{"blog_id":"8be4d7fd-b2ea-4b0f-ad3b-a6064553e06b","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003ePreviously we already install the apache kafka using the docker compose, create a simple workflow of how the apache is running. You can visit this site to learn the apache kafka installation and the basic workflow if you are not familir with these topic (\u003ca href=\"https://barbarpotato.github.io/#/lab/d1c5319b-3019-4576-9368-d3757bf35c6a\" target=\"_blank\" style=\"color: inherit; background-color: transparent;\"\u003eIntroduction to Apache Kafka\u003c/a\u003e). To create a Kafka Python client that interacts with your Kafka instance, you can use the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003econfluent-kafka\u003c/code\u003e\u0026nbsp;library, which is a high-performance Kafka client built on the librdkafka C library.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eInstall the Kafka Python Client\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFirst, you need to install the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003econfluent-kafka\u003c/code\u003e\u0026nbsp;library. You can do this using pip. Note: You can create virtual environment of your project folder if you dont want to install this library to your global library package. by using the command: python -m venv \u0026lt;the name of your virtual environment\u0026gt;. then activating the venv using this command: source \u0026lt;your_venv_name\u0026gt;/Scripts/activate.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003epip install confluent-kafka\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003ch3\u003e\u003cstrong\u003eProduce Message\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eCreate a Python script to produce messages to the Kafka topic\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003efrom confluent_kafka import Producer\nimport socket\n\ndef delivery_report(err, msg):\n    if err is not None:\n        print(f\"Message delivery failed: {err}\")\n    else:\n        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n\np = Producer({'bootstrap.servers': 'localhost:9092', 'client.id': socket.gethostname()})\n\ntopic = 'test'\n\nfor i in range(10):\n    p.produce(topic, key=str(i), value=f\"Message {i}\", callback=delivery_report)\n    p.poll(0)\n\np.flush()\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch3\u003e\u003cstrong\u003eConsume Messages\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eCreate another Python script to consume messages from the Kafka topic.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003efrom confluent_kafka import Consumer, KafkaError\n\nc = Consumer({\n    'bootstrap.servers': 'localhost:9092',\n    'group.id': 'mygroup',\n    'auto.offset.reset': 'earliest'\n})\n\nc.subscribe(['test'])\n\nwhile True:\n    msg = c.poll(1.0)\n\n    if msg is None:\n        continue\n    if msg.error():\n        if msg.error().code() == KafkaError._PARTITION_EOF:\n            print(f'%% {msg.topic()} [{msg.partition()}] reached end at offset {msg.offset()}')\n        elif msg.error():\n            raise KafkaException(msg.error())\n    else:\n        print(f'Received message: {msg.value().decode(\"utf-8\")}')\n\nc.close()\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003ethe consumer script initializes a Kafka consumer using the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eConsumer\u003c/code\u003e\u0026nbsp;class, subscribes to the specified topic(s) with the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003esubscribe\u003c/code\u003e\u0026nbsp;method, and retrieves messages from the Kafka topic using the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003epoll\u003c/code\u003e\u0026nbsp;method. Finally, the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eclose\u003c/code\u003e\u0026nbsp;method closes the consumer and commits the final offsets. This setup allows you to produce and consume messages using your Python Kafka client. Now you can save the file.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eTo run the producer and consumer, open two terminal windows or tabs. In the first terminal, run the producer by executing\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003epython producer.py\u003c/code\u003e, and in the second terminal, run the consumer by executing\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003epython consumer.py\u003c/code\u003e. The producer script will send 10 messages to the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003etest\u003c/code\u003e\u0026nbsp;topic, while the consumer script will receive and print these messages. The producer script initializes a Kafka producer using the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eProducer\u003c/code\u003e\u0026nbsp;class and sends messages to a specified topic with the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eproduce\u003c/code\u003e\u0026nbsp;method. A\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003edelivery_report\u003c/code\u003e\u0026nbsp;callback function confirms message delivery, and the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003epoll\u003c/code\u003e\u0026nbsp;method serves delivery reports, which should be called regularly. The\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eflush\u003c/code\u003e\u0026nbsp;method waits for all messages in the producer queue to be delivered. On the other hand, the consumer script initializes a Kafka consumer using the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eConsumer\u003c/code\u003e\u0026nbsp;class, subscribes to the specified topic(s) with the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003esubscribe\u003c/code\u003e\u0026nbsp;method, and retrieves messages from the Kafka topic using the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003epoll\u003c/code\u003e\u0026nbsp;method. Finally, the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eclose\u003c/code\u003e\u0026nbsp;method closes the consumer and commits the final offsets. This setup allows you to produce and consume messages using your Python Kafka client.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eHere is the example output about what we are doing:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_python_client_1.png?alt=media\u0026token=4c079070-9f99-436f-b81c-8b9d0dbada30'/\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_python_client_2.png?alt=media\u0026token=7a151ac1-e386-4aef-a99a-f2d72fe611fd'/\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003ch1\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eIn conclusion, we successfully set up Apache Kafka using Docker, enabling a reliable environment for message streaming. Following this, we implemented a Kafka Python client using the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003econfluent-kafka\u003c/code\u003e\u0026nbsp;library, which provides high-performance Kafka producer and consumer capabilities. The producer script was designed to initialize a Kafka producer, send messages to a specified topic, and confirm message delivery through a callback function, while regularly polling for delivery reports and ensuring all messages are delivered with a flush operation. The consumer script was crafted to initialize a Kafka consumer, subscribe to the desired topic(s), retrieve messages, and close the consumer after committing final offsets. By running the producer and consumer scripts in separate terminal windows, you verified the setup by successfully sending and receiving messages. This end-to-end setup demonstrates a functional Kafka environment and a practical Python client for producing and consuming messages, laying a strong foundation for building more complex message-driven applications.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_python.png?alt=media\u0026token=10c2db3d-b180-4bad-af2a-90aee92c47e1","image_alt":"Kafka with Pyhton","short_description":"In this lab we will bring back the apache kafka to work with the application environment. In this case we are going to use the apache kafka to a simple python application.","timestamp":"2024-09-20 11:35:05","title":"Build Kafka Python Client"},{"blog_id":"82db10ae-f820-45d6-b985-9ac22fa7046f","description":"\u003cdiv id=\"content-1\"\u003e\u003ch1\u003eWhat is GRPC?\u003c/h1\u003e\u003cp\u003egRPC (Google Remote Procedure Call) is a high-performance, open-source framework developed by Google that enables efficient communication between distributed systems using remote procedure calls (RPCs). It leverages HTTP/2 for transport, Protocol Buffers (protobuf) for serialization, and supports features like multiplexing, streaming, and low-latency communication, making it ideal for modern microservices architectures and real-time applications. gRPC is increasingly used in this era due to its ability to handle complex, high-throughput workloads while ensuring consistent, language-agnostic communication between services, which is crucial for building scalable, resilient, and performant systems.\u003c/p\u003e\u003cp\u003egRPC is highly relevant today due to its efficient communication using Protocol Buffers for serialization, which reduces message sizes and improves performance. It leverages HTTP/2 for lower latency and higher throughput, supports multiple programming languages, and provides strongly typed contracts for consistency. With built-in support for streaming, SSL/TLS for security, and automatic code generation, gRPC enhances development speed and ensures secure, real-time communication. Its compatibility with cloud-native platforms and microservices architectures further makes it a valuable tool for modern distributed systems.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgrpc_architecture.png?alt=media\u0026token=fff5674e-8a0b-4e46-805d-feed5b3cf0f5'/\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cp\u003eThe basic architecture of gRPC involves several key components that facilitate communication between a client and server. Here's a breakdown of the architecture:\u003c/p\u003e\u003ch3\u003e1.\u0026nbsp;\u003cstrong\u003eProtocol Buffers (Protobufs):\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDefinition Language\u003c/strong\u003e: Protobufs are used to define the service methods and message types. These definitions are stored in\u0026nbsp;\u003ccode\u003e.proto\u003c/code\u003e\u0026nbsp;files.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSerialization\u003c/strong\u003e: Protobufs serialize data into a compact binary format, which is efficient for transmission.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e2.\u0026nbsp;\u003cstrong\u003eService Definition:\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eService Interface\u003c/strong\u003e: In the\u0026nbsp;\u003ccode\u003e.proto\u003c/code\u003e\u0026nbsp;file, services are defined with methods that can be called remotely by clients. Each method specifies the request and response message types.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e3.\u0026nbsp;\u003cstrong\u003eCode Generation:\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eClient and Server Stubs\u003c/strong\u003e: The\u0026nbsp;\u003ccode\u003e.proto\u003c/code\u003e\u0026nbsp;file is used to generate client and server code in various languages. These stubs handle the serialization and deserialization of messages and the underlying gRPC protocol details.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e4.\u0026nbsp;\u003cstrong\u003eClient-Side:\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStub\u003c/strong\u003e: The client uses a generated stub to invoke methods on the server. The stub provides a local representation of the remote service.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eChannel\u003c/strong\u003e: The client creates a communication channel to the server, which manages the connection and handles the low-level details of the communication.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e5.\u0026nbsp;\u003cstrong\u003eServer-Side:\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e: The server implements the service methods defined in the\u0026nbsp;\u003ccode\u003e.proto\u003c/code\u003e\u0026nbsp;file. These methods process incoming requests and generate responses.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eServer\u003c/strong\u003e: The server listens for incoming requests, dispatches them to the appropriate service method implementations, and sends back responses.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e6.\u0026nbsp;\u003cstrong\u003eCommunication:\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHTTP/2\u003c/strong\u003e: gRPC uses HTTP/2 as the transport protocol, providing features like multiplexing, flow control, and header compression.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eStreams\u003c/strong\u003e: gRPC supports different types of RPCs, including unary (single request/response), server streaming, client streaming, and bidirectional streaming.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch1\u003eBasic gRPC Workflow:\u003c/h1\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eDefine the Service\u003c/strong\u003e: Create a \u003ccode\u003e.proto\u003c/code\u003e file with service and message definitions.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGenerate Code\u003c/strong\u003e: Use the \u003ccode\u003eprotoc\u003c/code\u003e compiler to generate client and server code from the \u003ccode\u003e.proto\u003c/code\u003e file.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImplement the Server\u003c/strong\u003e: Write the server-side code to implement the service methods.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCreate the Client\u003c/strong\u003e: Write the client-side code to call the service methods using the generated stub.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEstablish Communication\u003c/strong\u003e: The client and server communicate over a channel using HTTP/2, with data serialized and deserialized using Protobufs.\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch3\u003eIn gRPC, the client can indeed send data to the server. gRPC supports several types of communication patterns, including:\u003c/h3\u003e\u003ch3\u003e1. Unary RPC:\u003c/h3\u003e\u003cul\u003e\u003cli\u003eThe client sends a single request to the server and receives a single response.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003erpc SayHello (HelloRequest) returns (HelloReply) {}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003ch3\u003e2. Server Streaming RPC:\u003c/h3\u003e\u003cul\u003e\u003cli\u003eThe client sends a single request to the server and receives a stream of responses.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003erpc ListFeatures (Rectangle) returns (stream Feature) {}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003ch3\u003e3. Client Streaming RPC:\u003c/h3\u003e\u003cul\u003e\u003cli\u003eThe client sends a stream of requests to the server and receives a single response.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003erpc RecordRoute (stream Point) returns (RouteSummary) {}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003ch3\u003e4. Bidirectional Streaming RPC:\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBoth the client and server send a stream of messages to each other.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003erpc RouteChat (stream RouteNote) returns (stream RouteNote) {}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003ch1\u003eExample\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003cp\u003eLet's try to do the key list that we define in above. in this example we are going to implement the unary RPC for the sake of the simplicity. and in this example we are going to implement the grpc within the node.js with the helper of our beloved database called MongoDb. for this example what we are going to achieve is the client side can search the book data from the mongodb database. To ensure that the client can get the data successfully, the client side need to get interact with our grpc server. in a nutshell, the server side grpc will serve the client side. To create a gRPC server in Node.js, we need to install the required packages:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003enpm install @grpc/grpc-js @grpc/proto-loader\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cp\u003eafter the installation is complete, we then create a proto file. in this file we define the service and the message that we are going to build. here is the example code:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003esyntax = \"proto3\";\n\nservice Book {\n  rpc GetBook (GetBookRequest) returns (BookReply) {}\n}\n\nmessage GetBookRequest {\n  string Title = 1;\n}\n\nmessage BookReply {\n  string Title = 1;\n  string Author = 2;\n  int32 Published = 3;\n  string Language = 4;\n  string Id = 5;\n  int32 Sales = 6;\n}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003cp\u003ethe proto file is ready. now we can load it up trough our core script to running our node server. dont forget that we are using the mongodb database. so the first step is to setup the connection between the mongodb and the server, and then we build up the grpc service in a top of mongodb service.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-32\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst grpc = require('@grpc/grpc-js');\nconst protoLoader = require('@grpc/proto-loader');\nconst path = require('path');\nconst { MongoClient } = require('mongodb');\n\n// Load the protobuf file\nconst PROTO_PATH = path.resolve(__dirname, 'service.proto');\nconst packageDefinition = protoLoader.loadSync(PROTO_PATH);\nconst proto = grpc.loadPackageDefinition(packageDefinition);\n\n// MongoDB connection URL and Database name\nconst url = 'mongodb://localhost:27017';\nconst dbName = 'Book';\n\nlet db;\nlet client;\n\n// Connect to MongoDB and start gRPC server\nasync function main() {\n    const client = new MongoClient(url);\n\n    try {\n        await client.connect();\n        console.log('Connected to MongoDB');\n        db = client.db(dbName);\n\n        // Start gRPC server after MongoDB connection\n        startGrpcServer();\n    } catch (e) {\n        console.error('Failed to connect to MongoDB', e);\n    } finally {\n        // Ensure MongoClient is closed if needed\n        // Uncomment if you need to close the client here:\n        // await client.close();\n    }\n}\n\n\n// gRPC service method\nfunction GetBook(call, callback) {\n    console.log('Received request:', call.request);\n    const { Title } = call.request;\n    const collection = db.collection('list');\n\n    console.log('Performing findOne query with Book:', Title);\n    collection.findOne({ Book: Title }, { maxTimeMS: 5000 })\n        .then(result =\u0026gt; {\n            console.log('findOne query completed');\n            if (result) {\n                const response = {\n                    Title: result.Book,\n                    Author: result['Author(s)'], // Adjusted field names\n                    Published: result['First published'],\n                    Language: result['Original language'],\n                    Id: result._id.toString(),\n                    Sales: result['Approximate sales in millions']\n                };\n                console.log('Sending response:', response);\n                callback(null, response);\n            } else {\n                console.log('Book not found');\n                callback(null, { Title: \"Book not found\" });\n            }\n        })\n        .catch(err =\u0026gt; {\n            console.error('Error fetching data:', err);\n            callback(err, null);\n        });\n}\n\n\n// Start the gRPC server\nfunction startGrpcServer() {\n    const server = new grpc.Server();\n    server.addService(proto.Book.service, { GetBook: GetBook });\n    const port = '50051';\n    server.bindAsync(`0.0.0.0:${port}`, grpc.ServerCredentials.createInsecure(), (error, port) =\u0026gt; {\n        if (error) {\n            console.error(`Failed to bind server: ${error.message}`);\n            process.exit(1); // Exit process if server fails to start\n        }\n        console.log(`Server running at http://0.0.0.0:${port}`);\n    });\n}\n\n// Run the main function\nmain();\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-33\"\u003e\u003cp\u003eWe can now run the server. by using the command node \u0026lt;the name of your server code file\u0026gt;\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-34\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgrpc-step-1.png?alt=media\u0026token=cef277a6-d5e2-45d9-80f2-6871e3715089'/\u003e\u003c/div\u003e\u003cdiv id=\"content-35\"\u003e\u003cp\u003eThe setup and implementation of the server.js is completed. now we move to the code of the client side. which is more simple. here is the code to call the available service from the server side that we implement previously. where in here we can access the GetBook service. and try to search the field named book with value\u0026nbsp;Where the Crawdads Sing\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-36\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst grpc = require('@grpc/grpc-js');\nconst protoLoader = require('@grpc/proto-loader');\nconst path = require('path');\n\n// Load the protobuf file\nconst PROTO_PATH = path.resolve(__dirname, 'service.proto');\nconst packageDefinition = protoLoader.loadSync(PROTO_PATH);\nconst proto = grpc.loadPackageDefinition(packageDefinition);\n\n// Create a new gRPC client\nconst client = new proto.Book('localhost:50051', grpc.credentials.createInsecure());\n\n// Call the service\nclient.GetBook({ Title: 'Where the Crawdads Sing' }, (error, response) =\u0026gt; {\n    if (error) {\n        console.error(`Error: ${error.message}`);\n    } else {\n        console.log(`Response: ${JSON.stringify(response)}`);\n    }\n});\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-37\"\u003e\u003cp\u003eWe can run this client side. and it the result of the book that we search is delivered to the client side.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-38\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgrpc-step-2.png?alt=media\u0026token=85d657cc-6260-492d-a70e-62d26b038da2'/\u003e\u003c/div\u003e\u003cdiv id=\"content-40\"\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003egRPC is a powerful framework for building efficient, high-performance communication between distributed systems. By leveraging HTTP/2 and Protocol Buffers (protobuf), gRPC enables low-latency, scalable interactions ideal for microservices and real-time applications. In this example, we demonstrated how to set up a gRPC server in Node.js that interacts with a MongoDB database. We defined a service using Protocol Buffers, implemented the server to handle requests, and created a client to query book data. This approach allows for efficient data retrieval and robust communication between client and server, showcasing gRPC's capabilities in modern software development. By understanding gRPC's architecture and workflow, and implementing it in a real-world scenario, you can build scalable, high-performance systems that handle complex workloads with ease.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgrpc_background.png?alt=media\u0026token=3df4f4c7-ea5f-4170-ae16-6656671bfd4a","image_alt":"GRPC Background","short_description":"We will explore about the grpc, which is the better version of the basic HTTP1.1 and make communications more faster between client-server","timestamp":"2024-09-20 11:34:55","title":"Create GRPC With MongoDB \u0026 Node.js"},{"blog_id":"40c0ec5c-b8f4-4965-8bd6-be9f65e867fb","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eIn the fast-paced world of software development, delivering high-quality code consistently is crucial. One of the methodologies that can help achieve this goal is Test-Driven Development (TDD). TDD is not just a testing technique; it‚Äôs a design methodology that can profoundly impact how you write and maintain code. This blog post will explore what TDD is, its benefits, and how you can start incorporating it into your development workflow.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003ch1\u003e\u003cstrong\u003eWhat is Test-Driven Development (TDD)?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTest-Driven Development is a software development process where you write tests before writing the actual code. The cycle typically follows three steps, known as the \u003cstrong\u003eRed-Green-Refactor\u003c/strong\u003e cycle:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eRed\u003c/strong\u003e: Write a test for a new functionality. Since the functionality is not yet implemented, the test will fail (hence the \"red\" phase).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGreen\u003c/strong\u003e: Write the minimum amount of code required to pass the test. At this stage, you aim to make the test pass, even if the solution isn‚Äôt perfect.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRefactor\u003c/strong\u003e: Improve the code without changing its functionality. This might involve cleaning up code, optimizing performance, or improving readability.\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp\u003eThis cycle is repeated for every piece of functionality you want to add to your application.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch1\u003e\u003cstrong\u003eBenefits of TDD\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003eBetter Code Quality\u003c/strong\u003e: Test-Driven Development (TDD) promotes writing only the code needed to pass predefined tests, which helps prevent overengineering. This focus on minimalism means developers are less likely to introduce unnecessary complexity. Moreover, the tests themselves act as a safety net, identifying bugs early in the development process. By catching issues before they escalate, TDD ensures that the code remains clean, efficient, and easy to understand.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eImproved Design\u003c/strong\u003e: TDD forces developers to think about the design and functionality of their code before they start writing it. This upfront consideration leads to better-structured, more modular, and decoupled code. Since the tests are written first, they guide the design, ensuring that each piece of the codebase is independently testable and reusable. This modularity not only makes the code easier to maintain but also simplifies future enhancements and refactoring.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFaster Debugging\u003c/strong\u003e: One of the standout benefits of TDD is its ability to streamline the debugging process. When a test fails, it provides an immediate indication of where the problem lies, allowing developers to pinpoint the issue quickly. Instead of spending hours sifting through code to find the root cause of a bug, developers can focus on the specific area that triggered the test failure, significantly reducing debugging time.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eDocumentation\u003c/strong\u003e: TDD naturally results in a comprehensive suite of tests that serve as living documentation for the codebase. These tests provide concrete examples of how functions and classes are expected to behave, making it easier for other developers (or even your future self) to understand the code‚Äôs purpose and usage. Unlike traditional documentation, which can become outdated, these tests are constantly updated as the code evolves, ensuring they remain relevant and accurate.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eIncreased Confidence\u003c/strong\u003e: With a robust suite of tests in place, developers can refactor or extend their code with greater confidence. The tests act as a safeguard, ensuring that any unintended side effects are caught immediately. This level of assurance is particularly valuable when making significant changes to the codebase, as it minimizes the risk of introducing new bugs and helps maintain the overall stability of the application.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch1\u003e\u003cstrong\u003eCommon Misconceptions About TDD\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003eTDD is just about testing\u003c/strong\u003e: While the name might suggest that TDD is primarily focused on testing, it‚Äôs actually more about guiding the design of your code. The tests you write before the implementation help shape the structure and functionality of the code. TDD drives the development process, ensuring that the code is built to meet the specific requirements laid out in the tests, which results in a more thoughtful and deliberate design.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTDD slows down development\u003c/strong\u003e: At first glance, TDD may seem to slow down development, as it requires writing tests before the actual code. However, this initial investment pays off in the long run. By catching bugs early and reducing the time spent on debugging, TDD often leads to faster overall development. Additionally, the improved code quality and design that result from TDD can reduce the need for extensive refactoring later in the project, further accelerating the development process.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTDD is only for large projects\u003c/strong\u003e: Another common misconception is that TDD is only beneficial for large, complex projects. In reality, TDD can be applied to projects of any size. Even in small projects, TDD helps ensure that the codebase remains maintainable and free of bugs. By adopting TDD from the start, developers can build a strong foundation that makes it easier to scale the project as it grows. The principles of TDD‚Äîwriting clean, testable code‚Äîare universally applicable, regardless of the project's scope.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003eHere's a simple example of a Test-Driven Development (TDD) workflow using \u003ccode\u003eunittest\u003c/code\u003e in Python. This example demonstrates the TDD cycle: \u003cstrong\u003eRed-Green-Refactor\u003c/strong\u003e.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch1\u003e\u003cstrong\u003eScenario\u003c/strong\u003e:\u003c/h1\u003e\u003cp\u003eYou want to create a function that returns the factorial of a number.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003ch3\u003e\u003cstrong\u003eStep 1: Write a Failing Test (Red)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFirst, you write a test for the functionality you want to implement, even though the function doesn‚Äôt exist yet. This test will naturally fail because the function isn‚Äôt defined.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport unittest\n\nclass TestFactorialFunction(unittest.TestCase):\n    def test_factorial_of_5(self):\n        result = factorial(5)\n        self.assertEqual(result, 120)\n\nif __name__ == '__main__':\n    unittest.main()\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cp\u003e\u003cstrong\u003eExplanation\u003c/strong\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThis test checks if the \u003ccode\u003efactorial\u003c/code\u003e function correctly returns 120 when called with the argument \u003ccode\u003e5\u003c/code\u003e.\u003c/li\u003e\u003cli\u003eSince the \u003ccode\u003efactorial\u003c/code\u003e function doesn't exist, running this test will produce an error.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003ch3\u003e\u003cstrong\u003eStep 2: Implement the Minimum Code to Pass the Test (Green)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eNext, you implement the \u003ccode\u003efactorial\u003c/code\u003e function, just enough to pass the test.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003edef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cp\u003eNow, if you run the test, it should pass:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e$ python test_factorial.py\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e.\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003ch3\u003e\u003cstrong\u003eStep 3: Refactor the Code\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFinally, you can refactor the code if necessary to improve its design or efficiency. In this case, the code is already quite clean, so no major refactoring is needed. However, if there were redundant code or opportunities to optimize, you would do that in this step.\u003c/p\u003e\u003cp\u003eYou could also add more tests to cover additional cases, such as the factorial of \u003ccode\u003e0\u003c/code\u003e or negative numbers, ensuring your function is robust:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eclass TestFactorialFunction(unittest.TestCase):\n    def test_factorial_of_5(self):\n        result = factorial(5)\n        self.assertEqual(result, 120)\n\n    def test_factorial_of_0(self):\n        result = factorial(0)\n        self.assertEqual(result, 1)\n\n    def test_factorial_of_1(self):\n        result = factorial(1)\n        self.assertEqual(result, 1)\n\n    def test_factorial_of_negative(self):\n        with self.assertRaises(ValueError):\n            factorial(-1)\n\nif __name__ == '__main__':\n    unittest.main()\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003cp\u003e\u003cstrong\u003eExplanation\u003c/strong\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThese additional tests handle edge cases, such as when \u003ccode\u003en\u003c/code\u003e is \u003ccode\u003e0\u003c/code\u003e, \u003ccode\u003e1\u003c/code\u003e, or negative.\u003c/li\u003e\u003cli\u003eThe \u003ccode\u003etest_factorial_of_negative\u003c/code\u003e test checks that the function raises a \u003ccode\u003eValueError\u003c/code\u003e when given a negative input.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf the function doesn't handle these cases yet, you would modify it:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003edef factorial(n):\n    if n \u0026lt; 0:\n        raise ValueError(\"Factorial is not defined for negative numbers\")\n    elif n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n - 1)\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eFrom this simple scenario, we know that the Test-Driven Development is more than just a testing technique‚Äîit‚Äôs a powerful design methodology that can lead to cleaner, more maintainable, and bug-free code. By adopting TDD, you can improve your development process, catch bugs early, and build software that stands the test of time. If you haven‚Äôt tried TDD yet, now is the perfect time to start. Happy coding!\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Ftest-driven-development-TDD.webp?alt=media\u0026token=c42519d2-3fd0-4451-af98-f2626ba059ba","image_alt":"tdd life cycle","short_description":"In the fast-paced world of software development, delivering high-quality code consistently is crucial. One of the methodologies that can help achieve this goal is Test-Driven Development (TDD). ","timestamp":"2024-09-20 11:34:49","title":"Test-Driven Development with Python"},{"blog_id":"b731f0f7-895d-46df-9377-cb5bf9008e0a","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eWhat is Redis?\u003c/h1\u003e\u003cp\u003eRedis is a key-value store that operates entirely in memory, making it significantly faster compared to traditional databases that store data on disk. Redis is known as a \u003cstrong\u003eNoSQL\u003c/strong\u003e database and is often used in applications that require high-speed data access.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch2\u003eAdvantages of Redis\u003c/h2\u003e\u003cp\u003eRedis offers several advantages that make it an appealing choice for many developers:\u003c/p\u003e\u003ch3\u003e1. \u003cstrong\u003eSpeed\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRedis stores data in memory rather than on disk. This means read and write operations occur in milliseconds, making Redis ideal for applications that require very fast response times.\u003c/p\u003e\u003ch3\u003e2. \u003cstrong\u003eSupport for Various Data Structures\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRedis doesn‚Äôt just store strings but also supports various data structures such as hashes, lists, sets, and sorted sets. This provides flexibility in how data can be organized and accessed.\u003c/p\u003e\u003ch3\u003e3. \u003cstrong\u003eData Persistence\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eAlthough Redis is an in-memory database, it has options to save data to disk, either periodically or based on direct commands. This ensures that data is not lost when Redis is shut down or crashes.\u003c/p\u003e\u003ch3\u003e4. \u003cstrong\u003eReplication\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRedis supports master-slave replication, where data can be replicated to multiple slave servers. This not only increases data availability but also allows Redis to scale the read load.\u003c/p\u003e\u003ch3\u003e5. \u003cstrong\u003ePub/Sub Mechanism\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRedis features a publish/subscribe mechanism that allows systems to communicate in real-time by subscribing to certain channels and receiving messages whenever they are published to those channels. This is particularly useful for building real-time applications such as chat systems and notifications.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003ch1\u003eUsing Redis in Applications\u003c/h1\u003e\u003cp\u003eRedis is often used in various scenarios, some of which include:\u003c/p\u003e\u003ch3\u003e1. \u003cstrong\u003eCache\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRedis is frequently used as a cache to store database query results or frequently accessed data, reducing the load on the main database and improving application response times.\u003c/p\u003e\u003ch3\u003e2. \u003cstrong\u003eSession Store\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eMany web applications use Redis to store user sessions. Using Redis allows sessions to be quickly accessed and deleted when no longer needed.\u003c/p\u003e\u003ch3\u003e3. \u003cstrong\u003eQueue Management\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRedis is used to manage job queues in distributed systems, such as background jobs, task scheduling, and more.\u003c/p\u003e\u003ch3\u003e4. \u003cstrong\u003eReal-time Analytics\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eDue to its speed, Redis is often used for real-time data processing, such as counting website visitors or monitoring application performance.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch1\u003eInstallation\u003c/h1\u003e\u003cp\u003eIn this lab, we are going to use the Redis services provided by vercel platform. To integrate Redis from Vercel into your Next.js project, you can follow these steps:\u003c/p\u003e\u003col\u003e\u003cli\u003eGo to the Vercel Redis dashboard and create a new Redis instance.\u003c/li\u003e\u003cli\u003eConfigure the Redis instance with the desired settings.\u003c/li\u003e\u003cli\u003eAfter creating the Redis instance, you will receive a connection URL, which typically looks like \u003ccode\u003erediss://:\u0026lt;password\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u003c/code\u003e.\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003ch3\u003eInstall Redis Client for Next.js\u003c/h3\u003e\u003cp\u003eYou will need a Redis client to connect to the Redis instance in your Next.js project. You can use the \u003ccode\u003eioredis\u003c/code\u003e library, a popular Redis client for Node.js.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003enpm install ioredis\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch3\u003eConfigure Environment Variables\u003c/h3\u003e\u003cp\u003eAdd the Redis connection URL to your environment variables. Create a \u003ccode\u003e.env.local\u003c/code\u003e file in the root of your Next.js project:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eREDIS_URL=rediss://:\u0026lt;password\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003ch3\u003eConnect to Redis in Next.js\u003c/h3\u003e\u003cp\u003eCreate a utility function to handle Redis connections. You can create a file like \u003ccode\u003eredis.js\u003c/code\u003e inside the \u003ccode\u003elib\u003c/code\u003e folder (or any other suitable location in your project):\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e// lib/redis.js\nimport Redis from 'ioredis';\n\nlet client;\n\nif (!client) {\n  client = new Redis(process.env.REDIS_URL);\n}\n\nexport default client;\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003ch3\u003eUse Redis in API Routes or Server Components\u003c/h3\u003e\u003cp\u003eYou can now use this Redis client in your API routes or server components. Here is an example of an API route using Redis:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport redis from \"@/app/libs/redis\";\n\nconst getHandler = async (req, res) =\u0026gt; {\n\n    try {\n\n        // Get the value from Redis\n        const value = await redis.get(\"Darwin AI: Active\");\n\n        // Send the response of a darwin service status\n        if (value) {\n            res.status(200).json({ \"Darwin\": \"Active\" });\n        } else {\n            res.status(200).json({ \"Darwin\": \"Inactive\" });\n        }\n    } catch (error) {\n        res.status(500).json({ error });\n    }\n}\n\nconst postHandler = async (req, res) =\u0026gt; {\n\n    try {\n        const current_timestamp = Date.now();\n\n        // Set a value in Redis. make it expired after 10 minutes\n        await redis.set(\"Darwin AI: Active\", current_timestamp, 'EX', 600);\n\n        // Get the value from Redis\n        const value = await redis.get(\"Darwin AI: Active\");\n\n        // Send the response of a darwin service status\n        if (value) {\n            res.status(200).json({ \"Darwin\": \"Active\" });\n        } else {\n            res.status(200).json({ \"Darwin\": \"Inactive\" });\n        }\n    } catch (error) {\n        res.status(500).json({ error });\n    }\n}\n\n\nconst handler = async (req, res) =\u0026gt; {\n    if (req.method === 'GET') {\n        return getHandler(req, res);\n    }\n    else if (req.method === 'POST') {\n        return postHandler(req, res);\n    }\n    else if (req.method === 'DELETE') {\n        return deleteHandler(req, res);\n    }\n    else {\n        return res.status(405).end(`Method ${req.method} Not Allowed`);\n    }\n};\n\nexport default handler;\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eRedis is a powerful and versatile tool in the world of application development. With its high speed, support for various data structures, and advanced features like persistence and replication, Redis can be used to significantly enhance application performance. In today‚Äôs fast-paced world, Redis offers an efficient solution to many challenges faced by developers.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fnext_redis.png?alt=media\u0026token=c1f3ad9d-f034-4581-91ab-40d65814e491","image_alt":"redis in next.js","short_description":"Redis, which stands for Remote Dictionary Server, is an in-memory data structure store that is commonly used as a database, cache, and message broker. ","timestamp":"2024-09-20 11:34:41","title":"Build Redis in Next.js"},{"blog_id":"1fab12ad-ebb2-435c-9dc7-17e4015e9b95","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eUnderstanding k6: A Load and Performance Testing Tool\u003c/h1\u003e\u003cp\u003eWhen developing an application, ensuring it can handle high traffic is crucial. This is where load testing and performance testing come into play, and \u003cstrong\u003ek6\u003c/strong\u003e is a powerful tool designed for this purpose. By using k6, you can assess whether your application can handle heavy traffic and perform optimally under various conditions.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch1\u003eWhat is k6?\u003c/h1\u003e\u003cp\u003ek6 is a command-line based tool specifically designed for load testing and performance testing. It allows you to simulate multiple users accessing your application, which helps identify potential issues when traffic spikes.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eKey Points About k6:\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTerminal-Based:\u003c/strong\u003e k6 operates directly through the terminal.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eJavaScript Testing:\u003c/strong\u003e Tests are written in JavaScript, making it accessible for web developers.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003eWhat k6 Does Not Do\u003c/h2\u003e\u003cp\u003eWhile k6 is a robust tool, it has its limitations:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNo Browser Interaction:\u003c/strong\u003e k6 does not run through a browser.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNo Node.js Dependency:\u003c/strong\u003e Unlike many JavaScript tools, k6 doesn't rely on Node.js but is built using Golang.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNo npm Modules Support:\u003c/strong\u003e If you require additional modules, you can't install them via \u003ccode\u003enpm install\u003c/code\u003e. Instead, you must include the files directly in your project.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003eLimitations of k6\u003c/h2\u003e\u003cp\u003ek6 uses Golang's Goja library to execute JavaScript, which means it doesn‚Äôt support some of the features available in Node.js. This limitation requires a different approach when setting up your testing environment.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003ch2\u003eInstallation\u003c/h2\u003e\u003cp\u003eYou can easily install k6 by following the instructions on their \u003ca href=\"https://k6.io/docs/get-started/installation/\" target=\"_blank\"\u003eofficial installation page\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003ch1\u003eInitial Project Setup\u003c/h1\u003e\u003cp\u003eTo get started with k6, you can set up a Node.js project:\u003c/p\u003e\u003cul\u003e\u003cli\u003eInitialize the project with \u003ccode\u003enpm init\u003c/code\u003e and set the type to \u003ccode\u003emodule\u003c/code\u003e.\u003c/li\u003e\u003cli\u003eInstall k6 as a module in your project.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003ek6 Library\u003c/h2\u003e\u003cp\u003ek6 comes with its own library that simplifies the process of creating tests. However, remember that the npm package for k6 doesn't contain actual JavaScript code‚Äîit‚Äôs mostly metadata. The k6 tool itself is built on Golang, so it doesn‚Äôt run on Node.js.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cp\u003eIf you inspect the k6 module, you‚Äôll find that it‚Äôs essentially an empty package with just a \u003ccode\u003epackage.json\u003c/code\u003e format.\u003c/p\u003e\u003ch3\u003eWriting k6 Scripts\u003c/h3\u003e\u003cp\u003eScripts in k6 are written in JavaScript and contain the code for the performance test along with the test scenarios you wish to execute. You can create a script manually or use a command to generate a basic script:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ek6 new src/ping.js\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cp\u003eThis command will create a new script at the specified location with a basic setup for performance testing.\t\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003ch3\u003eRunning a Script\u003c/h3\u003e\u003cp\u003eTo execute the performance test, use the following command:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ek6 run lokasi/file.js\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003ch1\u003eChecking the code\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport http from 'k6/http';\nimport { sleep } from 'k6';\n\nexport const options = {\n  stages: [\n    { duration: '10s', target: 10 },\n    { duration: '20s', target: 30 },\n    { duration: '30s', target: 50 },\n    { duration: '10s', target: 0 }\n  ]\n  // // A number specifying the number of VUs to run concurrently.\n  // vus: 10,\n  // // A string specifying the total duration of the test run.\n  // duration: '30s',\n\n  const body = {\n    name: \"Darmawan\"\n  };\n};\n\nexport default function () {\n\n  http.post(\"https://httpbin.test.k6.io/post\", JSON.stringify(body), {\n    headers: {\n      \"Content-Type\": \"application/json\",\n      \"Accept\": \"application/json\"\n    }\n  })\n\n  http.get('https://cerberry-backend.vercel.app/blogs/all');\n  sleep(0.1);\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cp\u003ewe'll explore a simple yet effective k6 script designed to conduct load testing on an API. The script simulates multiple virtual users (VUs) interacting with the application, allowing us to observe how it performs under varying levels of traffic. To start, the script imports the necessary modules from k6‚Äî\u003ccode\u003ehttp\u003c/code\u003e for making HTTP requests and \u003ccode\u003esleep\u003c/code\u003e to simulate user think time by pausing execution for a short duration.\u003c/p\u003e\u003cp\u003eThe core of the script lies in the \u003ccode\u003eoptions\u003c/code\u003e object, where we define a series of stages that dictate how the load should be applied. The test begins by gradually ramping up to 10 virtual users over the first 10 seconds. It then continues to increase to 30 users over the next 20 seconds, and finally reaches 50 users over a 30-second period. After this peak, the number of users is gradually reduced back to zero over the last 10 seconds. This staged approach provides valuable insights into the scalability and robustness of the application as it experiences increasing and then decreasing traffic.\u003c/p\u003e\u003cp\u003eWithin the \u003ccode\u003edefault\u003c/code\u003e function, the script defines two key actions that each virtual user will perform. First, a POST request is made to \u003ccode\u003ehttps://httpbin.test.k6.io/post\u003c/code\u003e, sending a JSON payload with a name value of \"Darmawan\". This simulates a scenario where a user might submit data to the server, such as filling out a form. Immediately after, a GET request is sent to \u003ccode\u003ehttps://cerberry-backend.vercel.app/blogs/all\u003c/code\u003e, retrieving a list of blogs. This sequence mirrors typical user behavior in a web application, where data submission is often followed by data retrieval or refreshing content. To simulate a brief delay between these actions, the script includes a \u003ccode\u003esleep(0.1)\u003c/code\u003e function, representing a short pause of 0.1 seconds.\u003c/p\u003e\u003cp\u003eTo execute this script, you would simply run it using the k6 command in your terminal. This approach allows you to assess how well your application can handle varying levels of traffic, identify potential bottlenecks, and ensure that it remains performant under load. By gradually increasing and then decreasing the load, this script provides a comprehensive view of your application's performance, helping you make informed decisions about optimizations and improvements.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003ch2\u003eUnderstanding the Summary Output\u003c/h2\u003e\u003cp\u003eAfter running a performance test, k6 generates a summary output that details the test results. By default, this output is displayed in the terminal, but you can also export it as a JSON file:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ek6 run file/script.js --summary-export output.json\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cp\u003eYou can find more information on the output metrics in the \u003ca href=\"https://grafana.com/docs/k6/latest/using-k6/metrics/reference/\" target=\"_blank\"\u003eGrafana documentation\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003ch2\u003eUsing the Web Dashboard\u003c/h2\u003e\u003cp\u003ek6 also offers a Web Dashboard for real-time monitoring of your tests. To enable this feature, you need to activate it through an environment variable. Detailed instructions are available in the \u003ca href=\"https://grafana.com/docs/k6/latest/results-output/web-dashboard/\" target=\"_blank\"\u003eWeb Dashboard documentation\u003c/a\u003e.\u003c/p\u003e\u003ch2\u003eUsing Stages for Dynamic Load Testing\u003c/h2\u003e\u003cp\u003ek6 provides a powerful feature called \u003cstrong\u003eStages\u003c/strong\u003e, which allows you to increase or decrease the number of virtual users during the test. This is particularly useful for simulating different traffic patterns over time. You can learn more about this feature in the \u003ca href=\"https://grafana.com/docs/k6/latest/using-k6/k6-options/reference/#stages\" target=\"_blank\"\u003eStages documentation\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eBy using k6 effectively, you can ensure your application is well-prepared to handle real-world traffic, preventing potential issues before they reach your users.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fk6.png?alt=media\u0026token=ee045685-94ba-421d-a189-aeb94cd7aa8c","image_alt":"k6 - performance testing","short_description":"k6 is a command-line based tool specifically designed for load testing and performance testing. It allows you to simulate multiple users accessing your application, which helps identify potential issues when traffic spikes.","timestamp":"2024-09-20 11:34:02","title":"Performance Testing with k6"}]},"__N_SSG":true},"page":"/","query":{},"buildId":"A-n-baZxhaPab-FReiSSJ","assetPrefix":"/Labs","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>