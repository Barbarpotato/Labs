<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Mastering Apache Spark: An Engaging Dive into Its Architecture and Clusters</title><meta name="description" content="Welcome to an in-depth exploration of Apache Spark’s architecture! Whether you’re new to Spark or looking to refresh your understanding, this interactive guide will walk you through the key concepts that power Spark’s ability to process massive datasets quickly and efficiently."/><meta property="og:title" content="Mastering Apache Spark: An Engaging Dive into Its Architecture and Clusters"/><meta property="og:description" content="Welcome to an in-depth exploration of Apache Spark’s architecture! Whether you’re new to Spark or looking to refresh your understanding, this interactive guide will walk you through the key concepts that power Spark’s ability to process massive datasets quickly and efficiently."/><meta property="og:type" content="article"/><meta property="og:url" content="https://barbarpotato.github.io/labs/undefined"/><meta name="next-head-count" content="8"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/Labs/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/Labs/_next/static/chunks/webpack-a39982bd6f80347b.js" defer=""></script><script src="/Labs/_next/static/chunks/framework-ecc4130bc7a58a64.js" defer=""></script><script src="/Labs/_next/static/chunks/main-1e09b50edce1e67f.js" defer=""></script><script src="/Labs/_next/static/chunks/pages/_app-753213cf38d40131.js" defer=""></script><script src="/Labs/_next/static/chunks/pages/labs/%5Bslug%5D-b6b888b0a2f30cff.js" defer=""></script><script src="/Labs/_next/static/A-n-baZxhaPab-FReiSSJ/_buildManifest.js" defer=""></script><script src="/Labs/_next/static/A-n-baZxhaPab-FReiSSJ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><article><h1>Mastering Apache Spark: An Engaging Dive into Its Architecture and Clusters</h1><p>2024-10-07 05:37:09</p><div><div id="content-0"><h1>Spark Architecture</h1><p>Imagine a Spark application as a bustling city. At the heart of this city is the <strong>Driver Program</strong>, which acts like the mayor overseeing everything that happens. The driver program is responsible for running your code, coordinating work, and making key decisions about how tasks should be executed. Like a city planner, it organizes and manages the tasks to be performed by the cluster, breaking down large jobs into smaller units of work. These jobs are then divided into <strong>tasks</strong> that Spark can run in parallel. The beauty of this architecture lies in its ability to scale, allowing multiple tasks to be completed simultaneously on different data partitions. These tasks are dispatched to <strong>executors</strong>, which are the hard workers of the cluster, doing the heavy lifting.</p></div><div id="content-4"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FApache-Spark-architecture.png?alt=media&token=ae8b882a-a359-4e63-aad6-a9a9f0a06ff8'/></div><div id="content-5"><p>Executors are independent workers spread across the cluster, each taking responsibility for a portion of the work. They are not just performing tasks but are also responsible for caching data, which can speed up future computations by reusing cached data rather than starting from scratch. Much like how a construction team works efficiently by reusing tools and materials at the job site, executors are optimized to perform computations without redundant effort.</p><p>To help you better visualize this, think of a project that requires breaking down and shipping parts to different locations. Each part can be processed independently and reassembled once all pieces are completed. The driver program ensures these tasks are coordinated correctly, and executors handle each piece of the job.</p><p>But what’s the relationship between <strong>jobs</strong>, <strong>tasks</strong>, and <strong>stages</strong>? Imagine you're the head of a large construction project, and you need to break down the overall project (the <strong>job</strong>) into manageable tasks for each construction team. A <strong>task</strong> in Spark operates on a specific partition of data, much like a construction team focusing on a particular section of the building. Once a group of tasks that don’t depend on any other data is identified, Spark bundles them into <strong>stages</strong>. A stage represents a set of tasks that can be executed independently without needing data from elsewhere in the project. However, sometimes a task will need information from another part of the dataset, requiring what’s known as a <strong>data shuffle</strong>. This shuffle is like coordinating deliveries between different construction teams—an operation that can slow things down due to the necessary data exchange, but essential for the overall completion of the job.</p><p>What happens when Spark needs to run in different environments? That’s where <strong>cluster modes</strong> come in. There are several ways Spark can be deployed, each suited to different use cases. The simplest is <strong>local mode</strong>, ideal for testing on your own machine. Imagine local mode as running your city’s planning department with just one employee—the driver program manages everything on its own, without help from external workers. This is great for testing, but when you need real performance, it’s time to deploy Spark in a cluster.</p><p>In a full cluster setup, Spark supports several modes. <strong>Spark Standalone</strong> is the quickest way to set up a cluster environment, ideal for smaller projects or when you want complete control over the infrastructure. For those already working in large-scale environments with Hadoop, <strong>YARN</strong> is a natural choice. It integrates seamlessly with the broader Hadoop ecosystem, making it easy to manage resources. For greater flexibility and the ability to handle dynamic workloads, <strong>Apache Mesos</strong> comes into play, providing a more robust partitioning and scaling system.</p><p>But if you’re looking for a modern, cloud-native approach, <strong>Kubernetes</strong> offers powerful benefits. Running Spark on Kubernetes is like managing a city that can grow or shrink as needed, using containers to deploy and scale your Spark applications. With Kubernetes, Spark becomes highly portable, making it easy to run your Spark jobs in any cloud environment. You can set up your Spark application inside containers and have them scale automatically based on demand, ensuring smooth processing even as workloads increase.</p></div><div id="content-6"><p><br></p><h1>Mastering the Art of Running Apache Spark Applications</h1><p>Ever wondered how to launch your Apache Spark application into action? Whether you’re processing mountains of data or just running local tests, Apache Spark’s flexibility makes it incredibly powerful. But let’s be real—understanding how to run a Spark application might seem daunting at first. Fear not! Here’s your step-by-step guide to Spark mastery.</p><p>At the heart of it all is the <code>spark-submit</code> script. Think of it as Spark’s personal conductor, ensuring your application runs smoothly across a distributed cluster or right on your local machine. With <code>spark-submit</code>, you’ve got full control: it lets you specify everything from the cluster manager you want to connect to, to how much memory and CPU cores your application needs. You can also include any additional files or libraries your app requires, making sure all the pieces are in place for a flawless run.</p><p>Now, let’s talk dependencies. In Spark, making sure the driver and executors have access to the right libraries is crucial. If you’re using Java or Scala, bundling all your code and libraries into a single uber-JAR (or fat JAR) is a common approach. This neat package ensures that everything is shipped out and accessible where it’s needed. For Python applications—aka PySpark—you’ll want to ensure that each node in your cluster has the exact same Python libraries installed. Imagine trying to run a marathon with mismatched shoes—it won’t end well, right? Same idea with your Spark dependencies.</p><p>If you’re in the mood for a more hands-on, experimental approach, then the <strong>Spark Shell</strong> is your playground. This interactive tool lets you dive right into Spark with either Scala or Python, without needing to write and submit an entire application. When you fire up the Spark Shell, it automatically sets up everything for you—giving you instant access to Spark’s APIs. You can run quick computations, play around with datasets, and see the results in real-time, making it perfect for debugging or just satisfying your curiosity.</p><p>So, to sum it all up: running an Apache Spark application is as easy as using <code>spark-submit</code> to launch your code, bundling your dependencies into an uber-JAR (or ensuring Python libraries are ready), and—if you're feeling adventurous—jumping into the Spark Shell for some interactive magic. Spark truly puts the power of distributed computing at your fingertips.</p></div><div id="content-7"><p>We now trying to submit Apache Spark applications from a python script. This exercise is straightforward thanks to Docker Compose. In this lab, you will:</p><ul><li>Install a Spark Master and Worker using Docker Compose</li><li>Create a python script containing a spark job</li><li>Submit the job to the cluster directly from python (Note: you’ll learn how to submit a job from the command line in the Kubernetes Lab)</li></ul><p><br></p></div><div id="content-8"><h2>Install a Apache Spark cluster using Docker Compose</h2></div><div id="content-9"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">git clone https://github.com/big-data-europe/docker-spark</code></pre></div><div id="content-11"><p>change to that directory and attempt to docker-compose up</p></div><div id="content-12"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">cd docker-spark
docker-compose up</code></pre></div><div id="content-13"><p>After quite some time you should see the following message:</p><p><em>Successfully registered with master spark://&lt;server address&gt;:7077</em></p></div><div id="content-14"><h2>Create Code</h2></div><div id="content-15"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">import findspark
findspark.init()
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, StringType
sc = SparkContext.getOrCreate(SparkConf().setMaster('spark://localhost:7077'))
sc.setLogLevel("INFO")
spark = SparkSession.builder.getOrCreate()
spark = SparkSession.builder.getOrCreate()
df = spark.createDataFrame(
    [
        (1, "foo"),
        (2, "bar"),
    ],
    StructType(
        [
            StructField("id", IntegerType(), False),
            StructField("txt", StringType(), False),
        ]
    ),
)
print(df.dtypes)
df.show()</code></pre></div><div id="content-16"><h2><strong>Execute code / submit Spark job</strong></h2><p>Now we execute the python file we saved earlier.</p><p>In the terminal, run the following commands to upgrade the pip installer to ensure you have the latest version by running the following commands.Now we execute the python file we saved earlier.</p></div><div id="content-17"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">rm -r ~/.cache/pip/selfcheck/
pip3 install --upgrade pip
pip install --upgrade distro-info</code></pre></div><div id="content-18"><p>Please enter the following commands in the terminal to download the spark environment.</p></div><div id="content-20"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">wget https://archive.apache.org/dist/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz 
&& tar xf spark-3.3.3-bin-hadoop3.tgz && rm -rf spark-3.3.3-bin-hadoop3.tgz</code></pre></div><div id="content-21"><p>Run the following commands to set up the&nbsp;&nbsp;which is preinstalled in the environment and&nbsp;&nbsp;which you just downloaded.</p></div><div id="content-22"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">export JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64
export SPARK_HOME=/home/project/spark-3.3.3-bin-hadoop3</code></pre></div><div id="content-23"><p>Install the required packages to set up the spark environment.</p></div><div id="content-24"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">pip install pyspark
python3 -m pip install findspark</code></pre></div><div id="content-25"><p>Type in the following command in the terminal to execute the Python script.</p></div><div id="content-26"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">python3 submit.py</code></pre></div><div id="content-27"><p>go to port 8080 to see the admin UI of the Spark master</p></div><div id="content-28"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fdemo-spark-submit.png?alt=media&token=b8fe9a3e-6bcf-42f4-82a7-2b1842b11e74'/></div><div id="content-29"><p><strong>Look at that!</strong> You can now see all your registered workers (we’ve got one for now) and the jobs you’ve submitted (just one at the moment) through Spark's slick interface. Want to dig even deeper? You can access the worker’s UI by heading to port 8081 and see what’s happening under the hood!</p><p>In this hands-on lab, you've set up your very own experimental Apache Spark cluster using Docker Compose. How cool is that? Now, you can easily submit Spark jobs directly from your Python code like a pro!&nbsp;</p><p>But wait, there’s more! In our next adventure—the Kubernetes lab—you’ll unlock the power of submitting Spark jobs right from the command line.</p></div></div></article></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"article":{"blog_id":"6b4113f2-f30c-4e12-a34a-f5c02abbd1cb","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eSpark Architecture\u003c/h1\u003e\u003cp\u003eImagine a Spark application as a bustling city. At the heart of this city is the \u003cstrong\u003eDriver Program\u003c/strong\u003e, which acts like the mayor overseeing everything that happens. The driver program is responsible for running your code, coordinating work, and making key decisions about how tasks should be executed. Like a city planner, it organizes and manages the tasks to be performed by the cluster, breaking down large jobs into smaller units of work. These jobs are then divided into \u003cstrong\u003etasks\u003c/strong\u003e that Spark can run in parallel. The beauty of this architecture lies in its ability to scale, allowing multiple tasks to be completed simultaneously on different data partitions. These tasks are dispatched to \u003cstrong\u003eexecutors\u003c/strong\u003e, which are the hard workers of the cluster, doing the heavy lifting.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FApache-Spark-architecture.png?alt=media\u0026token=ae8b882a-a359-4e63-aad6-a9a9f0a06ff8'/\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cp\u003eExecutors are independent workers spread across the cluster, each taking responsibility for a portion of the work. They are not just performing tasks but are also responsible for caching data, which can speed up future computations by reusing cached data rather than starting from scratch. Much like how a construction team works efficiently by reusing tools and materials at the job site, executors are optimized to perform computations without redundant effort.\u003c/p\u003e\u003cp\u003eTo help you better visualize this, think of a project that requires breaking down and shipping parts to different locations. Each part can be processed independently and reassembled once all pieces are completed. The driver program ensures these tasks are coordinated correctly, and executors handle each piece of the job.\u003c/p\u003e\u003cp\u003eBut what’s the relationship between \u003cstrong\u003ejobs\u003c/strong\u003e, \u003cstrong\u003etasks\u003c/strong\u003e, and \u003cstrong\u003estages\u003c/strong\u003e? Imagine you're the head of a large construction project, and you need to break down the overall project (the \u003cstrong\u003ejob\u003c/strong\u003e) into manageable tasks for each construction team. A \u003cstrong\u003etask\u003c/strong\u003e in Spark operates on a specific partition of data, much like a construction team focusing on a particular section of the building. Once a group of tasks that don’t depend on any other data is identified, Spark bundles them into \u003cstrong\u003estages\u003c/strong\u003e. A stage represents a set of tasks that can be executed independently without needing data from elsewhere in the project. However, sometimes a task will need information from another part of the dataset, requiring what’s known as a \u003cstrong\u003edata shuffle\u003c/strong\u003e. This shuffle is like coordinating deliveries between different construction teams—an operation that can slow things down due to the necessary data exchange, but essential for the overall completion of the job.\u003c/p\u003e\u003cp\u003eWhat happens when Spark needs to run in different environments? That’s where \u003cstrong\u003ecluster modes\u003c/strong\u003e come in. There are several ways Spark can be deployed, each suited to different use cases. The simplest is \u003cstrong\u003elocal mode\u003c/strong\u003e, ideal for testing on your own machine. Imagine local mode as running your city’s planning department with just one employee—the driver program manages everything on its own, without help from external workers. This is great for testing, but when you need real performance, it’s time to deploy Spark in a cluster.\u003c/p\u003e\u003cp\u003eIn a full cluster setup, Spark supports several modes. \u003cstrong\u003eSpark Standalone\u003c/strong\u003e is the quickest way to set up a cluster environment, ideal for smaller projects or when you want complete control over the infrastructure. For those already working in large-scale environments with Hadoop, \u003cstrong\u003eYARN\u003c/strong\u003e is a natural choice. It integrates seamlessly with the broader Hadoop ecosystem, making it easy to manage resources. For greater flexibility and the ability to handle dynamic workloads, \u003cstrong\u003eApache Mesos\u003c/strong\u003e comes into play, providing a more robust partitioning and scaling system.\u003c/p\u003e\u003cp\u003eBut if you’re looking for a modern, cloud-native approach, \u003cstrong\u003eKubernetes\u003c/strong\u003e offers powerful benefits. Running Spark on Kubernetes is like managing a city that can grow or shrink as needed, using containers to deploy and scale your Spark applications. With Kubernetes, Spark becomes highly portable, making it easy to run your Spark jobs in any cloud environment. You can set up your Spark application inside containers and have them scale automatically based on demand, ensuring smooth processing even as workloads increase.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch1\u003eMastering the Art of Running Apache Spark Applications\u003c/h1\u003e\u003cp\u003eEver wondered how to launch your Apache Spark application into action? Whether you’re processing mountains of data or just running local tests, Apache Spark’s flexibility makes it incredibly powerful. But let’s be real—understanding how to run a Spark application might seem daunting at first. Fear not! Here’s your step-by-step guide to Spark mastery.\u003c/p\u003e\u003cp\u003eAt the heart of it all is the \u003ccode\u003espark-submit\u003c/code\u003e script. Think of it as Spark’s personal conductor, ensuring your application runs smoothly across a distributed cluster or right on your local machine. With \u003ccode\u003espark-submit\u003c/code\u003e, you’ve got full control: it lets you specify everything from the cluster manager you want to connect to, to how much memory and CPU cores your application needs. You can also include any additional files or libraries your app requires, making sure all the pieces are in place for a flawless run.\u003c/p\u003e\u003cp\u003eNow, let’s talk dependencies. In Spark, making sure the driver and executors have access to the right libraries is crucial. If you’re using Java or Scala, bundling all your code and libraries into a single uber-JAR (or fat JAR) is a common approach. This neat package ensures that everything is shipped out and accessible where it’s needed. For Python applications—aka PySpark—you’ll want to ensure that each node in your cluster has the exact same Python libraries installed. Imagine trying to run a marathon with mismatched shoes—it won’t end well, right? Same idea with your Spark dependencies.\u003c/p\u003e\u003cp\u003eIf you’re in the mood for a more hands-on, experimental approach, then the \u003cstrong\u003eSpark Shell\u003c/strong\u003e is your playground. This interactive tool lets you dive right into Spark with either Scala or Python, without needing to write and submit an entire application. When you fire up the Spark Shell, it automatically sets up everything for you—giving you instant access to Spark’s APIs. You can run quick computations, play around with datasets, and see the results in real-time, making it perfect for debugging or just satisfying your curiosity.\u003c/p\u003e\u003cp\u003eSo, to sum it all up: running an Apache Spark application is as easy as using \u003ccode\u003espark-submit\u003c/code\u003e to launch your code, bundling your dependencies into an uber-JAR (or ensuring Python libraries are ready), and—if you're feeling adventurous—jumping into the Spark Shell for some interactive magic. Spark truly puts the power of distributed computing at your fingertips.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003eWe now trying to submit Apache Spark applications from a python script. This exercise is straightforward thanks to Docker Compose. In this lab, you will:\u003c/p\u003e\u003cul\u003e\u003cli\u003eInstall a Spark Master and Worker using Docker Compose\u003c/li\u003e\u003cli\u003eCreate a python script containing a spark job\u003c/li\u003e\u003cli\u003eSubmit the job to the cluster directly from python (Note: you’ll learn how to submit a job from the command line in the Kubernetes Lab)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch2\u003eInstall a Apache Spark cluster using Docker Compose\u003c/h2\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003egit clone https://github.com/big-data-europe/docker-spark\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cp\u003echange to that directory and attempt to docker-compose up\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ecd docker-spark\ndocker-compose up\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cp\u003eAfter quite some time you should see the following message:\u003c/p\u003e\u003cp\u003e\u003cem\u003eSuccessfully registered with master spark://\u0026lt;server address\u0026gt;:7077\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003ch2\u003eCreate Code\u003c/h2\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport findspark\nfindspark.init()\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField, StructType, IntegerType, StringType\nsc = SparkContext.getOrCreate(SparkConf().setMaster('spark://localhost:7077'))\nsc.setLogLevel(\"INFO\")\nspark = SparkSession.builder.getOrCreate()\nspark = SparkSession.builder.getOrCreate()\ndf = spark.createDataFrame(\n    [\n        (1, \"foo\"),\n        (2, \"bar\"),\n    ],\n    StructType(\n        [\n            StructField(\"id\", IntegerType(), False),\n            StructField(\"txt\", StringType(), False),\n        ]\n    ),\n)\nprint(df.dtypes)\ndf.show()\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003ch2\u003e\u003cstrong\u003eExecute code / submit Spark job\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eNow we execute the python file we saved earlier.\u003c/p\u003e\u003cp\u003eIn the terminal, run the following commands to upgrade the pip installer to ensure you have the latest version by running the following commands.Now we execute the python file we saved earlier.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003erm -r ~/.cache/pip/selfcheck/\npip3 install --upgrade pip\npip install --upgrade distro-info\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003cp\u003ePlease enter the following commands in the terminal to download the spark environment.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ewget https://archive.apache.org/dist/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz \n\u0026\u0026 tar xf spark-3.3.3-bin-hadoop3.tgz \u0026\u0026 rm -rf spark-3.3.3-bin-hadoop3.tgz\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cp\u003eRun the following commands to set up the\u0026nbsp;\u0026nbsp;which is preinstalled in the environment and\u0026nbsp;\u0026nbsp;which you just downloaded.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eexport JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64\nexport SPARK_HOME=/home/project/spark-3.3.3-bin-hadoop3\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003cp\u003eInstall the required packages to set up the spark environment.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-24\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003epip install pyspark\npython3 -m pip install findspark\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-25\"\u003e\u003cp\u003eType in the following command in the terminal to execute the Python script.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-26\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003epython3 submit.py\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-27\"\u003e\u003cp\u003ego to port 8080 to see the admin UI of the Spark master\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-28\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fdemo-spark-submit.png?alt=media\u0026token=b8fe9a3e-6bcf-42f4-82a7-2b1842b11e74'/\u003e\u003c/div\u003e\u003cdiv id=\"content-29\"\u003e\u003cp\u003e\u003cstrong\u003eLook at that!\u003c/strong\u003e You can now see all your registered workers (we’ve got one for now) and the jobs you’ve submitted (just one at the moment) through Spark's slick interface. Want to dig even deeper? You can access the worker’s UI by heading to port 8081 and see what’s happening under the hood!\u003c/p\u003e\u003cp\u003eIn this hands-on lab, you've set up your very own experimental Apache Spark cluster using Docker Compose. How cool is that? Now, you can easily submit Spark jobs directly from your Python code like a pro!\u0026nbsp;\u003c/p\u003e\u003cp\u003eBut wait, there’s more! In our next adventure—the Kubernetes lab—you’ll unlock the power of submitting Spark jobs right from the command line.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fspark.png?alt=media\u0026token=7a343f5c-0174-4a31-99e1-4943f9e135af","image_alt":"Apache Spark","short_description":"Welcome to an in-depth exploration of Apache Spark’s architecture! Whether you’re new to Spark or looking to refresh your understanding, this interactive guide will walk you through the key concepts that power Spark’s ability to process massive datasets quickly and efficiently.","timestamp":"2024-10-07 05:37:09","title":"Mastering Apache Spark: An Engaging Dive into Its Architecture and Clusters"}},"__N_SSG":true},"page":"/labs/[slug]","query":{"slug":"6b4113f2-f30c-4e12-a34a-f5c02abbd1cb"},"buildId":"A-n-baZxhaPab-FReiSSJ","assetPrefix":"/Labs","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>