<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Building a Simple CQRS Pattern Architecture</title><meta name="description" content="In this lab we will implement simple CQRS architecture pattern using apache kafka as a message broker, elastic search as a search service and mysql database as a command service."/><meta property="og:title" content="Building a Simple CQRS Pattern Architecture"/><meta property="og:description" content="In this lab we will implement simple CQRS architecture pattern using apache kafka as a message broker, elastic search as a search service and mysql database as a command service."/><meta property="og:type" content="article"/><meta property="og:url" content="https://barbarpotato.github.io/labs/undefined"/><meta name="next-head-count" content="8"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/Labs/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/Labs/_next/static/chunks/webpack-a39982bd6f80347b.js" defer=""></script><script src="/Labs/_next/static/chunks/framework-ecc4130bc7a58a64.js" defer=""></script><script src="/Labs/_next/static/chunks/main-1e09b50edce1e67f.js" defer=""></script><script src="/Labs/_next/static/chunks/pages/_app-753213cf38d40131.js" defer=""></script><script src="/Labs/_next/static/chunks/pages/labs/%5Bslug%5D-b6b888b0a2f30cff.js" defer=""></script><script src="/Labs/_next/static/A-n-baZxhaPab-FReiSSJ/_buildManifest.js" defer=""></script><script src="/Labs/_next/static/A-n-baZxhaPab-FReiSSJ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><article><h1>Building a Simple CQRS Pattern Architecture</h1><p>2024-12-07 09:06:28</p><div><p><span style="color: rgb(255, 255, 255);">The&nbsp;</span><strong style="color: rgb(255, 255, 255);">Command Query Responsibility Segregation (CQRS)</strong><span style="color: rgb(255, 255, 255);">&nbsp;pattern is an architectural pattern used to separate the&nbsp;</span><strong style="color: rgb(255, 255, 255);">write</strong><span style="color: rgb(255, 255, 255);">&nbsp;(commands) and&nbsp;</span><strong style="color: rgb(255, 255, 255);">read</strong><span style="color: rgb(255, 255, 255);">&nbsp;(queries) sides of an application. This separation ensures scalability, performance optimization, and flexibility, especially for systems with complex business logic.</span></p><p><span style="color: rgb(255, 255, 255);">This article explains how to design a&nbsp;</span><strong style="color: rgb(255, 255, 255);">simple CQRS pattern</strong><span style="color: rgb(255, 255, 255);">&nbsp;and implement it in a practical example.</span></p><p><br></p><p><span style="color: rgb(255, 255, 255);"><img src="https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fcqrs.png?alt=media&amp;token=6eab7b0b-37d8-49f2-9137-27dadd766c96" alt="CQRS Basic Pattern" width="720px"></span></p><p><br></p><h2><strong style="color: rgb(255, 255, 255);">Why Do We Need CQRS?</strong></h2><p><span style="color: rgb(255, 255, 255);">CQRS is designed to address challenges in systems where the&nbsp;</span><strong style="color: rgb(255, 255, 255);">read</strong><span style="color: rgb(255, 255, 255);">&nbsp;and&nbsp;</span><strong style="color: rgb(255, 255, 255);">write</strong><span style="color: rgb(255, 255, 255);">&nbsp;operations have distinct requirements. It is particularly helpful in large, complex applications with high performance, scalability, and maintainability needs. Here’s a breakdown of its importance:</span></p><p><span style="color: rgb(255, 255, 255);">Here’s a detailed explanation of&nbsp;</span><strong style="color: rgb(255, 255, 255);">why we need CQRS (Command Query Responsibility Segregation)</strong><span style="color: rgb(255, 255, 255);">:</span></p><h3><strong style="color: rgb(255, 255, 255);">Separation of Concerns</strong></h3><p><span style="color: rgb(255, 255, 255);">In traditional CRUD-based architectures, the same model is often used for both reading and writing data. This can lead to problems such as:</span></p><p><span style="color: rgb(255, 255, 255);">1.Bloated models trying to handle both reads and writes.</span></p><p><span style="color: rgb(255, 255, 255);">2.Tight coupling between read and write logic, making it harder to change one without affecting the other.</span></p><h3><strong style="color: rgb(255, 255, 255);">Optimization of Reads and Writes</strong></h3><p><span style="color: rgb(255, 255, 255);">In many applications, the requirements for&nbsp;</span><strong style="color: rgb(255, 255, 255);">reading data</strong><span style="color: rgb(255, 255, 255);">&nbsp;differ significantly from&nbsp;</span><strong style="color: rgb(255, 255, 255);">writing data</strong><span style="color: rgb(255, 255, 255);">:</span></p><p><strong style="color: rgb(255, 255, 255);">1.Writes</strong><span style="color: rgb(255, 255, 255);">&nbsp;may require strict validation, transactional consistency, and complex domain logic.</span></p><p><strong style="color: rgb(255, 255, 255);">2.Reads</strong><span style="color: rgb(255, 255, 255);">&nbsp;often focus on speed, scalability, and simplicity, potentially requiring optimized or denormalized views of data.</span></p><h2><strong style="color: rgb(255, 255, 255);">Pre-requisites for this lab:</strong></h2><p><span style="color: rgb(255, 255, 255);">In this article we will build simple cqrs architecture with apache kafka, elastic search, and the backend service (Express.js).</span></p><p><span style="color: rgb(255, 255, 255);">&nbsp;VM 1: Runs&nbsp;</span><strong style="color: rgb(255, 255, 255);">Apache Kafka</strong><span style="color: rgb(255, 255, 255);">&nbsp;for handling messaging and event distribution.</span></p><p><span style="color: rgb(255, 255, 255);">&nbsp;VM 2: Runs&nbsp;</span><strong style="color: rgb(255, 255, 255);">Elasticsearch</strong><span style="color: rgb(255, 255, 255);">&nbsp;for read-optimized data storage and retrieval.</span></p><p><strong style="color: rgb(255, 255, 255);">Express.js Services</strong><span style="color: rgb(255, 255, 255);">:</span></p><p><span style="color: rgb(255, 255, 255);">&nbsp;A&nbsp;</span><strong style="color: rgb(255, 255, 255);">Command service</strong><span style="color: rgb(255, 255, 255);">&nbsp;with an endpoint for inserting data into the system.</span></p><p><span style="color: rgb(255, 255, 255);">&nbsp;A&nbsp;</span><strong style="color: rgb(255, 255, 255);">Query service</strong><span style="color: rgb(255, 255, 255);">&nbsp;to retrieve data from Elasticsearch.</span></p><p><span style="color: rgb(255, 255, 255);">&nbsp;Mysql Database that connected to Express.js service</span></p><h2><strong style="color: rgb(255, 255, 255);">Planned Architecture</strong></h2><p><span style="color: rgb(255, 255, 255);"><img src="https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FCQRS.webp?alt=media&amp;token=58c5dcef-485c-482d-8c6a-459473e51f04" alt="Planned CQRS Architecture for this lab" width="720px"></span></p><p><span style="color: rgb(255, 255, 255);">The architecture depicted in your diagram showcases a practical implementation of the CQRS (Command Query Responsibility Segregation) pattern using separate services and data stores for handling write and read operations. At its core, this design focuses on decoupling the responsibilities of updating and retrieving data, ensuring better scalability, performance, and maintainability.</span></p><p><span style="color: rgb(255, 255, 255);">The&nbsp;</span><strong style="color: rgb(255, 255, 255);">Command Service</strong><span style="color: rgb(255, 255, 255);">, built with Express.js, serves as the entry point for handling all write operations. Whenever a client sends a request to add or update data, the Command Service writes the data to a MySQL database. This database acts as the system's primary source of truth, ensuring the durability and consistency of all data. Once the data is successfully persisted in MySQL, the Command Service publishes an event to the&nbsp;</span><strong style="color: rgb(255, 255, 255);">Message Broker</strong><span style="color: rgb(255, 255, 255);">, implemented with Apache Kafka. The role of Kafka here is to act as an intermediary that reliably propagates changes across the system, enabling asynchronous communication between services.</span></p><p><span style="color: rgb(255, 255, 255);">On the other side of the architecture, a consumer service listens to the events broadcasted by Kafka. Whenever a new event is received, the consumer retrieves the relevant data from MySQL, transforms it if needed, and indexes it into an&nbsp;</span><strong style="color: rgb(255, 255, 255);">ElasticSearch instance</strong><span style="color: rgb(255, 255, 255);">. ElasticSearch, being optimized for querying and search operations, ensures that data is structured for fast retrieval. This makes it the perfect choice for systems that need to handle complex queries or search-heavy workloads without compromising performance.</span></p><p><span style="color: rgb(255, 255, 255);">The&nbsp;</span><strong style="color: rgb(255, 255, 255);">Read Service</strong><span style="color: rgb(255, 255, 255);">, also built with Express.js, provides an API for retrieving data from ElasticSearch. By querying ElasticSearch directly, the Read Service delivers low-latency responses to clients, even under high query loads. This design ensures that the performance of the read operations does not interfere with or degrade the performance of write operations in the Command Service. The use of ElasticSearch also enables advanced search capabilities, such as full-text search, aggregations, and filtering, which are often slow or complex to implement in traditional relational databases.</span></p><p><span style="color: rgb(255, 255, 255);">This architecture embodies the essence of CQRS by segregating the responsibilities of writing and querying data into distinct paths. The Command Service and MySQL handle writes and ensure data consistency, while the Read Service and ElasticSearch are optimized for delivering fast and efficient queries. The inclusion of Kafka as a Message Broker enables asynchronous processing, allowing the system to remain responsive to client requests even when downstream systems take time to process data.</span></p><h2><strong style="color: rgb(255, 255, 255);">Setting Up Apache Kafka on Ubuntu Server:</strong></h2><p><span style="color: rgb(255, 255, 255);">In this guide, I’ll walk you through setting up&nbsp;</span><strong style="color: rgb(255, 255, 255);">Apache Kafka</strong><span style="color: rgb(255, 255, 255);">&nbsp;on an&nbsp;</span><strong style="color: rgb(255, 255, 255);">Ubuntu Server</strong><span style="color: rgb(255, 255, 255);">&nbsp;running on a virtual machine. While you can certainly use Docker and Docker Compose for a Kafka setup, I decided to go the manual route to test the installation process. So, if you’re ready to roll up your sleeves, let’s dive in!</span></p><p><strong style="color: rgb(255, 255, 255);">Step 1: Install Java</strong></p><p><span style="color: rgb(255, 255, 255);">Kafka runs on the Java Virtual Machine (JVM), so the first step is to install Java. We’ll use OpenJDK 17 for this:</span></p><div><pre><code>sudo apt update
sudo apt install openjdk-17-jdk -y
</code></pre></div><p><span style="color: rgb(255, 255, 255);">Once installed, you can verify the version with:</span></p><div><pre><code>java -version
</code></pre></div><h4><strong style="color: rgb(255, 255, 255);">Step 2: Download Kafka</strong></h4><p><span style="color: rgb(255, 255, 255);">Next, we need to download the Kafka binaries. Use the following command to grab the latest Kafka release:</span></p><div><pre><code>wget https://downloads.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz
</code></pre></div><p><span style="color: rgb(255, 255, 255);">After downloading, extract the archive:</span></p><div><pre><code>tar -xvf kafka_2.13-3.6.0.tgz
</code></pre></div><p><span style="color: rgb(255, 255, 255);">Now, let’s move the extracted Kafka directory to&nbsp;/opt for easier access:</span></p><div><pre><code>sudo mv kafka_2.13-3.6.0 /opt/kafka
</code></pre></div><p><span style="color: rgb(255, 255, 255);">Finally, navigate to the Kafka directory:</span></p><div><pre><code>cd /opt/kafka
</code></pre></div><h4><strong style="color: rgb(255, 255, 255);">Step 3: Configure Kafka Server Properties</strong></h4><p><span style="color: rgb(255, 255, 255);">Before we start Kafka, we need to tweak its configuration a bit. Open the&nbsp;</span><strong style="color: rgb(255, 255, 255);">server.properties</strong><span style="color: rgb(255, 255, 255);">&nbsp;file with a text editor:</span></p><div><pre><code>nano config/server.properties
</code></pre></div><p><span style="color: rgb(255, 255, 255);">Here are a couple of key settings to look for:</span></p><ul><li><span style="color: rgb(255, 255, 255);">log.dirs: This is where Kafka will store its log files. You can set it to a directory of your choice.</span></li><li><span style="color: rgb(255, 255, 255);">zookeper.connect: Ensure this points to your ZooKeeper instance. If you’re running ZooKeeper locally, the default setting should work.</span></li></ul><h4><strong style="color: rgb(255, 255, 255);">Step 4: Start ZooKeeper</strong></h4><p><span style="color: rgb(255, 255, 255);">Kafka relies on&nbsp;</span><strong style="color: rgb(255, 255, 255);">ZooKeeper</strong><span style="color: rgb(255, 255, 255);">&nbsp;to manage its metadata, so we’ll need to start ZooKeeper before starting Kafka. Use the following command to get ZooKeeper up and running:</span></p><div><pre><code>bin/zookeeper-server-start.sh config/zookeeper.properties
</code></pre></div><p><span style="color: rgb(255, 255, 255);">To run ZooKeeper as a background process (so you can keep using your terminal), use this instead:</span></p><div><pre><code>bin/zookeeper-server-start.sh config/zookeeper.properties &gt; /dev/null 2&gt;&amp;1 &amp;
</code></pre></div><h4><strong style="color: rgb(255, 255, 255);">Step 5: Start the Kafka Broker</strong></h4><p><span style="color: rgb(255, 255, 255);">Now that ZooKeeper is running, it’s time to fire up Kafka. Use this command to start the Kafka broker:</span></p><div><pre><code>bin/kafka-server-start.sh config/server.properties
</code></pre></div><p><span style="color: rgb(255, 255, 255);">Or, to run Kafka in the background:</span></p><div><pre><code>bin/kafka-server-start.sh config/server.properties &gt; /dev/null 2&gt;&amp;1 &amp;
</code></pre></div><h4><strong style="color: rgb(255, 255, 255);">Step 6: Test Your Kafka Setup</strong></h4><p><span style="color: rgb(255, 255, 255);">Congratulations! Your Kafka instance is now up and running. Let’s do a quick test to ensure everything works as expected.</span></p><ol><li><strong style="color: rgb(255, 255, 255);">Create a Topic</strong></li><li><span style="color: rgb(255, 255, 255);">Kafka organizes messages into topics. Let’s create a topic named&nbsp;test-topic:</span></li></ol><div><pre><code>bin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
</code></pre></div><ol><li><strong style="color: rgb(255, 255, 255);">List Topics</strong></li><li><span style="color: rgb(255, 255, 255);">To confirm that the topic was created, list all topics:</span></li></ol><div><pre><code>bin/kafka-topics.sh --list --bootstrap-server localhost:9092
</code></pre></div><ol><li><strong style="color: rgb(255, 255, 255);">Start a Producer</strong></li><li><span style="color: rgb(255, 255, 255);">A producer sends messages to a Kafka topic. Start the producer for&nbsp;test-topic:</span></li></ol><div><pre><code>bin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092
</code></pre></div><ol><li><span style="color: rgb(255, 255, 255);">Type a few messages in the terminal, and they’ll be sent to the topic.</span></li><li><strong style="color: rgb(255, 255, 255);">Start a Consumer</strong></li><li><span style="color: rgb(255, 255, 255);">A consumer reads messages from a topic. Start a consumer to read messages from&nbsp;test-topic:</span></li></ol><div><pre><code>bin/kafka-console-consumer.sh --topic test-topic --from-beginning --bootstrap-server localhost:9092
</code></pre></div><ol><li><span style="color: rgb(255, 255, 255);">You should see the messages you typed in the producer terminal appear here!</span></li></ol><p><br></p><p><br></p><h2><strong style="color: rgb(255, 255, 255);">Part 2: Installing Elasticsearch on a Virtual Machine</strong></h2><h4><strong style="color: rgb(255, 255, 255);">Step 1: SSH into Your Server</strong></h4><p><span style="color: rgb(255, 255, 255);">Connect to your EC2 instance (or VM):</span></p><div><pre><code>ssh -i /path/to/your-key.pem ec2-user@your-ec2-public-ip
</code></pre></div><h4><strong style="color: rgb(255, 255, 255);">Step 2: Install Java</strong></h4><p><span style="color: rgb(255, 255, 255);">Elasticsearch also needs Java:</span></p><div><pre><code>sudo apt update
sudo apt install -y openjdk-11-jdk
</code></pre></div><h4><strong style="color: rgb(255, 255, 255);">Step 3: Install Elasticsearch</strong></h4><div><pre><code>wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
sudo apt install -y apt-transport-https
echo "deb https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-8.x.list

sudo apt update

sudo apt install -y elasticsearch
</code></pre></div><h4><strong style="color: rgb(255, 255, 255);">Step 4: Enable and Start Elasticsearch</strong></h4><div><pre><code>sudo systemctl enable elasticsearch
sudo systemctl start elasticsearch
</code></pre></div><h4><strong style="color: rgb(255, 255, 255);">Step 5: Configure Elasticsearch for External Access</strong></h4><p><span style="color: rgb(255, 255, 255);">Edit the configuration:</span></p><div><pre><code>sudo nano /etc/elasticsearch/elasticsearch.yml
</code></pre></div><p><span style="color: rgb(255, 255, 255);">Set these properties:</span></p><div><pre><code>network.host: 0.0.0.0
http.port: 9200
</code></pre></div><p><span style="color: rgb(255, 255, 255);">Restart Elasticsearch:</span></p><div><pre><code>sudo systemctl restart elasticsearch
</code></pre></div><h4><strong style="color: rgb(255, 255, 255);">Step 6: Verify Elasticsearch</strong></h4><p><span style="color: rgb(255, 255, 255);">Run:</span></p><div><pre><code>curl -X GET http://localhost:9200
</code></pre></div><p><span style="color: rgb(255, 255, 255);">You should see a JSON response!</span></p><p><br></p><h2><strong style="color: rgb(255, 255, 255);">Part:3 Explanation of the Express.js POST Route for Inserting User Data and Sending to Kafka</strong></h2><div><pre><code>import express from 'express';
import { Kafka } from 'kafkajs';
import mysql from 'mysql2/promise'; // MySQL client for Node.js

const app = express();
app.use(express.json()); // Parse JSON request bodies

// Kafka producer setup
const kafka = new Kafka({
&nbsp; &nbsp; clientId: 'my-app',
&nbsp; &nbsp; brokers: ['localhost:9092'], // Replace with your Kafka broker(s)
});

const producer = kafka.producer();

// Connect Kafka producer
async function connectProducer() {
&nbsp; &nbsp; await producer.connect();
}

connectProducer().catch(console.error);

// MySQL database connection setup
const dbConfig = {
&nbsp; &nbsp; host: 'localhost',
&nbsp; &nbsp; user: 'root', // Replace with your MySQL username
&nbsp; &nbsp; password: 'root', // Replace with your MySQL password
&nbsp; &nbsp; database: 'user', // Replace with your database name
};

let connection;

async function connectDatabase() {
&nbsp; &nbsp; try {
&nbsp; &nbsp; &nbsp; &nbsp; connection = await mysql.createConnection(dbConfig);
&nbsp; &nbsp; &nbsp; &nbsp; console.log('Connected to MySQL database');
&nbsp; &nbsp; } catch (error) {
&nbsp; &nbsp; &nbsp; &nbsp; console.error('Error connecting to MySQL:', error);
&nbsp; &nbsp; &nbsp; &nbsp; process.exit(1);
&nbsp; &nbsp; }
}

connectDatabase();

// POST route to insert user data
app.post('/users', async (req, res) =&gt; {
&nbsp; &nbsp; const userData = req.body;

&nbsp; &nbsp; try {
&nbsp; &nbsp; &nbsp; &nbsp; // Insert data into MySQL database
&nbsp; &nbsp; &nbsp; &nbsp; const { name, email, password } = userData; // Assuming user data has 'name' and 'email' fields
&nbsp; &nbsp; &nbsp; &nbsp; const [result] = await connection.execute(
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'INSERT INTO user (username, email, password) VALUES (?, ?, ?)',
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [name, email, password]
&nbsp; &nbsp; &nbsp; &nbsp; );

&nbsp; &nbsp; &nbsp; &nbsp; console.log('User inserted into database:', result);

&nbsp; &nbsp; &nbsp; &nbsp; // Send the user data to Kafka topic 'user-topic'
&nbsp; &nbsp; &nbsp; &nbsp; await producer.send({
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; topic: 'users-topic', // Replace with your topic
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; messages: [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { value: JSON.stringify(userData) },
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ],
&nbsp; &nbsp; &nbsp; &nbsp; });

&nbsp; &nbsp; &nbsp; &nbsp; res.status(201).json({ message: 'User data inserted into database and sent to Kafka' });
&nbsp; &nbsp; } catch (error) {
&nbsp; &nbsp; &nbsp; &nbsp; console.error('Error processing request:', error);
&nbsp; &nbsp; &nbsp; &nbsp; res.status(500).json({ message: 'Error processing request' });
&nbsp; &nbsp; }
});

// Start Express server
app.listen(3000, () =&gt; {
&nbsp; &nbsp; console.log('Server running on http://localhost:3000');
});
</code></pre></div><p><br></p><p><span style="color: rgb(255, 255, 255);">In the given code snippet, an Express.js route (/users) is created to handle&nbsp;</span><strong style="color: rgb(255, 255, 255);">POST requests</strong><span style="color: rgb(255, 255, 255);">. This route processes user data by first saving it into a MySQL database and then sending the same data to a Kafka topic. Below is a step-by-step explanation of how this route works:</span></p><h4><strong style="color: rgb(255, 255, 255);">1.&nbsp;Endpoint Definition and Request Handling</strong></h4><div><pre><code>The app.post('/users', async (req, res) defines a route that listens for POST requests at the /users endpoint. It uses async/await to handle asynchronous operations such as database insertion and Kafka messaging. The req.body object is used to extract the data sent by the client in the request payload.const userData = req.body;
</code></pre></div><p><span style="color: rgb(255, 255, 255);">The&nbsp;userData object contains the user-provided information, typically in JSON format. For example, it might include fields like&nbsp;name. email and password.</span></p><h4><strong style="color: rgb(255, 255, 255);">2.&nbsp;Inserting User Data into MySQL</strong></h4><p><span style="color: rgb(255, 255, 255);">To store user data, the route uses a prepared SQL statement to prevent&nbsp;</span><strong style="color: rgb(255, 255, 255);">SQL injection attacks</strong><span style="color: rgb(255, 255, 255);">. The&nbsp;</span>connection.execute()<span style="color: rgb(255, 255, 255);">&nbsp;function interacts with the database, where&nbsp;name, email and passwords fields are inserted into a&nbsp;user table.</span></p><div><pre><code>const [result] = await connection.execute(
    'INSERT INTO user (username, email, password) VALUES (?, ?, ?)',
    [name, email, password]
);
</code></pre></div><ul><li><strong style="color: rgb(255, 255, 255);">Prepared Statement</strong><span style="color: rgb(255, 255, 255);">: The&nbsp;? placeholders in the SQL query are replaced with actual values (name, email, password) safely.</span></li><li><strong style="color: rgb(255, 255, 255);">Deconstructed User Data</strong><span style="color: rgb(255, 255, 255);">: The&nbsp;name,&nbsp;email, and&nbsp;password fields are extracted from the&nbsp;userData object for better readability and security.</span></li><li><strong style="color: rgb(255, 255, 255);">Result</strong><span style="color: rgb(255, 255, 255);">: The&nbsp;connection.execite()&nbsp;method returns an array, where&nbsp;result contains metadata about the operation, such as the number of rows affected.</span></li></ul><p><span style="color: rgb(255, 255, 255);">If the operation succeeds, a log is generated to confirm that the user data was inserted into the database:</span></p><div><pre><code>console.log('User inserted into database:', result);
</code></pre></div><h4><strong style="color: rgb(255, 255, 255);">3.&nbsp;Sending Data to a Kafka Topic</strong></h4><p><span style="color: rgb(255, 255, 255);">After successfully storing the data in MySQL, the route sends the same data to a Kafka topic for further processing. Kafka is often used to handle large-scale distributed messaging and stream processing.</span></p><div><pre><code>await producer.send({
    topic: 'users-topic', // Replace with your topic
    messages: [
        { value: JSON.stringify(userData) },
    ],
});
</code></pre></div><ul><li><strong style="color: rgb(255, 255, 255);">Kafka Producer</strong><span style="color: rgb(255, 255, 255);">p: The producer object is an instance of Kafka's producer client, which is responsible for sending messages to Kafka topics.</span></li><li><strong style="color: rgb(255, 255, 255);">Topic Name</strong><span style="color: rgb(255, 255, 255);">: The&nbsp;topic field specifies the destination Kafka topic (user-topic in this case). This is where the message will be sent for further processing by Kafka consumers.</span></li><li><strong style="color: rgb(255, 255, 255);">Message Payload</strong><span style="color: rgb(255, 255, 255);">: The&nbsp;message array contains the data to be sent. Each message is an object with a&nbsp;value field, which holds the serialized user data (converted to JSON using&nbsp;JSON.stringfy(userData)).</span></li></ul><p><span style="color: rgb(255, 255, 255);">This mechanism ensures that user data is available for other systems (e.g., analytics, logging, or notifications) in near real-time.</span></p><h4><strong style="color: rgb(255, 255, 255);">4.&nbsp;Response to the Client</strong></h4><p>If both the database insertion and Kafka message-sending steps succeed, the server sends a 201 Created response to the client with a success message:</p><div><pre><code>res.status(201).json({ message: 'User data inserted into database and sent to Kafka' });
</code></pre></div><p>The 201 status code indicates that the request was successfully processed and a new resource was created.</p><h4><strong style="color: rgb(255, 255, 255);">5.&nbsp;Error Handling</strong></h4><p>The try-catch block ensures that errors during either database insertion or Kafka messaging are gracefully handled. If an error occurs, it is logged for debugging purposes, and the client receives a 500 Internal Server Error response:</p><div><pre><code>catch (error) {
    console.error('Error processing request:', error);
    res.status(500).json({ message: 'Error processing request' });
}
</code></pre></div><p><span style="color: rgb(255, 255, 255);">This approach provides transparency to developers and prevents the application from crashing due to unhandled exceptions.</span></p><p><br></p><p><span style="color: rgb(255, 255, 255);">After building the express.js service to insert data in mysql database and message broker. we will create the instance that listening on this producer.</span></p><p><br></p><h2><strong style="color: rgb(255, 255, 255);">Part 4: Explanation of Kafka Consumer with Elasticsearch Integration</strong></h2><div><pre><code>import { Kafka } from 'kafkajs';
import { Client } from '@elastic/elasticsearch';

// Kafka consumer setup
const kafka = new Kafka({
&nbsp; &nbsp; clientId: 'express-app',
&nbsp; &nbsp; brokers: ['192.168.128.207:9092'], // Replace with your Kafka broker(s)
});

const consumer = kafka.consumer({ groupId: 'user-group' });

// Elasticsearch client setup
const esClient = new Client({
&nbsp; &nbsp; node: 'http://localhost:9200', // Replace with your Elasticsearch URL
});

// Kafka consumer processing
async function consumeMessages() {
&nbsp; &nbsp; await consumer.connect();
&nbsp; &nbsp; await consumer.subscribe({ topic: 'users-topic', fromBeginning: true }); // Replace with your topic

&nbsp; &nbsp; await consumer.run({
&nbsp; &nbsp; &nbsp; &nbsp; eachMessage: async ({ topic, partition, message }) =&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const userData = JSON.parse(message.value.toString());

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // Insert data into Elasticsearch
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; try {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const response = await esClient.index({
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; index: 'users', // The Elasticsearch index to use
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; id: message.name,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; document: userData,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; console.log('User data inserted into Elasticsearch:', response);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } catch (error) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; console.error('Error inserting data into Elasticsearch:', error);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; },
&nbsp; &nbsp; });
}

consumeMessages().catch(console.error);
</code></pre></div><p>This code demonstrates how to integrate Kafka as a messaging system and Elasticsearch as a search and data indexing tool in a Node.js application. The overall flow involves consuming messages from a Kafka topic and then indexing the received data into Elasticsearch for further use, such as querying or searching.</p><p>To start, the script imports two essential libraries: KafkaJS and Elasticsearch client. KafkaJS is a JavaScript library used for interacting with Kafka, which is a distributed streaming platform. The Kafka client allows you to create consumers that can listen to Kafka topics and process the messages in real time. On the other hand, the Elasticsearch client facilitates communication with an Elasticsearch cluster, enabling the ability to store and index documents, which can later be queried or analyzed.</p><p>The Kafka consumer is set up by first initializing the Kafka client with a unique clientId (express-app) and specifying the Kafka brokers. These brokers are the Kafka servers where the consumer will connect. The consumer is created with a groupId, which is user-group in this case. The group ID helps manage message consumption across multiple instances of the consumer. When consumers with the same group ID listen to a Kafka topic, Kafka ensures that each partition of the topic is assigned to only one consumer in the group, effectively balancing the load.</p><p>Next, the code sets up the Elasticsearch client by specifying the address of the Elasticsearch node (http://localhost:9200). This client will be used to interact with the Elasticsearch service where the user data will be indexed. Elasticsearch is widely used for its powerful search and analytics capabilities, which can handle large volumes of data and provide fast search results.</p><p>Once both Kafka and Elasticsearch clients are set up, the consumeMessages() function is created to handle the actual logic of consuming messages from Kafka. This function first connects to the Kafka cluster and subscribes to the users-topic. By subscribing to the topic, the consumer listens for new messages that are published to that topic. The fromBeginning: true option ensures that the consumer starts processing messages from the very beginning of the topic’s log, meaning it will consume all the messages from when it first subscribes, not just new messages that arrive after it subscribes.</p><p>The function then uses consumer.run() to begin consuming messages. Each message from the Kafka topic is processed in the eachMessage callback function. Inside this function, the message's value is parsed from a buffer into a JavaScript object (since Kafka messages are typically sent as binary data). The parsed data, which represents user information in this case, is then indexed into Elasticsearch. The esClient.index() method is used to insert this data into the users index in Elasticsearch. A unique identifier for the document is generated using the message's name field. This id ensures that each document in Elasticsearch can be uniquely identified.</p><p>If the data insertion into Elasticsearch is successful, a response from Elasticsearch is logged to the console, confirming that the user data has been indexed. If an error occurs while inserting the data, the error is caught and logged.</p><p>Finally, the consumeMessages() function is invoked, and any unhandled errors are caught by console.error() to prevent the application from crashing. This ensures that the consumer will keep running and processing messages as they arrive, continuously feeding new data into Elasticsearch for indexing.</p><h2><strong style="color: rgb(255, 255, 255);">Part 5: Wrapup what we have been doing</strong></h2><p>The architecture discussed in this article aligns well with the CQRS (Command Query Responsibility Segregation) pattern, which is a powerful design pattern that separates the logic of reading data (queries) from the logic of writing data (commands). By implementing Kafka and Elasticsearch in conjunction with MySQL and Express.js, we create a robust system that effectively adheres to the principles of CQRS.</p><p>In this architecture, the write operations (commands) are handled by the /users POST route in the Express.js service. When user data is received, it's inserted into the MySQL database and sent to Kafka. Kafka acts as the message bus, decoupling the data-writing process from the read operations and ensuring that data can be asynchronously processed and consumed by different systems or services.</p><p>The read operations (queries) are efficiently handled by Elasticsearch. After the data is consumed from Kafka and indexed into Elasticsearch, it becomes readily available for fast and scalable querying. Elasticsearch's ability to index and search large volumes of data makes it an excellent fit for handling query-based operations in this architecture.</p><p>By using CQRS, we ensure that the system is optimized for both reading and writing operations, enabling high scalability and responsiveness. Kafka, as the message broker, enables asynchronous communication and allows for horizontal scaling in the system. Meanwhile, Elasticsearch ensures that queries on user data are fast, efficient, and scalable.</p><p>This CQRS-based approach also helps with performance optimization, as read and write concerns are handled separately. It allows for the scaling of each part of the system independently, depending on whether there is a higher load on reading or writing. The separation of concerns promotes better maintainability, flexibility, and scalability, making this architecture ideal for modern, high-traffic applications requiring real-time data processing and analytics.</p><p>In conclusion, by integrating Kafka, Elasticsearch, and Express.js with a CQRS approach, this system architecture offers a scalable, maintainable, and highly performant solution for handling real-time data in applications where reading and writing data need to be optimized separately.</p></div></article></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"article":{"blog_id":"0af995c6-3bd5-405a-ad71-6ebeaa675d38","description":"\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eCommand Query Responsibility Segregation (CQRS)\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;pattern is an architectural pattern used to separate the\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ewrite\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;(commands) and\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eread\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;(queries) sides of an application. This separation ensures scalability, performance optimization, and flexibility, especially for systems with complex business logic.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThis article explains how to design a\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003esimple CQRS pattern\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;and implement it in a practical example.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u003cimg src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fcqrs.png?alt=media\u0026amp;token=6eab7b0b-37d8-49f2-9137-27dadd766c96\" alt=\"CQRS Basic Pattern\" width=\"720px\"\u003e\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eWhy Do We Need CQRS?\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eCQRS is designed to address challenges in systems where the\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eread\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;and\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ewrite\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;operations have distinct requirements. It is particularly helpful in large, complex applications with high performance, scalability, and maintainability needs. Here’s a breakdown of its importance:\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eHere’s a detailed explanation of\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ewhy we need CQRS (Command Query Responsibility Segregation)\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e:\u003c/span\u003e\u003c/p\u003e\u003ch3\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eSeparation of Concerns\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn traditional CRUD-based architectures, the same model is often used for both reading and writing data. This can lead to problems such as:\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e1.Bloated models trying to handle both reads and writes.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e2.Tight coupling between read and write logic, making it harder to change one without affecting the other.\u003c/span\u003e\u003c/p\u003e\u003ch3\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eOptimization of Reads and Writes\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn many applications, the requirements for\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ereading data\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;differ significantly from\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ewriting data\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e:\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e1.Writes\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;may require strict validation, transactional consistency, and complex domain logic.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e2.Reads\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;often focus on speed, scalability, and simplicity, potentially requiring optimized or denormalized views of data.\u003c/span\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePre-requisites for this lab:\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn this article we will build simple cqrs architecture with apache kafka, elastic search, and the backend service (Express.js).\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;VM 1: Runs\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eApache Kafka\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;for handling messaging and event distribution.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;VM 2: Runs\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eElasticsearch\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;for read-optimized data storage and retrieval.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eExpress.js Services\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e:\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;A\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eCommand service\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;with an endpoint for inserting data into the system.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;A\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eQuery service\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;to retrieve data from Elasticsearch.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;Mysql Database that connected to Express.js service\u003c/span\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePlanned Architecture\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u003cimg src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FCQRS.webp?alt=media\u0026amp;token=58c5dcef-485c-482d-8c6a-459473e51f04\" alt=\"Planned CQRS Architecture for this lab\" width=\"720px\"\u003e\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe architecture depicted in your diagram showcases a practical implementation of the CQRS (Command Query Responsibility Segregation) pattern using separate services and data stores for handling write and read operations. At its core, this design focuses on decoupling the responsibilities of updating and retrieving data, ensuring better scalability, performance, and maintainability.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eCommand Service\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e, built with Express.js, serves as the entry point for handling all write operations. Whenever a client sends a request to add or update data, the Command Service writes the data to a MySQL database. This database acts as the system's primary source of truth, ensuring the durability and consistency of all data. Once the data is successfully persisted in MySQL, the Command Service publishes an event to the\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eMessage Broker\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e, implemented with Apache Kafka. The role of Kafka here is to act as an intermediary that reliably propagates changes across the system, enabling asynchronous communication between services.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eOn the other side of the architecture, a consumer service listens to the events broadcasted by Kafka. Whenever a new event is received, the consumer retrieves the relevant data from MySQL, transforms it if needed, and indexes it into an\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eElasticSearch instance\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e. ElasticSearch, being optimized for querying and search operations, ensures that data is structured for fast retrieval. This makes it the perfect choice for systems that need to handle complex queries or search-heavy workloads without compromising performance.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eRead Service\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e, also built with Express.js, provides an API for retrieving data from ElasticSearch. By querying ElasticSearch directly, the Read Service delivers low-latency responses to clients, even under high query loads. This design ensures that the performance of the read operations does not interfere with or degrade the performance of write operations in the Command Service. The use of ElasticSearch also enables advanced search capabilities, such as full-text search, aggregations, and filtering, which are often slow or complex to implement in traditional relational databases.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThis architecture embodies the essence of CQRS by segregating the responsibilities of writing and querying data into distinct paths. The Command Service and MySQL handle writes and ensure data consistency, while the Read Service and ElasticSearch are optimized for delivering fast and efficient queries. The inclusion of Kafka as a Message Broker enables asynchronous processing, allowing the system to remain responsive to client requests even when downstream systems take time to process data.\u003c/span\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eSetting Up Apache Kafka on Ubuntu Server:\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn this guide, I’ll walk you through setting up\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eApache Kafka\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;on an\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eUbuntu Server\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;running on a virtual machine. While you can certainly use Docker and Docker Compose for a Kafka setup, I decided to go the manual route to test the installation process. So, if you’re ready to roll up your sleeves, let’s dive in!\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 1: Install Java\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eKafka runs on the Java Virtual Machine (JVM), so the first step is to install Java. We’ll use OpenJDK 17 for this:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo apt update\nsudo apt install openjdk-17-jdk -y\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eOnce installed, you can verify the version with:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ejava -version\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 2: Download Kafka\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eNext, we need to download the Kafka binaries. Use the following command to grab the latest Kafka release:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ewget https://downloads.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eAfter downloading, extract the archive:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003etar -xvf kafka_2.13-3.6.0.tgz\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eNow, let’s move the extracted Kafka directory to\u0026nbsp;/opt for easier access:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo mv kafka_2.13-3.6.0 /opt/kafka\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eFinally, navigate to the Kafka directory:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ecd /opt/kafka\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 3: Configure Kafka Server Properties\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eBefore we start Kafka, we need to tweak its configuration a bit. Open the\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eserver.properties\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;file with a text editor:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003enano config/server.properties\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eHere are a couple of key settings to look for:\u003c/span\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003elog.dirs: This is where Kafka will store its log files. You can set it to a directory of your choice.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003ezookeper.connect: Ensure this points to your ZooKeeper instance. If you’re running ZooKeeper locally, the default setting should work.\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 4: Start ZooKeeper\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eKafka relies on\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eZooKeeper\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;to manage its metadata, so we’ll need to start ZooKeeper before starting Kafka. Use the following command to get ZooKeeper up and running:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/zookeeper-server-start.sh config/zookeeper.properties\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eTo run ZooKeeper as a background process (so you can keep using your terminal), use this instead:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/zookeeper-server-start.sh config/zookeeper.properties \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 5: Start the Kafka Broker\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eNow that ZooKeeper is running, it’s time to fire up Kafka. Use this command to start the Kafka broker:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-server-start.sh config/server.properties\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eOr, to run Kafka in the background:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-server-start.sh config/server.properties \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 6: Test Your Kafka Setup\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eCongratulations! Your Kafka instance is now up and running. Let’s do a quick test to ensure everything works as expected.\u003c/span\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eCreate a Topic\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eKafka organizes messages into topics. Let’s create a topic named\u0026nbsp;test-topic:\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eList Topics\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eTo confirm that the topic was created, list all topics:\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-topics.sh --list --bootstrap-server localhost:9092\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStart a Producer\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eA producer sends messages to a Kafka topic. Start the producer for\u0026nbsp;test-topic:\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eType a few messages in the terminal, and they’ll be sent to the topic.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStart a Consumer\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eA consumer reads messages from a topic. Start a consumer to read messages from\u0026nbsp;test-topic:\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-console-consumer.sh --topic test-topic --from-beginning --bootstrap-server localhost:9092\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eYou should see the messages you typed in the producer terminal appear here!\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePart 2: Installing Elasticsearch on a Virtual Machine\u003c/strong\u003e\u003c/h2\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 1: SSH into Your Server\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eConnect to your EC2 instance (or VM):\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003essh -i /path/to/your-key.pem ec2-user@your-ec2-public-ip\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 2: Install Java\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eElasticsearch also needs Java:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo apt update\nsudo apt install -y openjdk-11-jdk\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 3: Install Elasticsearch\u003c/strong\u003e\u003c/h4\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ewget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\nsudo apt install -y apt-transport-https\necho \"deb https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-8.x.list\n\nsudo apt update\n\nsudo apt install -y elasticsearch\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 4: Enable and Start Elasticsearch\u003c/strong\u003e\u003c/h4\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo systemctl enable elasticsearch\nsudo systemctl start elasticsearch\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 5: Configure Elasticsearch for External Access\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eEdit the configuration:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo nano /etc/elasticsearch/elasticsearch.yml\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eSet these properties:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003enetwork.host: 0.0.0.0\nhttp.port: 9200\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eRestart Elasticsearch:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo systemctl restart elasticsearch\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 6: Verify Elasticsearch\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eRun:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ecurl -X GET http://localhost:9200\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eYou should see a JSON response!\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePart:3 Explanation of the Express.js POST Route for Inserting User Data and Sending to Kafka\u003c/strong\u003e\u003c/h2\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eimport express from 'express';\nimport { Kafka } from 'kafkajs';\nimport mysql from 'mysql2/promise'; // MySQL client for Node.js\n\nconst app = express();\napp.use(express.json()); // Parse JSON request bodies\n\n// Kafka producer setup\nconst kafka = new Kafka({\n\u0026nbsp; \u0026nbsp; clientId: 'my-app',\n\u0026nbsp; \u0026nbsp; brokers: ['localhost:9092'], // Replace with your Kafka broker(s)\n});\n\nconst producer = kafka.producer();\n\n// Connect Kafka producer\nasync function connectProducer() {\n\u0026nbsp; \u0026nbsp; await producer.connect();\n}\n\nconnectProducer().catch(console.error);\n\n// MySQL database connection setup\nconst dbConfig = {\n\u0026nbsp; \u0026nbsp; host: 'localhost',\n\u0026nbsp; \u0026nbsp; user: 'root', // Replace with your MySQL username\n\u0026nbsp; \u0026nbsp; password: 'root', // Replace with your MySQL password\n\u0026nbsp; \u0026nbsp; database: 'user', // Replace with your database name\n};\n\nlet connection;\n\nasync function connectDatabase() {\n\u0026nbsp; \u0026nbsp; try {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; connection = await mysql.createConnection(dbConfig);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.log('Connected to MySQL database');\n\u0026nbsp; \u0026nbsp; } catch (error) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.error('Error connecting to MySQL:', error);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; process.exit(1);\n\u0026nbsp; \u0026nbsp; }\n}\n\nconnectDatabase();\n\n// POST route to insert user data\napp.post('/users', async (req, res) =\u0026gt; {\n\u0026nbsp; \u0026nbsp; const userData = req.body;\n\n\u0026nbsp; \u0026nbsp; try {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; // Insert data into MySQL database\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; const { name, email, password } = userData; // Assuming user data has 'name' and 'email' fields\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; const [result] = await connection.execute(\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; 'INSERT INTO user (username, email, password) VALUES (?, ?, ?)',\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; [name, email, password]\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; );\n\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.log('User inserted into database:', result);\n\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; // Send the user data to Kafka topic 'user-topic'\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; await producer.send({\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; topic: 'users-topic', // Replace with your topic\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; messages: [\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; { value: JSON.stringify(userData) },\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; ],\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; });\n\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; res.status(201).json({ message: 'User data inserted into database and sent to Kafka' });\n\u0026nbsp; \u0026nbsp; } catch (error) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.error('Error processing request:', error);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; res.status(500).json({ message: 'Error processing request' });\n\u0026nbsp; \u0026nbsp; }\n});\n\n// Start Express server\napp.listen(3000, () =\u0026gt; {\n\u0026nbsp; \u0026nbsp; console.log('Server running on http://localhost:3000');\n});\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn the given code snippet, an Express.js route (/users) is created to handle\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePOST requests\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e. This route processes user data by first saving it into a MySQL database and then sending the same data to a Kafka topic. Below is a step-by-step explanation of how this route works:\u003c/span\u003e\u003c/p\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e1.\u0026nbsp;Endpoint Definition and Request Handling\u003c/strong\u003e\u003c/h4\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eThe app.post('/users', async (req, res) defines a route that listens for POST requests at the /users endpoint. It uses async/await to handle asynchronous operations such as database insertion and Kafka messaging. The req.body object is used to extract the data sent by the client in the request payload.const userData = req.body;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe\u0026nbsp;userData object contains the user-provided information, typically in JSON format. For example, it might include fields like\u0026nbsp;name. email and password.\u003c/span\u003e\u003c/p\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e2.\u0026nbsp;Inserting User Data into MySQL\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eTo store user data, the route uses a prepared SQL statement to prevent\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eSQL injection attacks\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e. The\u0026nbsp;\u003c/span\u003econnection.execute()\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;function interacts with the database, where\u0026nbsp;name, email and passwords fields are inserted into a\u0026nbsp;user table.\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003econst [result] = await connection.execute(\n    'INSERT INTO user (username, email, password) VALUES (?, ?, ?)',\n    [name, email, password]\n);\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePrepared Statement\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;? placeholders in the SQL query are replaced with actual values (name, email, password) safely.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eDeconstructed User Data\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;name,\u0026nbsp;email, and\u0026nbsp;password fields are extracted from the\u0026nbsp;userData object for better readability and security.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eResult\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;connection.execite()\u0026nbsp;method returns an array, where\u0026nbsp;result contains metadata about the operation, such as the number of rows affected.\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIf the operation succeeds, a log is generated to confirm that the user data was inserted into the database:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003econsole.log('User inserted into database:', result);\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e3.\u0026nbsp;Sending Data to a Kafka Topic\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eAfter successfully storing the data in MySQL, the route sends the same data to a Kafka topic for further processing. Kafka is often used to handle large-scale distributed messaging and stream processing.\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eawait producer.send({\n    topic: 'users-topic', // Replace with your topic\n    messages: [\n        { value: JSON.stringify(userData) },\n    ],\n});\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eKafka Producer\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003ep: The producer object is an instance of Kafka's producer client, which is responsible for sending messages to Kafka topics.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eTopic Name\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;topic field specifies the destination Kafka topic (user-topic in this case). This is where the message will be sent for further processing by Kafka consumers.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eMessage Payload\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;message array contains the data to be sent. Each message is an object with a\u0026nbsp;value field, which holds the serialized user data (converted to JSON using\u0026nbsp;JSON.stringfy(userData)).\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThis mechanism ensures that user data is available for other systems (e.g., analytics, logging, or notifications) in near real-time.\u003c/span\u003e\u003c/p\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e4.\u0026nbsp;Response to the Client\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eIf both the database insertion and Kafka message-sending steps succeed, the server sends a 201 Created response to the client with a success message:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eres.status(201).json({ message: 'User data inserted into database and sent to Kafka' });\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe 201 status code indicates that the request was successfully processed and a new resource was created.\u003c/p\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e5.\u0026nbsp;Error Handling\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eThe try-catch block ensures that errors during either database insertion or Kafka messaging are gracefully handled. If an error occurs, it is logged for debugging purposes, and the client receives a 500 Internal Server Error response:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ecatch (error) {\n    console.error('Error processing request:', error);\n    res.status(500).json({ message: 'Error processing request' });\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThis approach provides transparency to developers and prevents the application from crashing due to unhandled exceptions.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eAfter building the express.js service to insert data in mysql database and message broker. we will create the instance that listening on this producer.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePart 4: Explanation of Kafka Consumer with Elasticsearch Integration\u003c/strong\u003e\u003c/h2\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eimport { Kafka } from 'kafkajs';\nimport { Client } from '@elastic/elasticsearch';\n\n// Kafka consumer setup\nconst kafka = new Kafka({\n\u0026nbsp; \u0026nbsp; clientId: 'express-app',\n\u0026nbsp; \u0026nbsp; brokers: ['192.168.128.207:9092'], // Replace with your Kafka broker(s)\n});\n\nconst consumer = kafka.consumer({ groupId: 'user-group' });\n\n// Elasticsearch client setup\nconst esClient = new Client({\n\u0026nbsp; \u0026nbsp; node: 'http://localhost:9200', // Replace with your Elasticsearch URL\n});\n\n// Kafka consumer processing\nasync function consumeMessages() {\n\u0026nbsp; \u0026nbsp; await consumer.connect();\n\u0026nbsp; \u0026nbsp; await consumer.subscribe({ topic: 'users-topic', fromBeginning: true }); // Replace with your topic\n\n\u0026nbsp; \u0026nbsp; await consumer.run({\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; eachMessage: async ({ topic, partition, message }) =\u0026gt; {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; const userData = JSON.parse(message.value.toString());\n\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; // Insert data into Elasticsearch\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; try {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; const response = await esClient.index({\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; index: 'users', // The Elasticsearch index to use\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; id: message.name,\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; document: userData,\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; });\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.log('User data inserted into Elasticsearch:', response);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; } catch (error) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.error('Error inserting data into Elasticsearch:', error);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; },\n\u0026nbsp; \u0026nbsp; });\n}\n\nconsumeMessages().catch(console.error);\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis code demonstrates how to integrate Kafka as a messaging system and Elasticsearch as a search and data indexing tool in a Node.js application. The overall flow involves consuming messages from a Kafka topic and then indexing the received data into Elasticsearch for further use, such as querying or searching.\u003c/p\u003e\u003cp\u003eTo start, the script imports two essential libraries: KafkaJS and Elasticsearch client. KafkaJS is a JavaScript library used for interacting with Kafka, which is a distributed streaming platform. The Kafka client allows you to create consumers that can listen to Kafka topics and process the messages in real time. On the other hand, the Elasticsearch client facilitates communication with an Elasticsearch cluster, enabling the ability to store and index documents, which can later be queried or analyzed.\u003c/p\u003e\u003cp\u003eThe Kafka consumer is set up by first initializing the Kafka client with a unique clientId (express-app) and specifying the Kafka brokers. These brokers are the Kafka servers where the consumer will connect. The consumer is created with a groupId, which is user-group in this case. The group ID helps manage message consumption across multiple instances of the consumer. When consumers with the same group ID listen to a Kafka topic, Kafka ensures that each partition of the topic is assigned to only one consumer in the group, effectively balancing the load.\u003c/p\u003e\u003cp\u003eNext, the code sets up the Elasticsearch client by specifying the address of the Elasticsearch node (http://localhost:9200). This client will be used to interact with the Elasticsearch service where the user data will be indexed. Elasticsearch is widely used for its powerful search and analytics capabilities, which can handle large volumes of data and provide fast search results.\u003c/p\u003e\u003cp\u003eOnce both Kafka and Elasticsearch clients are set up, the consumeMessages() function is created to handle the actual logic of consuming messages from Kafka. This function first connects to the Kafka cluster and subscribes to the users-topic. By subscribing to the topic, the consumer listens for new messages that are published to that topic. The fromBeginning: true option ensures that the consumer starts processing messages from the very beginning of the topic’s log, meaning it will consume all the messages from when it first subscribes, not just new messages that arrive after it subscribes.\u003c/p\u003e\u003cp\u003eThe function then uses consumer.run() to begin consuming messages. Each message from the Kafka topic is processed in the eachMessage callback function. Inside this function, the message's value is parsed from a buffer into a JavaScript object (since Kafka messages are typically sent as binary data). The parsed data, which represents user information in this case, is then indexed into Elasticsearch. The esClient.index() method is used to insert this data into the users index in Elasticsearch. A unique identifier for the document is generated using the message's name field. This id ensures that each document in Elasticsearch can be uniquely identified.\u003c/p\u003e\u003cp\u003eIf the data insertion into Elasticsearch is successful, a response from Elasticsearch is logged to the console, confirming that the user data has been indexed. If an error occurs while inserting the data, the error is caught and logged.\u003c/p\u003e\u003cp\u003eFinally, the consumeMessages() function is invoked, and any unhandled errors are caught by console.error() to prevent the application from crashing. This ensures that the consumer will keep running and processing messages as they arrive, continuously feeding new data into Elasticsearch for indexing.\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePart 5: Wrapup what we have been doing\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe architecture discussed in this article aligns well with the CQRS (Command Query Responsibility Segregation) pattern, which is a powerful design pattern that separates the logic of reading data (queries) from the logic of writing data (commands). By implementing Kafka and Elasticsearch in conjunction with MySQL and Express.js, we create a robust system that effectively adheres to the principles of CQRS.\u003c/p\u003e\u003cp\u003eIn this architecture, the write operations (commands) are handled by the /users POST route in the Express.js service. When user data is received, it's inserted into the MySQL database and sent to Kafka. Kafka acts as the message bus, decoupling the data-writing process from the read operations and ensuring that data can be asynchronously processed and consumed by different systems or services.\u003c/p\u003e\u003cp\u003eThe read operations (queries) are efficiently handled by Elasticsearch. After the data is consumed from Kafka and indexed into Elasticsearch, it becomes readily available for fast and scalable querying. Elasticsearch's ability to index and search large volumes of data makes it an excellent fit for handling query-based operations in this architecture.\u003c/p\u003e\u003cp\u003eBy using CQRS, we ensure that the system is optimized for both reading and writing operations, enabling high scalability and responsiveness. Kafka, as the message broker, enables asynchronous communication and allows for horizontal scaling in the system. Meanwhile, Elasticsearch ensures that queries on user data are fast, efficient, and scalable.\u003c/p\u003e\u003cp\u003eThis CQRS-based approach also helps with performance optimization, as read and write concerns are handled separately. It allows for the scaling of each part of the system independently, depending on whether there is a higher load on reading or writing. The separation of concerns promotes better maintainability, flexibility, and scalability, making this architecture ideal for modern, high-traffic applications requiring real-time data processing and analytics.\u003c/p\u003e\u003cp\u003eIn conclusion, by integrating Kafka, Elasticsearch, and Express.js with a CQRS approach, this system architecture offers a scalable, maintainable, and highly performant solution for handling real-time data in applications where reading and writing data need to be optimized separately.\u003c/p\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fcqrs-background-title.png?alt=media\u0026token=dd34dffa-1cc4-4b14-b4e2-f6840555b0e4","image_alt":"Image cover of CQRS Architecture","short_description":"In this lab we will implement simple CQRS architecture pattern using apache kafka as a message broker, elastic search as a search service and mysql database as a command service.","timestamp":"2024-12-07 09:06:28","title":"Building a Simple CQRS Pattern Architecture"}},"__N_SSG":true},"page":"/labs/[slug]","query":{"slug":"0af995c6-3bd5-405a-ad71-6ebeaa675d38"},"buildId":"A-n-baZxhaPab-FReiSSJ","assetPrefix":"/Labs","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>