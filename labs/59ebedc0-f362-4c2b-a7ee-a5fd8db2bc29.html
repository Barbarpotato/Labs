<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Create a DAG for Apache Airflow with Python Operator</title><meta name="description" content="In this lab, you will explore the Apache Airflow web user interface (UI). You will then create a Direct Acyclic Graph (DAG) using PythonOperator and finally run it through the Airflow web UI."/><meta property="og:title" content="Create a DAG for Apache Airflow with Python Operator"/><meta property="og:description" content="In this lab, you will explore the Apache Airflow web user interface (UI). You will then create a Direct Acyclic Graph (DAG) using PythonOperator and finally run it through the Airflow web UI."/><meta property="og:type" content="article"/><meta property="og:url" content="https://barbarpotato.github.io/labs/undefined"/><meta name="next-head-count" content="8"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/Labs/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/Labs/_next/static/chunks/webpack-a39982bd6f80347b.js" defer=""></script><script src="/Labs/_next/static/chunks/framework-ecc4130bc7a58a64.js" defer=""></script><script src="/Labs/_next/static/chunks/main-1e09b50edce1e67f.js" defer=""></script><script src="/Labs/_next/static/chunks/pages/_app-753213cf38d40131.js" defer=""></script><script src="/Labs/_next/static/chunks/pages/labs/%5Bslug%5D-b6b888b0a2f30cff.js" defer=""></script><script src="/Labs/_next/static/A-n-baZxhaPab-FReiSSJ/_buildManifest.js" defer=""></script><script src="/Labs/_next/static/A-n-baZxhaPab-FReiSSJ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><article><h1>Create a DAG for Apache Airflow with Python Operator</h1><p>2024-09-20 11:35:46</p><div><div id="content-0"><p class="ql-align-justify">Apache Airflow is an open-source platform designed for orchestrating and managing complex workflows and data pipelines. It allows users to programmatically author, schedule, and monitor workflows through a rich web-based user interface. Airflow uses directed acyclic graphs (DAGs) to represent workflows, making it highly flexible and scalable for a wide range of use cases. With built-in support for dynamic pipeline generation, Airflow enables users to create workflows that adapt to changes in data and business logic. Additionally, its extensible architecture allows integration</p></div><div id="content-1"><img src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_architecture.png?alt=media&token=2ff31468-d678-4134-b46c-4f0da4f4ff93'/></div><div id="content-3"><p class="ql-align-justify">Apache Airflow's architecture is designed to be modular and scalable, consisting of several key components: the Scheduler, the Executor, the Web Server, and the Metadata Database. The Scheduler handles the scheduling of tasks, ensuring they run at the correct times and in the correct order. The Executor runs the tasks, which can be distributed across multiple workers for scalability. The Web Server provides a user-friendly interface for managing and monitoring workflows, while the Metadata Database stores the state of tasks and workflows.</p><p class="ql-align-justify">Workflows in Airflow are defined as Directed Acyclic Graphs (DAGs), where each node represents a task, and edges define the dependencies between tasks. This structure allows for clear visualization of the workflow and its dependencies. Tasks within a DAG can range from simple data processing steps to complex data transfer operations, and they can be scheduled to run at specific intervals or triggered by external events. Airflow supports a wide range of operators for different tasks, including BashOperator for running bash scripts, PythonOperator for executing Python code, and many more for interacting with various data systems and services. This flexibility and the ability to create dynamic, code-driven workflows make Airflow a powerful tool for orchestrating data pipelines and workflows.</p><p class="ql-align-justify">In this lab we will learn how to install apache airflow and create some basic DAG to perform ETL task. First we want to make sure our apache airflow in in our local machine for demonstration. We will install using docker and docker compose for it. Lets create new folder and named it as we want. In this case im going to create folder name apache_airflow. Then lets&nbsp;create a Dockerfile inside that folder and fill the file with this code:</p><p><br></p></div><div id="content-4"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">FROM apache/airflow:latest
USER root
RUN apt-get update && \
    apt-get -y install git && \
    apt-get clean

USER airflow
</code></pre></div><div id="content-5"><p class="ql-align-justify">Overall, this Dockerfile snippet customizes the Airflow Docker image by adding Git, which can be useful for tasks such as cloning repositories or managing code directly within the Airflow environment. Lets run this docker file. If the build image is successful, we then jump to the next step where we are going to create a new file docker compose to use our image and create a volums for our data. Below is the example of how we build our compose to run apache airflow:</p></div><div id="content-6"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">version: '3'
services:
  apacheairflow:
    image: apacheairflow:latest

    volumes:
      - ./airflow:/opt/airflow

    ports:
      - "8080:8080"
    command: airflow standalone
</code></pre></div><div id="content-7"><p class="ql-align-justify">After this, we can access our apache airflow in our local machine by accessing trough <a href="http://localhost:8080/" target="_blank">http://localhost:8080</a>. Dont forget to build up our compose file. By running docker-compose up -d. We can type our username Admin and we wil get our password from our folder generated from /airflow/stand_alone_admin_password.txt.</p><p class="ql-align-justify">Now we can create our DAG file in our /airflow folder. Create new folder name dags and fill the python file inside of it. The name is our wishes.&nbsp;Then we can create a simple task from it. But we need to import some thing like:</p></div><div id="content-8"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;"># Import the libraries
from datetime import timedelta
# The DAG object; we'll need this to instantiate a DAG
from airflow.models import DAG
# Operators; you need this to write tasks!
from airflow.operators.python import PythonOperator

# This makes scheduling easy
from airflow.utils.dates import days_ago</code></pre></div><div id="content-9"><p class="ql-align-justify">Now we can define our function, DAG Argument, DAG Definitions, Define the task, and create a task pipeline from this file.</p><p class="ql-align-justify">In this case. I just want to grab some data from the api and save it to json file.Lets create our functions first called extract here is the code for the extract data from the api random jokes:</p></div><div id="content-10"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">def extract():
    response = requests.get('https://official-joke-api.appspot.com/random_joke')
    joke = response.json()

    # Define the path to the JSON file
    file_path = '/opt/airflow/jokes/joke.json'
    
    try:
        # Ensure the directory exists
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Read existing jokes from the JSON file if it exists
        if os.path.exists(file_path):
            with open(file_path, 'r') as f:
                # Try to load the existing jokes, handle if file is empty
                content = f.read()
                jokes = json.loads(content) if content else []
        else:
            jokes = []

        # Append the new joke to the list
        jokes.append(joke)
        
        # Write the updated list of jokes to the JSON file
        with open(file_path, 'w') as f:
            json.dump(jokes, f, indent=4)
        
        print(f"Joke saved to {file_path}: {joke['setup']} - {joke['punchline']}")
    except Exception as e:
        print(f"Error: {e}")
        raise</code></pre></div><div id="content-11"><p>After that we can create our DAG Arguments and DAG Definitions:</p></div><div id="content-12"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;"># You can override them on a per-task basis during operator initialization
default_args = {
    'owner': 'Darmawan',
    'start_date': days_ago(0),
    'email': ['darmawanjr88@gmail.com'],
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

# Define the DAG
dag = DAG(
    'jokes-callable-dag',
    default_args=default_args,
    description='My first DAG',
    schedule_interval=timedelta(days=1),
)
</code></pre></div><div id="content-13"><p>Then we can define the task named execute_extract to call the extract function</p></div><div id="content-14"><p>And build the the task pipeline for now we just have a single task when we have multiple task we can call our defined task like a sequence call like task_1 &gt;&gt; task_2 &gt;&gt; task_3 and so on. Now we can go to the web UI and triggering the DAG that we build.</p></div><div id="content-15"><img src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_1.png?alt=media&token=1b05157a-20dc-4748-bb40-ff9bbaeb23be'/></div><div id="content-16"><p>Horray we dit it! We can create a simple task from the apache airflow. We can make another complex task from our idea and our problems that we faced from our life.</p></div></div></article></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"article":{"blog_id":"59ebedc0-f362-4c2b-a7ee-a5fd8db2bc29","description":"\u003cdiv id=\"content-0\"\u003e\u003cp class=\"ql-align-justify\"\u003eApache Airflow is an open-source platform designed for orchestrating and managing complex workflows and data pipelines. It allows users to programmatically author, schedule, and monitor workflows through a rich web-based user interface. Airflow uses directed acyclic graphs (DAGs) to represent workflows, making it highly flexible and scalable for a wide range of use cases. With built-in support for dynamic pipeline generation, Airflow enables users to create workflows that adapt to changes in data and business logic. Additionally, its extensible architecture allows integration\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003cimg src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_architecture.png?alt=media\u0026token=2ff31468-d678-4134-b46c-4f0da4f4ff93'/\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp class=\"ql-align-justify\"\u003eApache Airflow's architecture is designed to be modular and scalable, consisting of several key components: the Scheduler, the Executor, the Web Server, and the Metadata Database. The Scheduler handles the scheduling of tasks, ensuring they run at the correct times and in the correct order. The Executor runs the tasks, which can be distributed across multiple workers for scalability. The Web Server provides a user-friendly interface for managing and monitoring workflows, while the Metadata Database stores the state of tasks and workflows.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eWorkflows in Airflow are defined as Directed Acyclic Graphs (DAGs), where each node represents a task, and edges define the dependencies between tasks. This structure allows for clear visualization of the workflow and its dependencies. Tasks within a DAG can range from simple data processing steps to complex data transfer operations, and they can be scheduled to run at specific intervals or triggered by external events. Airflow supports a wide range of operators for different tasks, including BashOperator for running bash scripts, PythonOperator for executing Python code, and many more for interacting with various data systems and services. This flexibility and the ability to create dynamic, code-driven workflows make Airflow a powerful tool for orchestrating data pipelines and workflows.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eIn this lab we will learn how to install apache airflow and create some basic DAG to perform ETL task. First we want to make sure our apache airflow in in our local machine for demonstration. We will install using docker and docker compose for it. Lets create new folder and named it as we want. In this case im going to create folder name apache_airflow. Then lets\u0026nbsp;create a Dockerfile inside that folder and fill the file with this code:\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eFROM apache/airflow:latest\nUSER root\nRUN apt-get update \u0026\u0026 \\\n    apt-get -y install git \u0026\u0026 \\\n    apt-get clean\n\nUSER airflow\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cp class=\"ql-align-justify\"\u003eOverall, this Dockerfile snippet customizes the Airflow Docker image by adding Git, which can be useful for tasks such as cloning repositories or managing code directly within the Airflow environment. Lets run this docker file. If the build image is successful, we then jump to the next step where we are going to create a new file docker compose to use our image and create a volums for our data. Below is the example of how we build our compose to run apache airflow:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eversion: '3'\nservices:\n  apacheairflow:\n    image: apacheairflow:latest\n\n    volumes:\n      - ./airflow:/opt/airflow\n\n    ports:\n      - \"8080:8080\"\n    command: airflow standalone\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp class=\"ql-align-justify\"\u003eAfter this, we can access our apache airflow in our local machine by accessing trough \u003ca href=\"http://localhost:8080/\" target=\"_blank\"\u003ehttp://localhost:8080\u003c/a\u003e. Dont forget to build up our compose file. By running docker-compose up -d. We can type our username Admin and we wil get our password from our folder generated from /airflow/stand_alone_admin_password.txt.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eNow we can create our DAG file in our /airflow folder. Create new folder name dags and fill the python file inside of it. The name is our wishes.\u0026nbsp;Then we can create a simple task from it. But we need to import some thing like:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# Import the libraries\nfrom datetime import timedelta\n# The DAG object; we'll need this to instantiate a DAG\nfrom airflow.models import DAG\n# Operators; you need this to write tasks!\nfrom airflow.operators.python import PythonOperator\n\n# This makes scheduling easy\nfrom airflow.utils.dates import days_ago\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cp class=\"ql-align-justify\"\u003eNow we can define our function, DAG Argument, DAG Definitions, Define the task, and create a task pipeline from this file.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eIn this case. I just want to grab some data from the api and save it to json file.Lets create our functions first called extract here is the code for the extract data from the api random jokes:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003edef extract():\n    response = requests.get('https://official-joke-api.appspot.com/random_joke')\n    joke = response.json()\n\n    # Define the path to the JSON file\n    file_path = '/opt/airflow/jokes/joke.json'\n    \n    try:\n        # Ensure the directory exists\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        \n        # Read existing jokes from the JSON file if it exists\n        if os.path.exists(file_path):\n            with open(file_path, 'r') as f:\n                # Try to load the existing jokes, handle if file is empty\n                content = f.read()\n                jokes = json.loads(content) if content else []\n        else:\n            jokes = []\n\n        # Append the new joke to the list\n        jokes.append(joke)\n        \n        # Write the updated list of jokes to the JSON file\n        with open(file_path, 'w') as f:\n            json.dump(jokes, f, indent=4)\n        \n        print(f\"Joke saved to {file_path}: {joke['setup']} - {joke['punchline']}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n        raise\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cp\u003eAfter that we can create our DAG Arguments and DAG Definitions:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# You can override them on a per-task basis during operator initialization\ndefault_args = {\n    'owner': 'Darmawan',\n    'start_date': days_ago(0),\n    'email': ['darmawanjr88@gmail.com'],\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Define the DAG\ndag = DAG(\n    'jokes-callable-dag',\n    default_args=default_args,\n    description='My first DAG',\n    schedule_interval=timedelta(days=1),\n)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cp\u003eThen we can define the task named execute_extract to call the extract function\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cp\u003eAnd build the the task pipeline for now we just have a single task when we have multiple task we can call our defined task like a sequence call like task_1 \u0026gt;\u0026gt; task_2 \u0026gt;\u0026gt; task_3 and so on. Now we can go to the web UI and triggering the DAG that we build.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cimg src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_1.png?alt=media\u0026token=1b05157a-20dc-4748-bb40-ff9bbaeb23be'/\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cp\u003eHorray we dit it! We can create a simple task from the apache airflow. We can make another complex task from our idea and our problems that we faced from our life.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_python.jpg?alt=media\u0026token=f6c31e13-c63f-49c5-b0ad-b54ec8509560","image_alt":"Apache airflow with Python","short_description":"In this lab, you will explore the Apache Airflow web user interface (UI). You will then create a Direct Acyclic Graph (DAG) using PythonOperator and finally run it through the Airflow web UI.","timestamp":"2024-09-20 11:35:46","title":"Create a DAG for Apache Airflow with Python Operator"}},"__N_SSG":true},"page":"/labs/[slug]","query":{"slug":"59ebedc0-f362-4c2b-a7ee-a5fd8db2bc29"},"buildId":"A-n-baZxhaPab-FReiSSJ","assetPrefix":"/Labs","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>