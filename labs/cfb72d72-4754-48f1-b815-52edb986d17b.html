<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Introduction to Apache Kafka</title><meta name="description" content="In this lab we will be covering about what is the apache kafka. How it works and little bit cover about the installation and the basic workflow demonstration"/><meta property="og:title" content="Introduction to Apache Kafka"/><meta property="og:description" content="In this lab we will be covering about what is the apache kafka. How it works and little bit cover about the installation and the basic workflow demonstration"/><meta property="og:type" content="article"/><meta property="og:url" content="https://barbarpotato.github.io/labs/undefined"/><meta name="next-head-count" content="8"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/Labs/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/Labs/_next/static/chunks/webpack-a39982bd6f80347b.js" defer=""></script><script src="/Labs/_next/static/chunks/framework-ecc4130bc7a58a64.js" defer=""></script><script src="/Labs/_next/static/chunks/main-1e09b50edce1e67f.js" defer=""></script><script src="/Labs/_next/static/chunks/pages/_app-753213cf38d40131.js" defer=""></script><script src="/Labs/_next/static/chunks/pages/labs/%5Bslug%5D-b6b888b0a2f30cff.js" defer=""></script><script src="/Labs/_next/static/A-n-baZxhaPab-FReiSSJ/_buildManifest.js" defer=""></script><script src="/Labs/_next/static/A-n-baZxhaPab-FReiSSJ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><article><h1>Introduction to Apache Kafka</h1><p>2024-09-20 11:35:12</p><div><div id="content-0"><h1 class="ql-align-justify"><strong>Introduction</strong></h1><p class="ql-align-justify">Apache Kafka is an event streaming platform that helps in moving and storing large amounts of data in real-time. It is like a central hub where different sources of data can send their events, and these events can be consumed by various applications or systems.</p><p class="ql-align-justify">Think of it as a postal service. Imagine you have different people sending letters from different locations, and you want all these letters to be collected in one place so that anyone who needs them can access them. Apache Kafka acts as that central place where all the letters (events) are collected and stored. It ensures that the events are delivered reliably and can be accessed by multiple applications or systems.</p><p class="ql-align-justify">Kafka is highly scalable, meaning it can handle a large volume of data and process it quickly. It is also fault-tolerant, which means it can recover from failures and ensure that no data is lost. Additionally, Kafka is open source, so it can be used for free and customized according to specific needs.</p></div><div id="content-1"><h1 class="ql-align-justify"><strong>Architecture</strong></h1></div><div id="content-2"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_kafka_architecture.png?alt=media&token=4f637bf3-00ec-4dc6-9fb9-9f5602e9d18a'/></div><div id="content-3"><p class="ql-align-justify">The architecture of Apache Kafka consists of several key components that work together to enable the efficient streaming and processing of data. Here is a simplified explanation of the architecture and its components:</p><ol><li class="ql-align-justify">Producers: Producers are the sources of data in Kafka. They send events or messages to Kafka topics. Topics are like categories or channels where events are organized and stored.</li><li class="ql-align-justify">Topics: Topics are the central entities in Kafka. They represent a specific category or stream of events. Producers send events to specific topics, and consumers subscribe to topics to receive those events.</li><li class="ql-align-justify">Brokers: Brokers are the servers in the Kafka cluster. They receive events from producers, store them, and distribute them to consumers. Each broker can handle a specific amount of data and provides fault tolerance by replicating data across multiple brokers.</li><li class="ql-align-justify">Consumers: Consumers are the applications or systems that subscribe to topics and receive events from Kafka. They process the events according to their specific requirements.</li><li class="ql-align-justify">Partitions: Topics are divided into partitions, which are individual ordered sequences of events. Each partition is stored on a specific broker. Partitioning allows for parallel processing and scalability.</li><li class="ql-align-justify">Consumer Groups: Consumer groups are a way to scale the consumption of events. Multiple consumers can be part of a consumer group, and each consumer within the group will receive a subset of the events from the topic partitions.</li><li class="ql-align-justify">ZooKeeper (deprecated in newer versions): ZooKeeper is a distributed coordination service that was used in older versions of Kafka for managing the cluster and maintaining metadata. However, in newer versions, Kafka Raft (KRaft) is used to eliminate the dependency on ZooKeeper.</li><li class="ql-align-justify">Kafka Connect: Kafka Connect is a framework for importing and exporting data to and from Kafka. It allows seamless integration with external systems and data sources.</li><li class="ql-align-justify">Kafka Streams: Kafka Streams is a library that enables stream processing of data within Kafka. It allows developers to build real-time applications and perform transformations, aggregations, and analytics on the data.</li></ol></div><div id="content-4"><h1 class="ql-align-justify"><strong>Core Component</strong></h1><p class="ql-align-justify">Here's a breakdown of the core components of Kafka:</p><p class="ql-align-justify">1. Brokers: These are dedicated servers that receive, store, process, and distribute events. They are like the central hub for all the events.</p><p class="ql-align-justify">2. Topics: These are containers or databases of events. Each topic stores specific types of events, such as logs, transactions, or metrics.</p><p class="ql-align-justify">3. Partitions: Topics are divided into different partitions, which are like smaller sections within a topic. This helps with scalability and performance.</p><p class="ql-align-justify">4. Replications: Partitions are duplicated and stored in different brokers. This ensures fault tolerance and allows for parallel processing of events.</p><p class="ql-align-justify">5. Producers: These are client applications that publish events into topics. They can associate events with a key to ensure they go to the same partition. 6. Consumers: These are client applications that subscribe to topics and read events from them. They can read events as they occur or go back and read events from the beginning. To build an event streaming pipeline, we create topics, publish events using producers, and consume events using consumers. We can also use the Kafka command-line interface (CLI) to manage topics, producers, and consumers. Kafka provides a powerful and scalable solution for processing and analyzing streams of events.</p></div><div id="content-5"><h1 class="ql-align-justify"><strong>Installation</strong></h1><p class="ql-align-justify">In this lab we wil going to test the apache kafka using the docker compose. Here is the file of the docker compose, so you can test it immediately without thinking how to installed it.</p></div><div id="content-6"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1</code></pre></div><div id="content-7"><p>We can now running this docker compose by type in the terminal : docker-compose up -d. and then wait for all the setup to be completed. Once we completed we can now access the kafka container using this command:</p></div><div id="content-8"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">docker exec -it &lt;kafka-container-id&gt; bash</code></pre></div><div id="content-9"><p>Now we are inside of the container. Now we can create a topic inside this apache kafka. we simply create a topic named test. Here is the command in the terminal:</p></div><div id="content-10"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">kafka-topics --create --topic test --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1</code></pre></div><div id="content-11"><p>Once we create a topic now we can prepare other 2 terminal for the apache kafka demonstration. where the one terminal is used fot the producer and the other one is going to be a consumer. Lets begin with the first termnal and type:</p></div><div id="content-12"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">kafka-console-producer --topic test --bootstrap-server localhost:9092</code></pre></div><div id="content-14"><p>Now our producer topic is available in this terminal. we can type a few messages and hit&nbsp;<code>Enter</code>&nbsp;after each message. In the second terminal we are going to make the consumer. here is the command:</p></div><div id="content-15"><pre style="background-color: black; color: white; padding:10px; border-radius: 5px;"><code style="color: white;">docker exec -it &lt;kafka-container-id&gt; kafka-console-consumer --topic test --bootstrap-server localhost:9092 --from-beginning</code></pre></div><div id="content-20"><p>Now we should see the messages we produced in the previous step. Here is the output of what he have been doing so far</p></div><div id="content-21"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_1.png?alt=media&token=09d39473-77b7-46a3-ac9d-2dd44b252e1a'/></div><div id="content-22"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_2.png?alt=media&token=b3885d54-45ce-41a7-8b43-3eccc3c52241'/></div><div id="content-23"><h1>Conclusion</h1><p>We've successfully installed Apache Kafka using Docker and tested it by producing and consuming messages. This setup can be expanded with additional configurations and services as needed for our use case.</p></div></div></article></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"article":{"blog_id":"cfb72d72-4754-48f1-b815-52edb986d17b","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1 class=\"ql-align-justify\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h1\u003e\u003cp class=\"ql-align-justify\"\u003eApache Kafka is an event streaming platform that helps in moving and storing large amounts of data in real-time. It is like a central hub where different sources of data can send their events, and these events can be consumed by various applications or systems.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eThink of it as a postal service. Imagine you have different people sending letters from different locations, and you want all these letters to be collected in one place so that anyone who needs them can access them. Apache Kafka acts as that central place where all the letters (events) are collected and stored. It ensures that the events are delivered reliably and can be accessed by multiple applications or systems.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eKafka is highly scalable, meaning it can handle a large volume of data and process it quickly. It is also fault-tolerant, which means it can recover from failures and ensure that no data is lost. Additionally, Kafka is open source, so it can be used for free and customized according to specific needs.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch1 class=\"ql-align-justify\"\u003e\u003cstrong\u003eArchitecture\u003c/strong\u003e\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_kafka_architecture.png?alt=media\u0026token=4f637bf3-00ec-4dc6-9fb9-9f5602e9d18a'/\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp class=\"ql-align-justify\"\u003eThe architecture of Apache Kafka consists of several key components that work together to enable the efficient streaming and processing of data. Here is a simplified explanation of the architecture and its components:\u003c/p\u003e\u003col\u003e\u003cli class=\"ql-align-justify\"\u003eProducers: Producers are the sources of data in Kafka. They send events or messages to Kafka topics. Topics are like categories or channels where events are organized and stored.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eTopics: Topics are the central entities in Kafka. They represent a specific category or stream of events. Producers send events to specific topics, and consumers subscribe to topics to receive those events.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eBrokers: Brokers are the servers in the Kafka cluster. They receive events from producers, store them, and distribute them to consumers. Each broker can handle a specific amount of data and provides fault tolerance by replicating data across multiple brokers.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eConsumers: Consumers are the applications or systems that subscribe to topics and receive events from Kafka. They process the events according to their specific requirements.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003ePartitions: Topics are divided into partitions, which are individual ordered sequences of events. Each partition is stored on a specific broker. Partitioning allows for parallel processing and scalability.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eConsumer Groups: Consumer groups are a way to scale the consumption of events. Multiple consumers can be part of a consumer group, and each consumer within the group will receive a subset of the events from the topic partitions.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eZooKeeper (deprecated in newer versions): ZooKeeper is a distributed coordination service that was used in older versions of Kafka for managing the cluster and maintaining metadata. However, in newer versions, Kafka Raft (KRaft) is used to eliminate the dependency on ZooKeeper.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eKafka Connect: Kafka Connect is a framework for importing and exporting data to and from Kafka. It allows seamless integration with external systems and data sources.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eKafka Streams: Kafka Streams is a library that enables stream processing of data within Kafka. It allows developers to build real-time applications and perform transformations, aggregations, and analytics on the data.\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch1 class=\"ql-align-justify\"\u003e\u003cstrong\u003eCore Component\u003c/strong\u003e\u003c/h1\u003e\u003cp class=\"ql-align-justify\"\u003eHere's a breakdown of the core components of Kafka:\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e1. Brokers: These are dedicated servers that receive, store, process, and distribute events. They are like the central hub for all the events.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e2. Topics: These are containers or databases of events. Each topic stores specific types of events, such as logs, transactions, or metrics.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e3. Partitions: Topics are divided into different partitions, which are like smaller sections within a topic. This helps with scalability and performance.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e4. Replications: Partitions are duplicated and stored in different brokers. This ensures fault tolerance and allows for parallel processing of events.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e5. Producers: These are client applications that publish events into topics. They can associate events with a key to ensure they go to the same partition. 6. Consumers: These are client applications that subscribe to topics and read events from them. They can read events as they occur or go back and read events from the beginning. To build an event streaming pipeline, we create topics, publish events using producers, and consume events using consumers. We can also use the Kafka command-line interface (CLI) to manage topics, producers, and consumers. Kafka provides a powerful and scalable solution for processing and analyzing streams of events.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch1 class=\"ql-align-justify\"\u003e\u003cstrong\u003eInstallation\u003c/strong\u003e\u003c/h1\u003e\u003cp class=\"ql-align-justify\"\u003eIn this lab we wil going to test the apache kafka using the docker compose. Here is the file of the docker compose, so you can test it immediately without thinking how to installed it.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eversion: '3.8'\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:latest\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n    ports:\n      - \"2181:2181\"\n\n  kafka:\n    image: confluentinc/cp-kafka:latest\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003eWe can now running this docker compose by type in the terminal : docker-compose up -d. and then wait for all the setup to be completed. Once we completed we can now access the kafka container using this command:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003edocker exec -it \u0026lt;kafka-container-id\u0026gt; bash\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cp\u003eNow we are inside of the container. Now we can create a topic inside this apache kafka. we simply create a topic named test. Here is the command in the terminal:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekafka-topics --create --topic test --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cp\u003eOnce we create a topic now we can prepare other 2 terminal for the apache kafka demonstration. where the one terminal is used fot the producer and the other one is going to be a consumer. Lets begin with the first termnal and type:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekafka-console-producer --topic test --bootstrap-server localhost:9092\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cp\u003eNow our producer topic is available in this terminal. we can type a few messages and hit\u0026nbsp;\u003ccode\u003eEnter\u003c/code\u003e\u0026nbsp;after each message. In the second terminal we are going to make the consumer. here is the command:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003edocker exec -it \u0026lt;kafka-container-id\u0026gt; kafka-console-consumer --topic test --bootstrap-server localhost:9092 --from-beginning\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cp\u003eNow we should see the messages we produced in the previous step. Here is the output of what he have been doing so far\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_1.png?alt=media\u0026token=09d39473-77b7-46a3-ac9d-2dd44b252e1a'/\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_2.png?alt=media\u0026token=b3885d54-45ce-41a7-8b43-3eccc3c52241'/\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eWe've successfully installed Apache Kafka using Docker and tested it by producing and consuming messages. This setup can be expanded with additional configurations and services as needed for our use case.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_kafka.png?alt=media\u0026token=50fe3034-c769-4d0b-b54c-5c0ad0bbd0e5","image_alt":"Apache Kafka","short_description":"In this lab we will be covering about what is the apache kafka. How it works and little bit cover about the installation and the basic workflow demonstration","timestamp":"2024-09-20 11:35:12","title":"Introduction to Apache Kafka"}},"__N_SSG":true},"page":"/labs/[slug]","query":{"slug":"cfb72d72-4754-48f1-b815-52edb986d17b"},"buildId":"A-n-baZxhaPab-FReiSSJ","assetPrefix":"/Labs","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>